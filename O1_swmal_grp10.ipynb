{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3718e159",
   "metadata": {},
   "source": [
    "# SWMAL O1\n",
    "\n",
    "**15-09-2023**\n",
    "\n",
    "**[Gr10]**\n",
    "\n",
    "1. Nina Thomsen - 202003675\n",
    "2. Clara Mouritsen - 202006533\n",
    "3. Marcin Szymanek - 202009418\n",
    "\n",
    "\n",
    "**When seeing ** besides text or code, it is made with the help of ChatGBT or similar AI.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2f58b",
   "metadata": {},
   "source": [
    "## Python Basics\n",
    "\n",
    "\n",
    "### Qa Load and test the `libitmal` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b0f1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AssertInRange', 'CheckFloat', 'DToXy', 'GenerateConfusionMatrix', 'GenerateResults', 'InRange', 'Iterable', 'ListToMatrix', 'ListToVector', 'PrintMatrix', 'ResetRandom', 'ShowResult', 'TEST', 'TestAll', 'TestCheckFloat', 'TestPrintMatrix', 'TestVarName', 'VarName', 'XyToD', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'ctxlib', 'inf', 'inspect', 'isFloat', 'isList', 'isNumpyArray', 'nan', 'np', 'random', 're', 'sklearn']\n",
      "\n",
      "Here is the placement of the itmal:\n",
      "C:\\Users\\ninat\\OneDrive\\Skrivebord\\Softwareteknologi\\Machine-Learning\\gitmal\\libitmal\\utils.py\n",
      "\n",
      "Test function:\n",
      "TestPrintMatrix...(no regression testing)\n",
      "X=[[   1.    2.]\n",
      "   [   3. -100.]\n",
      "   [   1.   -1.]]\n",
      "X=[[ 1.  2.]\n",
      "   ...\n",
      "   [ 1. -1.]]\n",
      "X=[[   1.\n",
      "       2.    ]\n",
      "   [   3.0001\n",
      "    -100.    ]\n",
      "   [   1.\n",
      "      -1.    ]]\n",
      "X=[[   1.    2.]\n",
      "   [   3. -100.]\n",
      "   [   1.   -1.]]\n",
      "OK\n",
      "TEST: OK\n",
      "ALL OK\n"
     ]
    }
   ],
   "source": [
    "from libitmal import utils as itmalutils\n",
    "\n",
    "print(dir(itmalutils))\n",
    "\n",
    "print(\"\\nHere is the placement of the itmal:\")\n",
    "print(itmalutils.__file__)\n",
    "\n",
    "print(\"\\nTest function:\")\n",
    "itmalutils.TestAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17f1d2",
   "metadata": {},
   "source": [
    "### Qb Create your own module, with some functions, and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76c1c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libriary placement:\n",
      "<module 'libitmal.my_module' from 'C:\\\\Users\\\\ninat\\\\OneDrive\\\\Skrivebord\\\\Softwareteknologi\\\\Machine-Learning\\\\gitmal\\\\libitmal\\\\my_module.py'>\n",
      "\n",
      "Calling self made module:\n",
      "Hello, Nina!\n"
     ]
    }
   ],
   "source": [
    "from libitmal import my_module as mym\n",
    "\n",
    "print(\"Libriary placement:\")\n",
    "print(mym)\n",
    "\n",
    "print(\"\\nCalling self made module:\")\n",
    "print(mym.say_hello(\"Nina\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9351f38",
   "metadata": {},
   "source": [
    "### Qc How do you 'recompile' a module?\n",
    "\n",
    "The load_ext is a command which loads the autoreload extension. \n",
    "The autoreload 2 means a module will reload everytime a cell is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafb24f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Clara!\n",
      "new hello\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(mym.say_hello(\"Clara\"))\n",
    "mym.new_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a67b8a",
   "metadata": {},
   "source": [
    "## Classes in Python\n",
    "\n",
    "### Qe Extend the class with some public and private functions and member variables\n",
    "\n",
    "#### Private variables\n",
    "If the variables is named with a single leading underscore, it tells python that it should be treated as a internal or private variable. However it can still be accessed from the outside, so it is up to the programmer to respect the private variables. \n",
    "#### Private function\n",
    "The same applies to function where a single underscore indicates it is meant to be private, but can still be accessed from the outside of the class. \n",
    "\n",
    "#### Self meaning\n",
    "When working with classes in python, the term \"self\" refers to the instance of the class that is currently being used.\n",
    "If you forget to include \"self\" in the parameter list, it will result in an error\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99aab2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyClass.myfun() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ninat\\OneDrive\\Skrivebord\\Softwareteknologi\\Machine-Learning\\Afleveringer\\Aflevering1_git\\swmal_grp10\\O1_swmal_grp10.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmyfun\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m myobjectx \u001b[39m=\u001b[39m MyClass()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m myobjectx\u001b[39m.\u001b[39;49mmyfun() \u001b[39m# Will result in error as no self is provided\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: MyClass.myfun() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def myfun(): # Missing self\n",
    "        print(\"myfun\")\n",
    "\n",
    "myobjectx = MyClass()\n",
    "        \n",
    "myobjectx.myfun() # Will result in error as no self is provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a532f83",
   "metadata": {},
   "source": [
    "### Qf Extend the class with a Constructor\n",
    "\n",
    "#### Constructor \n",
    "In python a constructor is called an initilizer and is declared by writing __init__ with 2 underscores on both sides. It is automatically declared when an object of the class is created. \n",
    "\n",
    "#### Desctructor \n",
    "The descructor in python is declared with __del__ but is not required as the class makes one internally if none is provided. Additionallt python relies heavily on automatic garbage collection and reference counting like C#. \n",
    "The destructor is called when del obj is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83e848c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructor called\n",
      "Destructor called\n"
     ]
    }
   ],
   "source": [
    "# **\n",
    "class MyClass:\n",
    "    def __init__(self, arg1, arg2):\n",
    "        self.arg1 = arg1\n",
    "        self.arg2 = arg2\n",
    "        print(\"Constructor called\")\n",
    "    def __del__(self):\n",
    "        print(\"Destructor called\")\n",
    "        \n",
    "obj = MyClass(\"Hello\", 42) #This will call the constructor\n",
    "del obj #This will call the destructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d022456",
   "metadata": {},
   "source": [
    "### Qg Extend the class with a to-string function\n",
    "\n",
    "To serialize a class in python, you can use a method called __str__, which returns a string representation of the object, allowing to \"print\" the class instance as a string.\n",
    "The __str__ method is called whenever you use the str() function or try to convert an object to a string using str(object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df5e70f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person(name='Nina', age=27)\n"
     ]
    }
   ],
   "source": [
    "# **\n",
    "class Person:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Person(name='{self.name}', age={self.age})\"\n",
    "\n",
    "person = Person(\"Nina\", 27)\n",
    "\n",
    "# Using __str__ method to get a string representation of the object\n",
    "string_representation = str(person)\n",
    "\n",
    "print(string_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9964179",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This exercise will touch the concepts of linear regression and the R2 score. We will calculate teta0 and teta1 which corresponds to A and B in y = Ax + B. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92366935-934a-4327-9e9f-30c232d0788a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16f54ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ninat\\OneDrive\\Skrivebord\\Softwareteknologi\\Machine-Learning\\Afleveringer\\Aflevering1_git\\swmal_grp10\\O1_swmal_grp10.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division, print_function, unicode_literals\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Common imports\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# to make this notebook's output stable across runs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804559bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "def prepare_country_stats(oecd_bli, gdp_per_capita):\n",
    "    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "    gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n",
    "                                  left_index=True, right_index=True)\n",
    "    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "    keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3b80b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 5875-4FB0\n",
      "\n",
      " Directory of C:\\Users\\ninat\\OneDrive\\Skrivebord\\Softwareteknologi\\Machine-Learning\\Afleveringer\\Aflevering1_git\\swmal_grp10\n",
      "\n",
      "14-09-2023  10:05    <DIR>          .\n",
      "08-09-2023  11:58    <DIR>          ..\n",
      "10-09-2023  16:06    <DIR>          .ipynb_checkpoints\n",
      "10-09-2023  15:45            24.815 cost_function.ipynb\n",
      "08-09-2023  13:48    <DIR>          datasets\n",
      "10-09-2023  17:22            42.754 dummy_classifier.ipynb\n",
      "10-09-2023  15:36           309.094 intro.ipynb\n",
      "10-09-2023  15:24             9.379 modules_and_classes.ipynb\n",
      "14-09-2023  10:05            75.627 O1_swmal_grp10.ipynb\n",
      "10-09-2023  16:57            17.563 performance_metrics.ipynb\n",
      "               6 File(s)        479.232 bytes\n",
      "               4 Dir(s)  111.766.945.792 bytes free\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "datapath = os.path.join(\"./datasets\", \"lifesat\", \"\")\n",
    "\n",
    "# NOTE: a ! prefix makes us able to run system commands..\n",
    "# (command 'dir' for windows, 'ls' for Linux or Macs)\n",
    "#\n",
    "\n",
    "! dir\n",
    "\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e4b029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.96242338]]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Code example\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "    gdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "except Exception as e:\n",
    "    print(f\"SWMAL NOTE: well, you need to have the 'datasets' dir in path, please unzip 'datasets.zip' and make sure that its included in the datapath='{datapath}' setting in the cell above..\")\n",
    "    raise e\n",
    "    \n",
    "# Prepare the data\n",
    "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "\n",
    "# Select a linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60234f",
   "metadata": {},
   "source": [
    "### Qa) The $\\theta$ parameters and the $R^2$ Score\n",
    "\n",
    "In this exercise we will find $\\theta0$ and $\\theta1$ from the aboves  OECD's life satisfactory score per BNP for a country. \n",
    "\n",
    "$\\theta0$ and $\\theta1$ corresponds to A and B, respectively, in the linear regression formular: Ax + B\n",
    "\n",
    "To exract these parameters using sklearn.linear_model.LinearRegression, we use the build in intercept_ and coef_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4397c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ninat\\OneDrive\\Skrivebord\\Softwareteknologi\\Machine-Learning\\Afleveringer\\Aflevering1_git\\swmal_grp10\\O1_swmal_grp10.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Make a prediction for Cyprus\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_new \u001b[39m=\u001b[39m [[\u001b[39m22587\u001b[39m]]  \u001b[39m# Cyprus' GDP per capita\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_new)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a prediction for Cyprus\n",
    "X_new = [[22587]]  # Cyprus' GDP per capita\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred) \n",
    "print()\n",
    "\n",
    "# Get Theta 0 = predict at x == 0\n",
    "t0 = model.intercept_\n",
    "print(\"Theta 0\\n\", t0)\n",
    "print()\n",
    "\n",
    "# Get Theta 1\n",
    "t1 = model.coef_\n",
    "print(\"Theta 1\\n\", t1)\n",
    "print()\n",
    "\n",
    "# Get R2\n",
    "r2 = model.score(X,y)\n",
    "\n",
    "print(\"R2 Score = \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The coefficient of determination, denoted as ùëÖ¬≤ (R-squared), measures the goodness of fit of a linear regression model to the data. It represents the proportion of the variance in the dependent variable (y) that can be explained by the independent variables (x) in the model.\n",
    "\n",
    "ùëÖ¬≤ can range from 0 to 1, and its interpretation is as follows:\n",
    "\n",
    "ùëÖ¬≤ = 0: The model explains none of the variance in the dependent variable, indicating a poor fit.\n",
    "0 < ùëÖ¬≤ < 1: The model explains some of the variance, with higher values indicating a better fit.\n",
    "ùëÖ¬≤ = 1: The model explains all of the variance, indicating a perfect fit where the observed values exactly match the predicted values.\n",
    "\n",
    "In practical applications, ùëÖ¬≤ values typically range between 0 and 1, with higher values indicating a better fit of the linear regression model to the data. However, it's important to note that a high ùëÖ¬≤ does not necessarily mean that the model is the best choice for making predictions, as it may still have other limitations such as overfitting. It's essential to consider other factors and evaluate the model's performance comprehensively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40862d90",
   "metadata": {},
   "source": [
    "## The Merits of the Fit-Predict Interface\n",
    "\n",
    "We replace the linear regression model from above with k-nearest neighbour instead. \n",
    "\n",
    "### Qb) Using k-Nearest Neighbors\n",
    "\n",
    "Change the linear regression model to a `sklearn.neighbors.KNeighborsRegressor` with k=3 (as in [HOML:p.22,bottom]), and rerun the `fit` and `predict` using this new model.\n",
    "\n",
    "What do the k-nearest neighbours estimate for Cyprus, compared to the linear regression (it should yield=5.77)?\n",
    "\n",
    "What _score-method_ does the k-nearest model use, and is it comparable to the linear regression model? \n",
    "\n",
    "Seek out the documentation in Scikit-learn, if the scoring methods are not equal, can they be compared to each other at all then?\n",
    "\n",
    "Remember to put pointer/text from the Sckikit-learn documentation in the journal...(did you find the right kNN model etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b19616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our raw data set:\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956adc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is our preprocessed data\n",
    "country_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da667467",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ninat\\OneDrive\\Skrivebord\\Softwareteknologi\\Machine-Learning\\Afleveringer\\Aflevering1_git\\swmal_grp10\\O1_swmal_grp10.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Prepare the data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ninat/OneDrive/Skrivebord/Softwareteknologi/Machine-Learning/Afleveringer/Aflevering1_git/swmal_grp10/O1_swmal_grp10.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mc_[country_stats[\u001b[39m\"\u001b[39m\u001b[39mGDP per capita\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn.neighbors\n",
    "# Prepare the data\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "print(\"X.shape=\",X.shape)\n",
    "print(\"y.shape=\",y.shape)\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "\n",
    "cyprus_gdp = 22587\n",
    "\n",
    "# Select and train a model\n",
    "knn = sklearn.neighbors.KNeighborsRegressor(3)\n",
    "\n",
    "knn.fit(X, y)\n",
    "\n",
    "cyprus_score = knn.predict([[22587]])\n",
    "score = knn.score(X, y)\n",
    "\n",
    "print(\"Cyprus life satisfaction = \", cyprus_score, \", model R2 score = \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score used in knearest neighbours regressor is the same as used in linear regression and they can be compared in how well the data fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727124f",
   "metadata": {},
   "source": [
    "### Qc) Tuning Parameter for k-Nearest Neighbors and A Sanity Check\n",
    "\n",
    "But that not the full story. Try plotting the prediction for both models in the same graph and tune the `k_neighbor` parameter of the `KNeighborsRegressor` model.  \n",
    "\n",
    "Choosing `k_neighbor=1` produces a nice `score=1`, that seems optimal...but is it really so good?\n",
    "\n",
    "Plotting the two models in a 'Life Satisfaction-vs-GDP capita' 2D plot by creating an array in the range 0 to 60000 (USD) (the `M` matrix below) and then predict the corresponding y value will sheed some light to this. \n",
    "\n",
    "Now reusing the plots stubs below, try to explain why the k-nearest neighbour with `k_neighbor=1` has such a good score.\n",
    "\n",
    "Does a score=1 with `k_neighbor=1`also mean that this would be the prefered estimator for the job?\n",
    "\n",
    "Hint here is a similar plot of a KNN for a small set of different k's:\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L01/Figs/regression_with_knn.png\"  alt=\"WARNING: could not get image from server.\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.linspace(0, 60000, 1000)\n",
    "M=np.empty([m.shape[0],1])\n",
    "M[:,0]=m\n",
    "def compare_model_plots(model_0, model_1, sample_data, plt_text):\n",
    "        sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "        plt.axis([0, 60000, 0, 10])\n",
    "\n",
    "        # create an test matrix M, with the same dimensionality as X, and in the range [0;60000] \n",
    "        # and a step size of your choice\n",
    "        \n",
    "\n",
    "        # from this test M data, predict the y values via the lin.reg. and k-nearest models\n",
    "        y_pred_lin = model_0.predict(M)\n",
    "        y_pred_knn = model_1.predict(M)   # ASSUMING the variable name 'knn' of your KNeighborsRegressor \n",
    "\n",
    "        # use plt.plot to plot x-y into the sample_data plot..\n",
    "        plt.plot(m, y_pred_lin, \"r\")\n",
    "        plt.plot(m, y_pred_knn, \"b\")\n",
    "        plt.text(5000, 8.5, plt_text, color = \"green\")\n",
    "\n",
    "def train_kmodel(dataX, dataY, neighbors):\n",
    "        model = sklearn.neighbors.KNeighborsRegressor(neighbors)\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "\n",
    "score_text = str(knn.score(X, y))\n",
    "compare_model_plots(lin1, knn, sample_data, \"K = 3, score: \" + score_text)\n",
    "knn_0 = train_kmodel(X, y, 6)\n",
    "score_text = str(knn_0.score(X, y))\n",
    "compare_model_plots(lin1, knn_0, sample_data, \"K = 6, score: \" + score_text)\n",
    "\n",
    "knn_1 = train_kmodel(X, y, 10)\n",
    "score_text = str(knn_1.score(X, y))\n",
    "compare_model_plots(lin1, knn_1, sample_data, \"K = 10, score:\" + score_text)\n",
    "\n",
    "knn_2 = train_kmodel(X, y, 1)\n",
    "score_text = str(knn_2.score(X, y))\n",
    "compare_model_plots(lin1, knn_2, sample_data, \"K = 1, score: \" + score_text)\n",
    "\n",
    "# Despite the high score, the model with the highest score is not well suited for use\n",
    "# This is because of a phenomenon called \"Overfitting\" - the system has to be able to \"generalize\" to predict new values\n",
    "# When overfit, it's too prone to changes and will completely change its predictions with even minor changes to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f7483",
   "metadata": {},
   "source": [
    "### Qd) Trying out a Neural Network\n",
    "\n",
    "Let us then try a Neural Network on the data, using the fit-predict interface allows us to replug a new model into our existing code.\n",
    "\n",
    "There are a number of different NN's available, let's just hook into Scikit-learns Multi-Layer Perceptron for regression, that is an 'MLPRegressor'. \n",
    "\n",
    "Now, the data-set for training the MLP is really not well scaled, so we need to tweak a lot of parameters in the MLP just to get it to produce some sensible output: with out preprocessing and scaling of the input data, `X`, the MLP is really a bad choice of model for the job since it so easily produces garbage output. \n",
    "\n",
    "Try training the `mlp` regression model below, predict the value for Cyprus, and find the `score` value for the training set...just as we did for the linear and KNN models.\n",
    "\n",
    "Can the `MLPRegressor` score function be compared with the linear and KNN-scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Setup MLPRegressor\n",
    "mlp = MLPRegressor( hidden_layer_sizes=(10,), solver='adam', activation='relu', tol=1E-5, max_iter=100000, verbose=True)\n",
    "mlp.fit(X, y.ravel())\n",
    "\n",
    "\n",
    "\n",
    "# lets make a MLP regressor prediction and redo the plots\n",
    "y_pred_mlp = mlp.predict(M) \n",
    "y_pred_lin = lin1.predict(M)\n",
    "y_pred_knn = knn_1.predict(M)\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    "plt.plot(m, y_pred_mlp, \"k\")\n",
    "\n",
    "cyp = mlp.predict([[22587]])\n",
    "mlp_score = mlp.score(X, y)\n",
    "\n",
    "print(\"Cyprus prediction: \", cyp)\n",
    "print(\"mlp_score : \", mlp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85acf77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08490131",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Vector and matrix representation in python\n",
    "\n",
    "Say, we have $d$ features for a given sample point. This $d$-sized feature column vector for a data-sample $i$ is then given by\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{SWMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\scriptsize #1}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "\\bx\\powni = \n",
    "    \\ac{c}{\n",
    "        x_1\\powni \\\\\n",
    "        x_2\\powni \\\\ \n",
    "        \\vdots \\\\\n",
    "        x_d\\powni\n",
    "     }  \n",
    "$$\n",
    "\n",
    "or typically written transposed to save as\n",
    "\n",
    "$$\n",
    "    \\bx\\powni = \\left[  x_1\\powni~~ x_2\\powni~~ \\cdots~~ x_d\\powni\\right]^T\n",
    "$$\n",
    "\n",
    "such that $\\bX$ can be constructed of the full set of $n$ samples of these feature vectors\n",
    "\n",
    "$$\n",
    "    \\bX = \n",
    "      \\ac{c}{\n",
    "        (\\bx\\pown{1})^T \\\\\n",
    "        (\\bx\\pown{2})^T \\\\\n",
    "        \\vdots \\\\\n",
    "        (\\bx\\pownn)^T\n",
    "      }\n",
    "$$\n",
    "\n",
    "or by explicitly writing out the full data matrix $\\bX$ consisting of scalars \n",
    "\n",
    "$$\n",
    "    \\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        }\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "but sometimes the notation is a little more fuzzy, leaving out the transpose operator for $\\mathbf x$ and in doing so just interpreting the $\\mathbf{x}^{(i)}$'s to be row vectors instead of column vectors.\n",
    "\n",
    "The target column vector, $\\mathbf y$, also has the dimension $n$ \n",
    "\n",
    "$$\n",
    "    \\by = \\ac{c}{\n",
    "            y\\pown{1} \\\\\n",
    "            y\\pown{2} \\\\\n",
    "            \\vdots \\\\\n",
    "            y\\pownn \\\\\n",
    "          }\n",
    "$$\n",
    "\n",
    "#### Qa Given the following $\\mathbf{x}^{(i)}$'s, construct and print the $\\mathbf X$ matrix in python.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "      \\bx\\pown{1} &= \\ac{c}{ 1, 2, 3}^T \\\\\n",
    "      \\bx\\pown{2} &= \\ac{c}{ 4, 2, 1}^T \\\\\n",
    "      \\bx\\pown{3} &= \\ac{c}{ 3, 8, 5}^T \\\\\n",
    "      \\bx\\pown{4} &= \\ac{c}{-9,-1, 0}^T\n",
    "    }\n",
    "$$\n",
    "\n",
    "##### Implementation Details\n",
    "\n",
    "Notice that the ```np.matrix``` class is getting deprecated! So, we use numpy's ```np.array``` as matrix container. Also, __do not__ use the built-in python lists or the numpy matrix subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qa\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([1,2,3,4]) # NOTE:  you'll need this later\n",
    "\n",
    "X = np.ndarray(shape=(4, 3))\n",
    "\n",
    "x_0 = np.array([1, 2, 3])\n",
    "x_1 = np.array([4, 2 ,1])\n",
    "x_2 = np.array([3, 8, 5])\n",
    "x_3 = np.array([-9, -1, 0])\n",
    "\n",
    "x = [x_0, x_1, x_2, x_3]\n",
    "\n",
    "for i, item in enumerate(x):\n",
    "    X[i] = item\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8179f60",
   "metadata": {},
   "source": [
    "### Norms, metrics or distances\n",
    "\n",
    "The $\\norm{2}$ Euclidian distance, or norm, for a vector of size $n$ is defined as \n",
    "\n",
    "$$\n",
    "    \\norm{2}:~~ ||\\bx||_2 = \\left( \\sum_{i=1}^{n} |x_i|^2 \\right)^{1/2}\\\\\n",
    "$$\n",
    "\n",
    "and the distance between two vectors is given by\n",
    "\n",
    "$$\n",
    "    \\ar{ll}{      \n",
    "          \\mbox{d}(\\bx,\\by) &= ||\\bx-\\by||_2\\\\\n",
    "                     &= \\left( \\sum_{i=1}^n \\left| x_{i}-y_{i} \\right|^2 \\right)^{1/2}\n",
    "    }\n",
    "$$ \n",
    "\n",
    "This Euclidian norm is sometimes also just denoted as $||\\bx||$, leaving out the 2 in the subscript.\n",
    "\n",
    "The squared $\\norm{2}$ for a vector can compactly be expressed via \n",
    "\n",
    "$$\n",
    "    \\norm{2}^2: ||\\bx||_2^2 = \\bx^\\top\\bx\n",
    "$$\n",
    "\n",
    "\n",
    "The $\\norm{1}$ 'City-block' norm is given by\n",
    "\n",
    "$$\n",
    "    \\norm{1}:~~ ||\\bx||_1 = \\sum_i |x_i|\n",
    "$$\n",
    "\n",
    "but $\\norm{1}$ is not used as intensive as its more popular $\\norm{2}$ cousin. \n",
    "\n",
    "Notice that $|x|$ in code means ```fabs(x)```.\n",
    "\n",
    "#### Qb Implement the $\\norm{1}$ and $\\norm{2}$ norms for vectors in python.\n",
    "\n",
    "First implementation must be a 'low-level'/explicit implementation---using primitive/build-in functions, like ```+```, ```*``` and power ```**``` only! The square-root function can be achieved via power like ```x**0.5```.\n",
    "\n",
    "Do NOT use any methods from libraries, like ```math.sqrt```, ```math.abs```, ```numpy.linalg.inner```, ```numpy.dot()``` or similar. Yes, using such libraries is an efficient way of building python software, but in this exercise we want to explicitly map the mathematichal formulaes to python code.\n",
    "\n",
    "Name your functions L1 and L2 respectively, they both take one vector as input argument.\n",
    "\n",
    "But test your implementation against some built-in functions, say  ```numpy.linalg.norm```\n",
    "\n",
    "When this works, and passes the tests below, optimize the $\\norm{2}$, such that it uses np.numpy's dot operator instead of an explicit sum, call this function ```L2Dot```. This implementation, ```L2Dot```, must be pythonic, i.e. it must not contain explicit for- or while-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1(x: np.array):\n",
    "    if type(x) == list:\n",
    "        x = np.array(x)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"l1: Input must be a 1 dimensional array. Actual dimensions: \" + str(x.ndim))\n",
    "    if not len(x) > 1:\n",
    "        raise ValueError(\"l1: Length of input must be bigger than 1\")\n",
    "    \n",
    "    sum = 0\n",
    "    for item in x:\n",
    "        if(item < 0):\n",
    "            item = -item\n",
    "        sum += item\n",
    "    return sum\n",
    "\n",
    "def l2(x: np.array):\n",
    "    if type(x) == list:\n",
    "        x = np.array(x)\n",
    "    if x.ndim != 1: \n",
    "        raise ValueError(\"l1: Input must be a 1 dimensional array. Actual dimensions: \" + str(x.ndim))\n",
    "    if not len(x) > 1:\n",
    "        raise ValueError(\"l1: Length of input must be bigger than 1\")\n",
    "    \n",
    "    sum_of_exp2 = 0\n",
    "    \n",
    "    for item in x:\n",
    "        if(item < 0):\n",
    "            item = -item\n",
    "        sum_of_exp2 += item**2\n",
    "    return sum_of_exp2**(1/2)\n",
    "\n",
    "def l2dot(x: np.array):\n",
    "    result = np.sqrt(np.dot(np.transpose(x), x))\n",
    "    return result\n",
    "\n",
    "# TEST vectors: here I test your implementation...calling your L1() and L2() functions\n",
    "tx=np.array([1, 2, 3, -1])\n",
    "ty=np.array([3,-1, 4,  1])\n",
    "\n",
    "expected_d1=8.0\n",
    "expected_d2=4.242640687119285\n",
    "\n",
    "d1=l1(tx-ty)\n",
    "d2=l2(tx-ty)\n",
    "\n",
    "print(f\"tx-ty={tx-ty}, d1-expected_d1={d1-expected_d1}, d2-expected_d2={d2-expected_d2}\")\n",
    "\n",
    "eps=1E-9 \n",
    "# NOTE: remember to import 'math' for fabs for the next two lines..\n",
    "\n",
    "import math\n",
    "\n",
    "assert math.fabs(d1-expected_d1)<eps, \"L1 dist seems to be wrong\" \n",
    "assert math.fabs(d2-expected_d2)<eps, \"L2 dist seems to be wrong\" \n",
    "\n",
    "print(\"OK(part-1)\")\n",
    "\n",
    "\n",
    "\n",
    "# comment-in once your L2Dot fun is ready...\n",
    "d2dot=l2dot(tx-ty)\n",
    "print(\"d2dot-expected_d2=\",d2dot-expected_d2)\n",
    "assert math.fabs(d2dot-expected_d2)<eps, \"L2Ddot dist seem to be wrong\" \n",
    "print(\"OK(part-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6511fba",
   "metadata": {},
   "source": [
    "## The cost function, $J$\n",
    "\n",
    "\n",
    "\n",
    "### Cost function in vector/matrix notation using $\\norm{2}$\n",
    "\n",
    "\n",
    "\n",
    "#### Loss or Objective Function using the Mean Squared Error\n",
    "\n",
    "This formulation is equal to the definition of the _mean-squared-error_, MSE (or indirectly also RMSE), here given in the general formulation for some random variable $Z$ \n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\mbox{MSE} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Z}_i-Z_i)^2 = \\frac{1}{n} SS\\\\\n",
    "        \\mbox{RMSE} &= \\sqrt{\\mbox{MSE}}\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "with sum-of-squares (SS) is given simply by\n",
    "\n",
    "$$\n",
    "    \\mbox{SS} = \\sum_{i=1}^{n} (\\hat{Z}_i-Z_i)^2\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "So, using the $\\norm{2}$ for the distance metric, is equal to saying that we want to minimize $J$ with respect to the MSE\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J &= \\mbox{MSE}(h(\\bX), \\by\\st{true}) \\\\\n",
    "          &= \\mbox{MSE}(\\by\\st{pred}~, \\by\\st{true}) \\\\\n",
    "          &= \\mbox{MSE}(\\hat{\\by}, \\by\\st{true})\n",
    "     }\n",
    "$$\n",
    "\n",
    "Note: when minimizing one can ignore the constant factor $1/n$ and it really does not matter if you minimize MSE or RMSE. Often $J$ is also multiplied by 1/2 to ease notation when trying to differentiate it.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J(\\bX,\\by\\st{true};\\btheta) &\\propto \\half ||\\by\\st{pred} - \\by\\st{true} ||_2^2 \\\\\n",
    "          &\\propto \\mbox{MSE}\n",
    "     }\n",
    "$$\n",
    "\n",
    "### MSE\n",
    "\n",
    "Now, let us take a look on how you calculate the MSE.\n",
    "\n",
    "The MSE uses the $\\norm{2}$ norm internally, well, actually $||\\cdot||^2_2$ to be precise, and basically just sums, means and roots the individual (scalar) losses (distances), we just saw before. \n",
    "\n",
    "And the RMSE is just an MSE with a final square-root call.\n",
    "\n",
    "### Qc Construct the Root Mean Square Error (RMSE) function (Equation 2-1 [HOML]).\n",
    "\n",
    "Call the function RMSE, and evaluate it using the $\\bX$ matrix and $\\by$ from Qa.\n",
    "\n",
    "We implement a dummy hypothesis function, that just takes the first column of $\\bX$ as its 'prediction'\n",
    "\n",
    "$$\n",
    "    h\\st{dummy}(\\bX) = \\bX(:,0)\n",
    "$$\n",
    "\n",
    "Do not re-implement the $\\norm{2}$ for the RMSE function, but call the '''L2''' function you just implemented internally in RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64522d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE2(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    if(len(y_pred) != len(y_true)):\n",
    "        raise ValueError(\"RMSE2: Inconsistent size of arrays: \" + str(len(y_pred)) + \" , \" + str(len(y_true)))\n",
    "    \n",
    "    diff_sqr = [(x - y)**2 for x, y in zip(y_pred, y_true)]\n",
    "    sos = l1(diff_sqr)\n",
    "    mse = 1/len(y_pred) * (sos)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def RMSE(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    if(len(y_pred) != len(y_true)):\n",
    "        raise ValueError(\"RMSE: Inconsistent size of arrays: \" + str(len(y_pred)) + \" , \" + str(len(y_true)))\n",
    "    \n",
    "    l2res = l2(1/2*(y_pred - y_true))\n",
    "    print(l2res)\n",
    "    rmse = np.sqrt(l2res)\n",
    "    \n",
    "    \n",
    "# Dummy h function:\n",
    "def h(X):    \n",
    "    if X.ndim!=2:\n",
    "        raise ValueError(\"excpeted X to be of ndim=2, got ndim=\",X.ndim)\n",
    "    if X.shape[0]==0 or X.shape[1]==0:\n",
    "        raise ValueError(\"X got zero data along the 0/1 axis, cannot continue\")\n",
    "    return X[:,0]\n",
    "\n",
    "# Calls your RMSE() function:\n",
    "r=RMSE2(h(X),y)\n",
    "\n",
    "# TEST vector:\n",
    "eps=1E-9\n",
    "expected=6.57647321898295\n",
    "print(f\"RMSE={r}, diff={r-expected}\")\n",
    "assert math.fabs(r-expected)<eps, \"your RMSE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17592be7",
   "metadata": {},
   "source": [
    "### MAE\n",
    "\n",
    "#### Qd Similar construct the Mean Absolute Error (MAE) function (Equation 2-2 [HOML]) and evaluate it.\n",
    "\n",
    "The MAE will algorithmic wise be similar to the MSE part from using the $\\norm{1}$ instead of the $\\norm{2}$ norm.\n",
    "\n",
    "Again, re-implementation of the$\\norm{1}$ is a no-go, call the '''L1''' instead internally i MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a32ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_pred, y_true):\n",
    "    if(len(y_pred) != len(y_true)):\n",
    "        raise ValueError(\"MAE: Inconsistent size of arrays: \" + str(len(y_pred)) + \" , \" + str(len(y_true)))\n",
    "    \n",
    "    # MAE = averaged sum of differences\n",
    "    mae = (1/len(y_pred)) * l1(y_pred - y_true)\n",
    "    return mae\n",
    "\n",
    "# Calls your MAE function:\n",
    "r=MAE(h(X), y)\n",
    "\n",
    "# TEST vector:\n",
    "expected=3.75\n",
    "print(f\"MAE={r}, diff={r-expected}\")\n",
    "assert math.fabs(r-expected)<eps, \"MAE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ee92a",
   "metadata": {},
   "source": [
    "## Pythonic Code\n",
    "\n",
    "### Robustness of Code\n",
    "\n",
    "Data validity checking is an essential part of robust code, and in Python the 'fail-fast' method is used extensively: instead of lingering on trying to get the 'best' out of an erroneous situation, the fail-fast pragma will be very loud about any data inconsistencies at the earliest possible moment.\n",
    "\n",
    "Hence robust code should include a lot of error checking, say as pre- and post-conditions (part of the design-by-contract programming) when calling a function: when entering the function you check that all parameters are ok (pre-condition), and when leaving you check the return parameter (post-conditions).  \n",
    "\n",
    "Normally assert-checking or exception-throwing will do the trick just fine, with the exception method being more _pythonic_.\n",
    "\n",
    "For the norm-function you could, for instance, test your input data to be 'vector' like, i.e. like\n",
    "\n",
    "```python\n",
    "    assert x.shape[0]>=0 and x.shape[1]==0\n",
    "    \n",
    "    if not x.ndim==1:\n",
    "        raise some error\n",
    "```\n",
    "or similar.\n",
    "\n",
    "#### Qe Robust Code \n",
    "\n",
    "Add error checking code (asserts or exceptions), that checks for right $\\hat\\by$-$\\by$ sizes of the MSE and MAE functions.\n",
    "\n",
    "Also add error checking to all you previously tested L2() and L1() functions, and re-run all your tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Qe ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424869d8",
   "metadata": {},
   "source": [
    "### Qf Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dff987",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "    The exercises above have a couple of different purposes:\n",
    "    * Get practice in programming mathematical functions in Python (Translating mathematical formulas to a Python syntax)\n",
    "    * Get practice in working with numpy arrays and basic array manipulation\n",
    "    * Get insight into the performance metrics that can be used to measure effectiveness of a prediction model (RMSE, MAE)\n",
    "    * Understand the importance of the cost function, and learn to implement different cost functions\n",
    "    \n",
    "    The last part is especially important because it feeds into the learning system: minimizing the cost function results should theoretically train \n",
    "    the machine learning model\n",
    "    \n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b87afb",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2018-12-18| CEF, initial.                  \n",
    "2019-01-31| CEF, spell checked and update. \n",
    "2019-02-04| CEF, changed d1/d2 in Qb to L1/L2. Fixe rev date error.\n",
    "2019-02-04| CEF, changed headline.\n",
    "2019-02-04| CEF, changed (.) in dist(x,y) to use pipes instead.\n",
    "2019-02-04| CEF, updated supervised learning fig, and changed , to ; for thetas, and change = to propto.\n",
    "2019-02-05| CEF, post lesson update, minor changes, added fabs around two test vectors.\n",
    "2019-02-07| CEF, updated def section. \n",
    "2019-09-01| CEF, updated for ITMAL v2.\n",
    "2019-09-04| CEF, updated for print-f and added conclusion Q.\n",
    "2019-09-05| CEF, fixed defect in print string and commented on fabs.\n",
    "2020-01-30| CEF, F20 ITMAL update.\n",
    "2020-02-03| CEF, minor text fixes.\n",
    "2020-02-24| CEF, elaborated on MAE and RMSE, emphasized not to use np functionality in L1 and L2.\n",
    "2020-09-03| CEF, E20 ITMAL update, updated figs paths.\n",
    "2020-09-06| CEF, added alt text.\n",
    "2020-09-07| CEF, updated HOML page refs.\n",
    "2021-01-12| CEF, F21 ITMAL update, moved revision table.\n",
    "2021-02-09| CEF, elaborated on test-vectors. Changed order of Design Matrix descriptions.\n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2022-02-25| CEF, removed inner product equations.\n",
    "2022-08-30| CEF, updated to v1 changes.\n",
    "2023-02-07| CEF, minor update for d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb022e",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Implementing a dummy binary-classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader from Scikit-learn. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "#### Qa  Load and display the MNIST data\n",
    "\n",
    "There is a `sklearn.datasets.fetch_openml` dataloader interface in Scikit-learn. You can load MNIST data like \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',??) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert to [0;1] via scaling (not always needed)\n",
    "#X = X / 255.\n",
    "```\n",
    "\n",
    "but you need to set parameters like `return_X_y` and `cache` if the default values are not suitable! \n",
    "\n",
    "Check out the documentation for the `fetch_openml` MNIST loader, try it out by loading a (X,y) MNIST data set, and plot a single digit via the `MNIST_PlotDigit` function here (input data is a 28x28 NMIST subimage)\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "```\n",
    "\n",
    "Finally, put the MNIST loader into a single function called `MNIST_GetDataSet()` so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "# mnist 784 dataset has id == 554: https://www.openml.org/search?type=data&status=active&id=554\n",
    "\n",
    "def MNIST_GetDataSet() -> pd.DataFrame:\n",
    "    X, y = fetch_openml(version=\"active\", data_id=554, return_X_y=True)\n",
    "    return X, y\n",
    "\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = MNIST_GetDataSet()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Test drawing from data. pd.DataFrame.loc[x] allows indexing pandas rows. reshape() is a numpy method, so convert to numpy array first\n",
    "MNIST_PlotDigit(X.loc[15].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6be1b",
   "metadata": {},
   "source": [
    "#### Qb  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOML], p.103.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.(We will be looking at cross-validation instead of the simple fit-predict in a later exercise.)\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader. In effect, we have now created a binary-classifier, that enable us to classify a particular data sample, $\\mathbf{x}(i)$ (that is a 28x28 image), as being a-class-5 or not-a-class-5. \n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "print(X.shape)\n",
    "print(y_train.dtype)\n",
    "y_train_5 = (y_train == '5')\n",
    "y_test_5 = (y_test == '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create a SGD classifier model and train it (fit)\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795af3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "sgd_predictions = sgd_clf.predict(X_test)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(sgd_predictions))\n",
    "# Get indices of True predictions to draw them later\n",
    "indices = []\n",
    "for index, val in enumerate(sgd_predictions):\n",
    "    if(val == True):\n",
    "        indices.append(index)\n",
    "\n",
    "# Classified this many numbers as '5'\n",
    "print(len(indices))\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5751b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PlotDigit(X_test.iloc[indices[0]].to_numpy())\n",
    "print(y_test_5.iloc[indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PlotDigit(X_test.iloc[indices[1]].to_numpy())\n",
    "print(y_test_5.iloc[indices[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PlotDigit(X_test.iloc[indices[2]].to_numpy())\n",
    "print(y_test_5.iloc[indices[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PlotDigit(X_test.iloc[indices[3]].to_numpy())\n",
    "print(y_test_5.iloc[indices[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified as 5\n",
    "MNIST_PlotDigit(X_test.iloc[indices[6]].to_numpy())\n",
    "print(y_test_5.iloc[indices[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified\n",
    "MNIST_PlotDigit(X_test.iloc[indices[8]].to_numpy())\n",
    "print(y_test_5.iloc[indices[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_PlotDigit(X_test.iloc[indices[13]].to_numpy())\n",
    "print(y_test_5.iloc[indices[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2ea7f",
   "metadata": {},
   "source": [
    "#### Qc Implement a dummy binary classifier\n",
    "\n",
    "Now we will try to create a Scikit-learn compatible estimator implemented via a python class. Follow the code found in [HOML], p.107 (for [HOML] 1st and 2nd editions: name you estimator `DummyClassifier` instead of `Never5Classifyer`).\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L02/Figs/class_base_estimator.png\" alt=\"WARNING: could not get image from server.\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit from `BaseEstimator` (and possibly also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duck typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and find a way to extract the accuracy `score` from the test data. You may implement an accuracy function yourself or just use the `sklearn.metrics.accuracy_score` function. \n",
    "\n",
    "Finally, compare the accuracy score from your `DummyClassifier` with the scores found in [HOML] \"Measuring Accuracy Using Cross-Validation\", p.107. Are they comparable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f26fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Classifies EVERYTHING in the most frequent class (here: not 5 because most of the images are not 5)\n",
    "dummy_clf = DummyClassifier()\n",
    "dummy_clf.fit(X_train, y_train_5)\n",
    "dummy_y = dummy_clf.predict(X_test)\n",
    "\n",
    "# Uses 'jaccard score' function, see: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "a_s_dummy = accuracy_score(y_test_5, dummy_y)\n",
    "\n",
    "c_v_s_dummy = cross_val_score(dummy_clf, X_train, y_train_5, cv = 3, scoring=\"accuracy\")\n",
    "print(a_s_dummy)\n",
    "print(c_v_s_dummy)\n",
    "# Scores are comparable. Scores are 0.91 because roughly 0.09 of images contain number '5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471343e",
   "metadata": {},
   "source": [
    "### Qd Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\"\"\n",
    "    These exercises are our first dive into the concept of using machine learning for classifications. The main purpose of the exercises are:\n",
    "    * Learn the basics of using api to download data from OpenML\n",
    "    * Split data into training and test set\n",
    "    * Use Stochastic Gradient Classifier to classify a dataset: our first binary classificator\n",
    "    * Train on training set, test on test set\n",
    "    * Notice that while the classifications will work in many cases, there will be mistakes both in false positives and false negatives\n",
    "    * Use dummy classifier, understand the scores from sklearn.metrics.accuracy_score() and sklearn.model_selection.cross_val_score\n",
    "    * Understand that the relatively high score is connected to low occurence of classified value, NOT the high accuracy of a machine learning model\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797e657",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "\n",
    "\n",
    "### Nomenclature\n",
    "\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "\n",
    "### Recall or Sensitivity\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "\n",
    "#### Accuracy Paradox\n",
    "\n",
    "\n",
    "\n",
    "### F-score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "with say element $c_{21}$ being the number of actual classes '1' being predicted (erroneously) as class '2'.\n",
    "\n",
    "### Nomenclature for the Confusion Matrix\n",
    "\n",
    "\n",
    "\n",
    "#### Qa Implement the Accuracy function and test it on the MNIST data.\n",
    "\n",
    "We now follow the convention in Scikit-learn, that a score funtion takes the arguments `y_true` and then `y_pred`\n",
    "\n",
    "```\n",
    "    sklearn.metrics.accuracy_score(y_true, y_pred, ..)\n",
    "```\n",
    "\n",
    "Implement a general accuracy function `MyAccuracy(y_true, y_pred)`. Again, implement the function you self from scratch, i.e. do not use any helper functions from Scikit-learn (implementing via `sklearn.metrics.confusion_matrix` is also not allowed, othewise you will then learn nothing!)\n",
    "\n",
    "Reuse your MNIST data loader and test the `MyAccuracy` function  both on your dummy classifier and on the Stochastic Gradient Descent classifier (with setup parameters as in [HOML]).\n",
    "\n",
    "Compare your accuracy score with the acutal value from `sklearn.metrics.accuracy_score()`.\n",
    "\n",
    "(Implementation note: what do you do, if the denominator is zero?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27565158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyAccuracy(y_true, y_pred):\n",
    "    if len(y_true) == 0: # in case the y_true is empty\n",
    "        return 0;\n",
    "    # Calculate the accuracy by comparing y_true and y_pred\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    accuracy = correct / total\n",
    "    return accuracy  \n",
    "    \n",
    "# TEST FUNCTION: example of a comperator, using Scikit-learn accuracy_score\n",
    "def TestAccuracy(y_true, y_pred):\n",
    "    a0=MyAccuracy(y_true, y_pred)\n",
    "    a1=accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nmy a          ={a0}\")\n",
    "    print(f\"scikit-learn a={a1}\")\n",
    "    \n",
    "TestAccuracy(y_test_5, sgd_predictions)\n",
    "TestAccuracy(y_test_5, dummy_y)\n",
    "\n",
    "#    # do some numerical comparison here, like\n",
    "#    #  if fabs(a0-a1)<eps then .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7df6d",
   "metadata": {},
   "source": [
    "#### Qb Implement Precision, Recall and $F_1$-score and test it on the MNIST data for both the SGD and Dummy classifier models\n",
    "\n",
    "Now, implement the `MyPrecision`, `MyRecall` and `MyF1Score` functions, again taking MNIST as input, using the SGD and the Dummy classifiers and make some test vectors to compare to the functions found in Scikit-learn...\n",
    "\n",
    "(Implementation note: as before, what do you do, if the denominator is zero?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyPrecision(y_true, y_pred):\n",
    "     # Check if the lengths of y_true and y_pred are the same\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays y_true and y_pred must have the same length.\")\n",
    "    \n",
    "    # Counter\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "     # Iterate through y_true and y_pred and find true positives and false positives\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label == 1 and pred_label == 1:\n",
    "            true_positives += 1 \n",
    "        elif true_label == 0 and pred_label == 1:\n",
    "            false_positives += 1\n",
    "    \n",
    "    # Handle case if denominator is zero\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    else:\n",
    "        # Calculate precision\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        return precision\n",
    "    \n",
    "\n",
    "def MyRecall(y_true, y_pred):\n",
    "     # Same check as in MyPrecision\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays y_true and y_pred must have the same length.\")\n",
    "        \n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    # Iterate through each pair of true and predicted labels\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label == 1 and pred_label == 1:\n",
    "            true_positives += 1\n",
    "        elif true_label == 1 and pred_label == 0:\n",
    "            false_negatives += 1\n",
    "            \n",
    "    # Handle case if denominator is zero\n",
    "    if true_positives + false_negatives == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        # Calculate recall as the ratio of true positives to (true positives + false negatives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        return recall\n",
    "    \n",
    "def MyF1Score(y_true, y_pred):\n",
    "     # Calculate precision \n",
    "    precision = MyPrecision(y_true, y_pred)\n",
    "    # Calculate recall\n",
    "    recall = MyRecall(y_true, y_pred)\n",
    "\n",
    "    # Handle the case when both precision and recall are zero\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        # Calculate F1-score as the harmonic mean of precision and recall\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score\n",
    "    \n",
    "# TEST FUNCTIONS\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def TestPrecision(y_true, y_pred):\n",
    "    a0=MyPrecision(y_true, y_pred)\n",
    "    a1=precision_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nmy p          ={a0}\")\n",
    "    print(f\"scikit-learn p={a1}\")\n",
    "    \n",
    "def TestRecall(y_true, y_pred):\n",
    "    a0=MyRecall(y_true, y_pred)\n",
    "    a1=recall_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nmy r          ={a0}\")\n",
    "    print(f\"scikit-learn r={a1}\")\n",
    "    \n",
    "def TestF1Score(y_true, y_pred):\n",
    "    a0=MyF1Score(y_true, y_pred)\n",
    "    a1=f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nmy f1          ={a0}\")\n",
    "    print(f\"scikit-learn f1={a1}\")\n",
    "    \n",
    "# TEST CODE SGD\n",
    "TestPrecision(y_test_5, sgd_predictions)\n",
    "TestRecall(y_test_5, sgd_predictions)\n",
    "TestF1Score(y_test_5,sgd_predictions)\n",
    "\n",
    "# TEST CODE DUMMY\n",
    "TestPrecision(y_test_5, dummy_y)\n",
    "TestRecall(y_test_5, dummy_y)\n",
    "TestF1Score(y_test_5,dummy_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97829cc7",
   "metadata": {},
   "source": [
    "#### Qc The Confusion Matrix\n",
    "\n",
    "Revisit your solution to Qb in the `dummy_classifier.ipynb`. Generate the confusion matrix for both the Dummy and the SGD classifier using the `scklearn.metrics.confusion_matrix` function. \n",
    "\n",
    "I got the two confusion matrices\n",
    "\n",
    "```\n",
    "M_dummy=[[18166     0]\n",
    "        [ 1834     0]]\n",
    "   \n",
    "M_SDG=[[17618   548]\n",
    "      [  267  1567]]\n",
    "\n",
    "```\n",
    "your data may look similar (but not 100% equal).\n",
    "\n",
    "How are the Scikit-learn confusion matrix organized, where are the TP, FP, FN and TN located in the matrix indices, and what happens if you mess up the parameters calling\n",
    "\n",
    "```python\n",
    "confusion_matrix(y_test_5_pred, y_test5)\n",
    "```\n",
    "\n",
    "instead of \n",
    "```python\n",
    "confusion_matrix(y_test_5, y_test_5_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Calling the function the right way\n",
    "con_matrix = confusion_matrix(y_test_5, y_test_5_pred)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e04f5",
   "metadata": {},
   "source": [
    "#### Qd A Confusion Matrix Heat-map\n",
    "\n",
    "Generate a _heat map_ image for the confusion matrices, `M_dummy` and `M_SGD` respectively, getting inspiration from [HOML] \"Error Analysis\", pp.122-125.\n",
    "\n",
    "This heat map could be an important guide for you when analysing multiclass data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd\n",
    "assert False, \"TODO: solve Qd, and remove me.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354b639",
   "metadata": {},
   "source": [
    "### Qe Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca82b3",
   "metadata": {},
   "source": [
    "We have dipped into the fundementals of performance metrics and the utitlity of the confusion matrix in assessing the effectiveness of machine learning models. We implemented and testet custom function for the four key metrics, which have provided a deeper understanding of their computation.\n",
    "\n",
    "These metrics are essential tools for model evaluation, allowing us to asses not only the overall predictive power of a model(accuracy) but also its ability to avoid false positives(precision), its capacity to capture all relevant instances(recall) and the harmonious balance between precesion and recall(f1 score).\n",
    "\n",
    "Through this exploration, we have gained valuable insights into the nuances of model performance evaluation, empowering us to make informed decisions and refine our machine learning models effectively.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bace07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
