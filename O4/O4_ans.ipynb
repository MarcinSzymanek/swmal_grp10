{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right ML algorithms\n",
    "\n",
    "To be able to choose the right machine learning models, we need to formulate a clear and concise description of our problem. We would like to predict cancer malignancy (diagnosis) by looking at the size of cell features from images of cell substracts. The features of the cells concern the mean texture, mean radius, area, concavity and other geometrical features of the cells themself. With that said, the task of predicting whether a cancer diagnosis is malignant or benign, is clearly a classification task. \n",
    "\n",
    "Since in our dataset we already have the labeled diagnosis, it seems prudent to test some of the supervised models which scikit-learn offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 33)\n",
      "33\n",
      "Data features: \n",
      " Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
      "      dtype='object')\n",
      "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
      "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
      "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
      "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
      "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
      "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
      "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
      "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
      "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
      "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
      "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
      "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
      "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
      "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
      "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
      "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
      "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
      "\n",
      "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "count        569.000000         569.000000       569.000000   \n",
      "mean           0.132369           0.254265         0.272188   \n",
      "std            0.022832           0.157336         0.208624   \n",
      "min            0.071170           0.027290         0.000000   \n",
      "25%            0.116600           0.147200         0.114500   \n",
      "50%            0.131300           0.211900         0.226700   \n",
      "75%            0.146000           0.339100         0.382900   \n",
      "max            0.222600           1.058000         1.252000   \n",
      "\n",
      "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
      "count            569.000000      569.000000               569.000000   \n",
      "mean               0.114606        0.290076                 0.083946   \n",
      "std                0.065732        0.061867                 0.018061   \n",
      "min                0.000000        0.156500                 0.055040   \n",
      "25%                0.064930        0.250400                 0.071460   \n",
      "50%                0.099930        0.282200                 0.080040   \n",
      "75%                0.161400        0.317900                 0.092080   \n",
      "max                0.291000        0.663800                 0.207500   \n",
      "\n",
      "       Unnamed: 32  \n",
      "count          0.0  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read dataset\n",
    "data_breast_cancer = pd.read_csv(\"breast_cancer_win/data.csv\")\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "print(data_breast_cancer.shape)\n",
    "features = data_breast_cancer.columns\n",
    "print(len(features))\n",
    "\n",
    "print(\"Data features: \\n\", features)\n",
    "print(data_breast_cancer.describe())\n",
    "\n",
    "# Prepare the dataset\n",
    "\n",
    "means_labels : list[str] = list(data_breast_cancer.columns[1:11])\n",
    "worst_labels : list[str] = list(data_breast_cancer.columns[-10: -1])\n",
    "\n",
    "# Drop the faulty column\n",
    "data_breast_cancer.drop(columns=[\"Unnamed: 32\", \"id\"], inplace=True)\n",
    "\n",
    "# Set the data diagnosis results to integers\n",
    "data_breast_cancer['diagnosis'] = data_breast_cancer['diagnosis'].map({\"M\":1,\"B\":0})\n",
    "# split dataframe into two based on diagnosis\n",
    "dfM=data_breast_cancer[data_breast_cancer['diagnosis'] == 1]\n",
    "dfB=data_breast_cancer[data_breast_cancer['diagnosis'] == 0]\n",
    "\n",
    "# Copy result array and drop it from our dataset\n",
    "y_all = data_breast_cancer[\"diagnosis\"].copy()\n",
    "data_breast_cancer.drop(columns=[\"diagnosis\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means: \n",
      "    radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   fractal_dimension_mean  \n",
      "0                 0.07871  \n",
      "1                 0.05667  \n",
      "2                 0.05999  \n",
      "3                 0.09744  \n",
      "4                 0.05883   \n",
      "\n",
      "Worst: \n",
      "    radius_worst  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0         25.38          17.33           184.60      2019.0            0.1622   \n",
      "1         24.99          23.41           158.80      1956.0            0.1238   \n",
      "2         23.57          25.53           152.50      1709.0            0.1444   \n",
      "3         14.91          26.50            98.87       567.7            0.2098   \n",
      "4         22.54          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \n",
      "0             0.6656           0.7119                0.2654          0.4601  \n",
      "1             0.1866           0.2416                0.1860          0.2750  \n",
      "2             0.4245           0.4504                0.2430          0.3613  \n",
      "3             0.8663           0.6869                0.2575          0.6638  \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "SE: \n",
      "    radius_se  texture_se  perimeter_se  area_se  smoothness_se  \\\n",
      "0     1.0950      0.9053         8.589   153.40       0.006399   \n",
      "1     0.5435      0.7339         3.398    74.08       0.005225   \n",
      "2     0.7456      0.7869         4.585    94.03       0.006150   \n",
      "3     0.4956      1.1560         3.445    27.23       0.009110   \n",
      "4     0.7572      0.7813         5.438    94.44       0.011490   \n",
      "\n",
      "   compactness_se  concavity_se  concave points_se  symmetry_se  \\\n",
      "0         0.04904       0.05373            0.01587      0.03003   \n",
      "1         0.01308       0.01860            0.01340      0.01389   \n",
      "2         0.04006       0.03832            0.02058      0.02250   \n",
      "3         0.07458       0.05661            0.01867      0.05963   \n",
      "4         0.02461       0.05688            0.01885      0.01756   \n",
      "\n",
      "   fractal_dimension_se  \n",
      "0              0.006193  \n",
      "1              0.003532  \n",
      "2              0.004571  \n",
      "3              0.009208  \n",
      "4              0.005115   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 3 categories: models can perform differently based on which data we feed them!\n",
    "data_breast_means = data_breast_cancer[data_breast_cancer.columns[0: 10]].copy()\n",
    "data_breast_worst = data_breast_cancer[data_breast_cancer.columns[-10: -1]].copy()\n",
    "data_breast_se = data_breast_cancer[data_breast_cancer.columns[10: -10]].copy()\n",
    "print(\"Means: \\n\", data_breast_means.head(), \"\\n\")\n",
    "print(\"Worst: \\n\", data_breast_worst.head(), \"\\n\")\n",
    "print(\"SE: \\n\", data_breast_se.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train + test set using KFold strategy. But really?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from python_src.sandbox import FullReport\n",
    "from time import time\n",
    "\n",
    "# Make a new kfold split and train/test a model\n",
    "def SplitWithKFold(fit_fun):\n",
    "    # Default splits into 5 folds\n",
    "    kf = KFold()\n",
    "    print(kf)\n",
    "    kfsplit = kf.split(data_breast_cancer)\n",
    "\n",
    "    i = 0\n",
    "    for idx_train, idx_test in kfsplit:\n",
    "        print(\"Test set: \", idx_test[0], \" to: \", idx_test[-1])\n",
    "        X_train, y_train = data_breast_cancer.iloc[idx_train], y_all[idx_train]\n",
    "        X_test, y_test = data_breast_cancer.iloc[idx_test], y_all[idx_test]\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "\n",
    "\n",
    "        # Train model\n",
    "\n",
    "# Just a container for the data\n",
    "class Data:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test) -> None:\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_breast_cancer, y_all, test_size = 0.25, shuffle=True)\n",
    "data = Data(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "def doGridSearch(data: Data, model, tuning_parameters: dict):\n",
    "\n",
    "    def trainAndReport(gridcv, data: Data):\n",
    "        start = time()\n",
    "        gridcv.fit(data.x_train, data.y_train)\n",
    "        t = time() - start\n",
    "\n",
    "        b0, m0 = FullReport(gridcv, data.x_test, data.y_test, t)\n",
    "        print('OK(grid-search)')\n",
    "        return b0, m0\n",
    "\n",
    "    grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV_layers,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "    scores = trainAndReport(grid_tuned, data)\n",
    "\n",
    "    return scores\n",
    "\n",
    "CV_layers = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SGDClassifier using GridSearchCV just as we did in O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n",
      "SEARCH TIME: 496.00 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\tbest 'f1_micro' score=0.9109165526675786\n",
      "\tbest index=8\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(max_iter=100000, penalty='elasticnet', tol=0.0001)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.826 (+/-0.197) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[ 1]: 0.836 (+/-0.192) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 2]: 0.890 (+/-0.064) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 3]: 0.866 (+/-0.116) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 4]: 0.833 (+/-0.076) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[ 5]: 0.887 (+/-0.070) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[ 6]: 0.887 (+/-0.060) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[ 7]: 0.883 (+/-0.065) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[ 8]: 0.911 (+/-0.070) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[ 9]: 0.820 (+/-0.252) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[10]: 0.852 (+/-0.053) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[11]: 0.873 (+/-0.058) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[12]: 0.864 (+/-0.124) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[13]: 0.840 (+/-0.205) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[14]: 0.836 (+/-0.116) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[15]: 0.878 (+/-0.104) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[16]: 0.862 (+/-0.078) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[17]: 0.847 (+/-0.084) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[18]: 0.897 (+/-0.079) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[19]: 0.876 (+/-0.032) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[20]: 0.894 (+/-0.084) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[21]: 0.894 (+/-0.073) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[22]: 0.887 (+/-0.062) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[23]: 0.859 (+/-0.059) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[24]: 0.803 (+/-0.215) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[25]: 0.778 (+/-0.292) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[26]: 0.843 (+/-0.178) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[27]: 0.852 (+/-0.118) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[28]: 0.887 (+/-0.092) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[29]: 0.871 (+/-0.084) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[30]: 0.890 (+/-0.068) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[31]: 0.911 (+/-0.085) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[32]: 0.838 (+/-0.123) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[33]: 0.843 (+/-0.084) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[34]: 0.746 (+/-0.292) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[35]: 0.810 (+/-0.224) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[36]: 0.883 (+/-0.116) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[37]: 0.824 (+/-0.181) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[38]: 0.871 (+/-0.083) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[39]: 0.899 (+/-0.081) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[40]: 0.864 (+/-0.110) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[41]: 0.890 (+/-0.063) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[42]: 0.892 (+/-0.093) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[43]: 0.880 (+/-0.069) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[44]: 0.885 (+/-0.072) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[45]: 0.840 (+/-0.073) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[46]: 0.814 (+/-0.170) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[47]: 0.857 (+/-0.100) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[48]: 0.854 (+/-0.138) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[49]: 0.845 (+/-0.139) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[50]: 0.871 (+/-0.131) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[51]: 0.852 (+/-0.064) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[52]: 0.899 (+/-0.065) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[53]: 0.876 (+/-0.076) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[54]: 0.869 (+/-0.055) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[55]: 0.901 (+/-0.081) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[56]: 0.866 (+/-0.100) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[57]: 0.864 (+/-0.134) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[58]: 0.850 (+/-0.095) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[59]: 0.885 (+/-0.046) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[60]: 0.864 (+/-0.069) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[61]: 0.902 (+/-0.091) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[62]: 0.847 (+/-0.091) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[63]: 0.866 (+/-0.069) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[64]: 0.869 (+/-0.085) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[65]: 0.892 (+/-0.101) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[66]: 0.876 (+/-0.062) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[67]: 0.883 (+/-0.066) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[68]: 0.868 (+/-0.104) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[69]: 0.880 (+/-0.085) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[70]: 0.838 (+/-0.147) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[71]: 0.859 (+/-0.117) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[72]: 0.897 (+/-0.023) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[73]: 0.885 (+/-0.069) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[74]: 0.838 (+/-0.248) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[75]: 0.878 (+/-0.106) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[76]: 0.831 (+/-0.184) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[77]: 0.897 (+/-0.057) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[78]: 0.843 (+/-0.068) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[79]: 0.906 (+/-0.077) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[80]: 0.864 (+/-0.097) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[81]: 0.850 (+/-0.128) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[82]: 0.808 (+/-0.234) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[83]: 0.854 (+/-0.198) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[84]: 0.859 (+/-0.079) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[85]: 0.876 (+/-0.116) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[86]: 0.749 (+/-0.327) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[87]: 0.892 (+/-0.090) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[88]: 0.880 (+/-0.113) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[89]: 0.902 (+/-0.081) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[90]: 0.890 (+/-0.081) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[91]: 0.890 (+/-0.057) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[92]: 0.883 (+/-0.107) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[93]: 0.787 (+/-0.153) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[94]: 0.819 (+/-0.215) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[95]: 0.880 (+/-0.062) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[96]: 0.558 (+/-0.198) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[97]: 0.474 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[98]: 0.529 (+/-0.252) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[99]: 0.524 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[100]: 0.526 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[101]: 0.472 (+/-0.238) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[102]: 0.376 (+/-0.004) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[103]: 0.561 (+/-0.191) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[104]: 0.476 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[105]: 0.591 (+/-0.225) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[106]: 0.591 (+/-0.224) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[107]: 0.575 (+/-0.199) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[108]: 0.526 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[109]: 0.523 (+/-0.249) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[110]: 0.510 (+/-0.282) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[111]: 0.524 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[112]: 0.474 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[113]: 0.425 (+/-0.199) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[114]: 0.524 (+/-0.244) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[115]: 0.467 (+/-0.256) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[116]: 0.436 (+/-0.238) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[117]: 0.460 (+/-0.271) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[118]: 0.516 (+/-0.233) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[119]: 0.611 (+/-0.280) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[120]: 0.472 (+/-0.238) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[121]: 0.575 (+/-0.199) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[122]: 0.625 (+/-0.323) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[123]: 0.587 (+/-0.215) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[124]: 0.493 (+/-0.227) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[125]: 0.425 (+/-0.199) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[126]: 0.457 (+/-0.307) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[127]: 0.423 (+/-0.201) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[128]: 0.415 (+/-0.400) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[129]: 0.488 (+/-0.267) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[130]: 0.526 (+/-0.244) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[131]: 0.481 (+/-0.266) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[132]: 0.604 (+/-0.255) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[133]: 0.641 (+/-0.367) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[134]: 0.609 (+/-0.280) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[135]: 0.425 (+/-0.199) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[136]: 0.528 (+/-0.416) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[137]: 0.444 (+/-0.274) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[138]: 0.432 (+/-0.194) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[139]: 0.430 (+/-0.375) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[140]: 0.476 (+/-0.244) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[141]: 0.476 (+/-0.244) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[142]: 0.613 (+/-0.254) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[143]: 0.458 (+/-0.210) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[144]: 0.815 (+/-0.169) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[145]: 0.819 (+/-0.199) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[146]: 0.873 (+/-0.096) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[147]: 0.800 (+/-0.204) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[148]: 0.894 (+/-0.077) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[149]: 0.911 (+/-0.082) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[150]: 0.880 (+/-0.097) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[151]: 0.887 (+/-0.096) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[152]: 0.850 (+/-0.164) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[153]: 0.854 (+/-0.115) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[154]: 0.887 (+/-0.091) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[155]: 0.894 (+/-0.065) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[156]: 0.873 (+/-0.090) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[157]: 0.756 (+/-0.274) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[158]: 0.854 (+/-0.164) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[159]: 0.786 (+/-0.210) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[160]: 0.880 (+/-0.080) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[161]: 0.866 (+/-0.071) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[162]: 0.906 (+/-0.089) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[163]: 0.869 (+/-0.078) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[164]: 0.847 (+/-0.128) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[165]: 0.873 (+/-0.070) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[166]: 0.876 (+/-0.114) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[167]: 0.883 (+/-0.094) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[168]: 0.883 (+/-0.096) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[169]: 0.876 (+/-0.074) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[170]: 0.843 (+/-0.128) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[171]: 0.908 (+/-0.045) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[172]: 0.899 (+/-0.087) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[173]: 0.906 (+/-0.065) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[174]: 0.908 (+/-0.056) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[175]: 0.880 (+/-0.087) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[176]: 0.901 (+/-0.066) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[177]: 0.725 (+/-0.219) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[178]: 0.840 (+/-0.189) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[179]: 0.859 (+/-0.084) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[180]: 0.878 (+/-0.118) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[181]: 0.817 (+/-0.153) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[182]: 0.904 (+/-0.079) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[183]: 0.838 (+/-0.103) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[184]: 0.878 (+/-0.101) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[185]: 0.901 (+/-0.096) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[186]: 0.899 (+/-0.066) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[187]: 0.899 (+/-0.082) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[188]: 0.906 (+/-0.068) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[189]: 0.869 (+/-0.091) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[190]: 0.852 (+/-0.068) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[191]: 0.871 (+/-0.095) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[192]: 0.876 (+/-0.041) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[193]: 0.838 (+/-0.188) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[194]: 0.793 (+/-0.269) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[195]: 0.887 (+/-0.070) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[196]: 0.885 (+/-0.116) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[197]: 0.892 (+/-0.103) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[198]: 0.894 (+/-0.090) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[199]: 0.885 (+/-0.093) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[200]: 0.878 (+/-0.106) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[201]: 0.836 (+/-0.145) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[202]: 0.838 (+/-0.096) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[203]: 0.854 (+/-0.063) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[204]: 0.859 (+/-0.107) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[205]: 0.838 (+/-0.067) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[206]: 0.800 (+/-0.108) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[207]: 0.840 (+/-0.066) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[208]: 0.897 (+/-0.054) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[209]: 0.887 (+/-0.090) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[210]: 0.894 (+/-0.073) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[211]: 0.897 (+/-0.080) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[212]: 0.836 (+/-0.192) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[213]: 0.819 (+/-0.225) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[214]: 0.869 (+/-0.052) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[215]: 0.770 (+/-0.241) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[216]: 0.840 (+/-0.214) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[217]: 0.878 (+/-0.050) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[218]: 0.892 (+/-0.085) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[219]: 0.869 (+/-0.085) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[220]: 0.883 (+/-0.054) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[221]: 0.901 (+/-0.092) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[222]: 0.892 (+/-0.053) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[223]: 0.899 (+/-0.075) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[224]: 0.843 (+/-0.157) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[225]: 0.819 (+/-0.205) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[226]: 0.866 (+/-0.126) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[227]: 0.852 (+/-0.074) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[228]: 0.880 (+/-0.074) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[229]: 0.786 (+/-0.178) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[230]: 0.876 (+/-0.068) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[231]: 0.814 (+/-0.228) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[232]: 0.880 (+/-0.084) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[233]: 0.892 (+/-0.083) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[234]: 0.892 (+/-0.085) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[235]: 0.897 (+/-0.093) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[236]: 0.876 (+/-0.038) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[237]: 0.805 (+/-0.245) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[238]: 0.873 (+/-0.086) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[239]: 0.883 (+/-0.149) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[240]: 0.575 (+/-0.199) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[241]: 0.477 (+/-0.249) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[242]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[243]: 0.577 (+/-0.201) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[244]: 0.570 (+/-0.195) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[245]: 0.406 (+/-0.229) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[246]: 0.420 (+/-0.203) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[247]: 0.406 (+/-0.123) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[248]: 0.478 (+/-0.250) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[249]: 0.577 (+/-0.201) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[250]: 0.524 (+/-0.244) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[251]: 0.370 (+/-0.460) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[252]: 0.561 (+/-0.335) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[253]: 0.573 (+/-0.208) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[254]: 0.618 (+/-0.301) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[255]: 0.376 (+/-0.004) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[256]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[257]: 0.523 (+/-0.249) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[258]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[259]: 0.533 (+/-0.262) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[260]: 0.471 (+/-0.243) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[261]: 0.469 (+/-0.257) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[262]: 0.669 (+/-0.178) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[263]: 0.376 (+/-0.004) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[264]: 0.575 (+/-0.199) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[265]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[266]: 0.476 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[267]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[268]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[269]: 0.608 (+/-0.265) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[270]: 0.455 (+/-0.203) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[271]: 0.423 (+/-0.201) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[272]: 0.478 (+/-0.250) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[273]: 0.526 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[274]: 0.526 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[275]: 0.554 (+/-0.315) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[276]: 0.425 (+/-0.199) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[277]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[278]: 0.570 (+/-0.217) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[279]: 0.526 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[280]: 0.373 (+/-0.009) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[281]: 0.624 (+/-0.004) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[282]: 0.521 (+/-0.236) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[283]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[284]: 0.580 (+/-0.204) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[285]: 0.366 (+/-0.037) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[286]: 0.458 (+/-0.266) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[287]: 0.474 (+/-0.244) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[288]: 0.771 (+/-0.297) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[289]: 0.843 (+/-0.239) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[290]: 0.800 (+/-0.206) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[291]: 0.866 (+/-0.084) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[292]: 0.887 (+/-0.088) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[293]: 0.908 (+/-0.104) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[294]: 0.880 (+/-0.079) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[295]: 0.859 (+/-0.068) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[296]: 0.871 (+/-0.085) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[297]: 0.850 (+/-0.218) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[298]: 0.839 (+/-0.231) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[299]: 0.852 (+/-0.069) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[300]: 0.883 (+/-0.097) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[301]: 0.866 (+/-0.061) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[302]: 0.861 (+/-0.097) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[303]: 0.852 (+/-0.161) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[304]: 0.892 (+/-0.092) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[305]: 0.880 (+/-0.095) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[306]: 0.892 (+/-0.100) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[307]: 0.883 (+/-0.086) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[308]: 0.843 (+/-0.177) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[309]: 0.892 (+/-0.031) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[310]: 0.871 (+/-0.121) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[311]: 0.897 (+/-0.075) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[312]: 0.833 (+/-0.224) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[313]: 0.829 (+/-0.220) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[314]: 0.880 (+/-0.089) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[315]: 0.854 (+/-0.075) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[316]: 0.873 (+/-0.069) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[317]: 0.873 (+/-0.090) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[318]: 0.899 (+/-0.094) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[319]: 0.880 (+/-0.092) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[320]: 0.873 (+/-0.088) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[321]: 0.815 (+/-0.304) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[322]: 0.838 (+/-0.172) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[323]: 0.864 (+/-0.142) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[324]: 0.850 (+/-0.148) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[325]: 0.908 (+/-0.083) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[326]: 0.845 (+/-0.110) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[327]: 0.819 (+/-0.114) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[328]: 0.892 (+/-0.096) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[329]: 0.901 (+/-0.108) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[330]: 0.894 (+/-0.085) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[331]: 0.887 (+/-0.085) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[332]: 0.833 (+/-0.253) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[333]: 0.831 (+/-0.192) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[334]: 0.774 (+/-0.203) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[335]: 0.890 (+/-0.057) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[336]: 0.885 (+/-0.093) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[337]: 0.859 (+/-0.109) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[338]: 0.892 (+/-0.047) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[339]: 0.782 (+/-0.321) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[340]: 0.885 (+/-0.091) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[341]: 0.887 (+/-0.077) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[342]: 0.901 (+/-0.084) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[343]: 0.894 (+/-0.100) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[344]: 0.829 (+/-0.139) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[345]: 0.803 (+/-0.143) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[346]: 0.862 (+/-0.099) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[347]: 0.742 (+/-0.300) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[348]: 0.866 (+/-0.124) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[349]: 0.864 (+/-0.120) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[350]: 0.876 (+/-0.062) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[351]: 0.805 (+/-0.217) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[352]: 0.897 (+/-0.090) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[353]: 0.890 (+/-0.087) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[354]: 0.901 (+/-0.091) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[355]: 0.885 (+/-0.097) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[356]: 0.890 (+/-0.078) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[357]: 0.887 (+/-0.041) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[358]: 0.864 (+/-0.083) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[359]: 0.871 (+/-0.049) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[360]: 0.864 (+/-0.094) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[361]: 0.880 (+/-0.075) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[362]: 0.854 (+/-0.089) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[363]: 0.803 (+/-0.303) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[364]: 0.894 (+/-0.098) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[365]: 0.876 (+/-0.075) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[366]: 0.894 (+/-0.096) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[367]: 0.890 (+/-0.068) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[368]: 0.901 (+/-0.066) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[369]: 0.885 (+/-0.048) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[370]: 0.890 (+/-0.110) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[371]: 0.859 (+/-0.083) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[372]: 0.847 (+/-0.144) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[373]: 0.880 (+/-0.091) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[374]: 0.871 (+/-0.096) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[375]: 0.857 (+/-0.091) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[376]: 0.892 (+/-0.101) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[377]: 0.890 (+/-0.094) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[378]: 0.873 (+/-0.091) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[379]: 0.885 (+/-0.091) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[380]: 0.871 (+/-0.057) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[381]: 0.883 (+/-0.041) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[382]: 0.890 (+/-0.072) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[383]: 0.855 (+/-0.139) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[384]: 0.624 (+/-0.004) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[385]: 0.446 (+/-0.307) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[386]: 0.575 (+/-0.199) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[387]: 0.474 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[388]: 0.575 (+/-0.199) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[389]: 0.526 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[390]: 0.427 (+/-0.196) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[391]: 0.608 (+/-0.270) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[392]: 0.470 (+/-0.242) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[393]: 0.526 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[394]: 0.425 (+/-0.199) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[395]: 0.575 (+/-0.199) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[396]: 0.624 (+/-0.004) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[397]: 0.526 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[398]: 0.437 (+/-0.189) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[399]: 0.441 (+/-0.213) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[400]: 0.535 (+/-0.217) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[401]: 0.474 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[402]: 0.474 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[403]: 0.425 (+/-0.199) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[404]: 0.543 (+/-0.330) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[405]: 0.474 (+/-0.253) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[406]: 0.524 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[407]: 0.476 (+/-0.244) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[408]: 0.333 (+/-0.169) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[409]: 0.575 (+/-0.364) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[410]: 0.545 (+/-0.289) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[411]: 0.442 (+/-0.445) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[412]: 0.526 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[413]: 0.479 (+/-0.236) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[414]: 0.456 (+/-0.206) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[415]: 0.474 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[416]: 0.476 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[417]: 0.627 (+/-0.009) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[418]: 0.476 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[419]: 0.476 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[420]: 0.573 (+/-0.201) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[421]: 0.420 (+/-0.180) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[422]: 0.524 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[423]: 0.523 (+/-0.240) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[424]: 0.474 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[425]: 0.476 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[426]: 0.486 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[427]: 0.425 (+/-0.199) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[428]: 0.481 (+/-0.256) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[429]: 0.571 (+/-0.210) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[430]: 0.526 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[431]: 0.526 (+/-0.244) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[432]: 0.876 (+/-0.120) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[433]: 0.908 (+/-0.050) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[434]: 0.826 (+/-0.231) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[435]: 0.878 (+/-0.108) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[436]: 0.871 (+/-0.097) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[437]: 0.890 (+/-0.086) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[438]: 0.868 (+/-0.077) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[439]: 0.892 (+/-0.074) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[440]: 0.887 (+/-0.108) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[441]: 0.810 (+/-0.207) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[442]: 0.866 (+/-0.081) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[443]: 0.850 (+/-0.183) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[444]: 0.890 (+/-0.088) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[445]: 0.866 (+/-0.136) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[446]: 0.887 (+/-0.107) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[447]: 0.812 (+/-0.338) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[448]: 0.890 (+/-0.080) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[449]: 0.885 (+/-0.082) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[450]: 0.885 (+/-0.114) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[451]: 0.894 (+/-0.079) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[452]: 0.838 (+/-0.199) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[453]: 0.852 (+/-0.160) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[454]: 0.847 (+/-0.109) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[455]: 0.869 (+/-0.090) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[456]: 0.895 (+/-0.079) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[457]: 0.810 (+/-0.289) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[458]: 0.826 (+/-0.235) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[459]: 0.845 (+/-0.119) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[460]: 0.899 (+/-0.078) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[461]: 0.890 (+/-0.084) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[462]: 0.890 (+/-0.097) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[463]: 0.897 (+/-0.084) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[464]: 0.876 (+/-0.077) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[465]: 0.869 (+/-0.098) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[466]: 0.899 (+/-0.088) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[467]: 0.871 (+/-0.105) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[468]: 0.897 (+/-0.080) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[469]: 0.826 (+/-0.252) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[470]: 0.754 (+/-0.345) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[471]: 0.831 (+/-0.197) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[472]: 0.883 (+/-0.065) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[473]: 0.899 (+/-0.108) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[474]: 0.885 (+/-0.079) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[475]: 0.897 (+/-0.083) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[476]: 0.864 (+/-0.095) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[477]: 0.857 (+/-0.129) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[478]: 0.871 (+/-0.105) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[479]: 0.833 (+/-0.063) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[480]: 0.890 (+/-0.125) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[481]: 0.852 (+/-0.182) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[482]: 0.892 (+/-0.067) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[483]: 0.866 (+/-0.044) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[484]: 0.869 (+/-0.062) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[485]: 0.892 (+/-0.072) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[486]: 0.883 (+/-0.062) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[487]: 0.883 (+/-0.084) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[488]: 0.817 (+/-0.151) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[489]: 0.897 (+/-0.068) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[490]: 0.850 (+/-0.235) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[491]: 0.878 (+/-0.036) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[492]: 0.871 (+/-0.108) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[493]: 0.862 (+/-0.095) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[494]: 0.878 (+/-0.093) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[495]: 0.892 (+/-0.088) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[496]: 0.897 (+/-0.078) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[497]: 0.878 (+/-0.106) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[498]: 0.887 (+/-0.089) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[499]: 0.890 (+/-0.097) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[500]: 0.871 (+/-0.070) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[501]: 0.899 (+/-0.032) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[502]: 0.904 (+/-0.065) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[503]: 0.906 (+/-0.064) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[504]: 0.878 (+/-0.094) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[505]: 0.850 (+/-0.106) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[506]: 0.829 (+/-0.145) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[507]: 0.871 (+/-0.071) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[508]: 0.892 (+/-0.077) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[509]: 0.883 (+/-0.093) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[510]: 0.883 (+/-0.099) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[511]: 0.876 (+/-0.084) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[512]: 0.899 (+/-0.046) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[513]: 0.855 (+/-0.114) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[514]: 0.883 (+/-0.050) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[515]: 0.897 (+/-0.087) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[516]: 0.878 (+/-0.087) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[517]: 0.864 (+/-0.129) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[518]: 0.897 (+/-0.057) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[519]: 0.817 (+/-0.049) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[520]: 0.880 (+/-0.069) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[521]: 0.880 (+/-0.089) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[522]: 0.897 (+/-0.098) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[523]: 0.876 (+/-0.077) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[524]: 0.805 (+/-0.131) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[525]: 0.848 (+/-0.184) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[526]: 0.866 (+/-0.102) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[527]: 0.890 (+/-0.099) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[528]: 0.303 (+/-0.227) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[529]: 0.570 (+/-0.194) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[530]: 0.446 (+/-0.315) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[531]: 0.432 (+/-0.193) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[532]: 0.425 (+/-0.199) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[533]: 0.467 (+/-0.179) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[534]: 0.540 (+/-0.244) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[535]: 0.486 (+/-0.195) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[536]: 0.512 (+/-0.297) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[537]: 0.481 (+/-0.277) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[538]: 0.463 (+/-0.218) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[539]: 0.523 (+/-0.249) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[540]: 0.615 (+/-0.284) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[541]: 0.427 (+/-0.201) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[542]: 0.371 (+/-0.328) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[543]: 0.514 (+/-0.282) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[544]: 0.486 (+/-0.241) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[545]: 0.519 (+/-0.193) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[546]: 0.427 (+/-0.196) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[547]: 0.446 (+/-0.168) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[548]: 0.530 (+/-0.252) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[549]: 0.444 (+/-0.193) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[550]: 0.474 (+/-0.244) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[551]: 0.519 (+/-0.239) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[552]: 0.489 (+/-0.282) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[553]: 0.528 (+/-0.238) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[554]: 0.484 (+/-0.534) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[555]: 0.458 (+/-0.266) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[556]: 0.543 (+/-0.283) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[557]: 0.526 (+/-0.248) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[558]: 0.524 (+/-0.244) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[559]: 0.624 (+/-0.004) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[560]: 0.498 (+/-0.310) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[561]: 0.524 (+/-0.244) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[562]: 0.575 (+/-0.199) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[563]: 0.425 (+/-0.199) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[564]: 0.512 (+/-0.226) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[565]: 0.510 (+/-0.348) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[566]: 0.561 (+/-0.197) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[567]: 0.476 (+/-0.244) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[568]: 0.429 (+/-0.210) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[569]: 0.472 (+/-0.238) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[570]: 0.479 (+/-0.241) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[571]: 0.575 (+/-0.199) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[572]: 0.526 (+/-0.405) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[573]: 0.425 (+/-0.199) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[574]: 0.514 (+/-0.245) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[575]: 0.524 (+/-0.244) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[576]: 0.756 (+/-0.312) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[577]: 0.866 (+/-0.143) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[578]: 0.869 (+/-0.127) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[579]: 0.817 (+/-0.204) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[580]: 0.892 (+/-0.075) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[581]: 0.908 (+/-0.046) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[582]: 0.894 (+/-0.088) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[583]: 0.883 (+/-0.099) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[584]: 0.897 (+/-0.078) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[585]: 0.876 (+/-0.057) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[586]: 0.822 (+/-0.101) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[587]: 0.852 (+/-0.049) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[588]: 0.796 (+/-0.233) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[589]: 0.881 (+/-0.136) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[590]: 0.813 (+/-0.240) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[591]: 0.796 (+/-0.188) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[592]: 0.868 (+/-0.057) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[593]: 0.866 (+/-0.109) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[594]: 0.892 (+/-0.049) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[595]: 0.875 (+/-0.112) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[596]: 0.850 (+/-0.066) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[597]: 0.873 (+/-0.101) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[598]: 0.894 (+/-0.061) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[599]: 0.847 (+/-0.116) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[600]: 0.862 (+/-0.099) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[601]: 0.845 (+/-0.224) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[602]: 0.824 (+/-0.192) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[603]: 0.864 (+/-0.057) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[604]: 0.875 (+/-0.089) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[605]: 0.878 (+/-0.074) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[606]: 0.890 (+/-0.120) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[607]: 0.871 (+/-0.105) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[608]: 0.885 (+/-0.077) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[609]: 0.852 (+/-0.097) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[610]: 0.904 (+/-0.083) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[611]: 0.904 (+/-0.093) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[612]: 0.817 (+/-0.128) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[613]: 0.876 (+/-0.083) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[614]: 0.843 (+/-0.049) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[615]: 0.833 (+/-0.237) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[616]: 0.890 (+/-0.102) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[617]: 0.880 (+/-0.107) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[618]: 0.883 (+/-0.055) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[619]: 0.892 (+/-0.096) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[620]: 0.840 (+/-0.024) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[621]: 0.906 (+/-0.042) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[622]: 0.845 (+/-0.117) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[623]: 0.859 (+/-0.102) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[624]: 0.880 (+/-0.035) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[625]: 0.897 (+/-0.085) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[626]: 0.887 (+/-0.041) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[627]: 0.831 (+/-0.097) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[628]: 0.880 (+/-0.108) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[629]: 0.878 (+/-0.096) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[630]: 0.887 (+/-0.114) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[631]: 0.883 (+/-0.081) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[632]: 0.887 (+/-0.050) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[633]: 0.866 (+/-0.032) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[634]: 0.864 (+/-0.108) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[635]: 0.854 (+/-0.085) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[636]: 0.864 (+/-0.057) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[637]: 0.871 (+/-0.096) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[638]: 0.784 (+/-0.207) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[639]: 0.832 (+/-0.233) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[640]: 0.871 (+/-0.079) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[641]: 0.871 (+/-0.098) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[642]: 0.885 (+/-0.105) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[643]: 0.883 (+/-0.080) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[644]: 0.887 (+/-0.075) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[645]: 0.869 (+/-0.104) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[646]: 0.883 (+/-0.020) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[647]: 0.871 (+/-0.107) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[648]: 0.796 (+/-0.214) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[649]: 0.852 (+/-0.107) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[650]: 0.866 (+/-0.090) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[651]: 0.827 (+/-0.153) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[652]: 0.878 (+/-0.090) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[653]: 0.885 (+/-0.066) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[654]: 0.885 (+/-0.079) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[655]: 0.871 (+/-0.086) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[656]: 0.899 (+/-0.084) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[657]: 0.857 (+/-0.182) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[658]: 0.880 (+/-0.035) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[659]: 0.883 (+/-0.133) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[660]: 0.894 (+/-0.074) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[661]: 0.847 (+/-0.026) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[662]: 0.864 (+/-0.115) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[663]: 0.864 (+/-0.146) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[664]: 0.890 (+/-0.095) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[665]: 0.878 (+/-0.053) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[666]: 0.873 (+/-0.046) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[667]: 0.880 (+/-0.070) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[668]: 0.862 (+/-0.177) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[669]: 0.911 (+/-0.057) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[670]: 0.908 (+/-0.037) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[671]: 0.880 (+/-0.091) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[672]: 0.526 (+/-0.396) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[673]: 0.474 (+/-0.244) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[674]: 0.570 (+/-0.195) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[675]: 0.507 (+/-0.289) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[676]: 0.563 (+/-0.195) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[677]: 0.507 (+/-0.208) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[678]: 0.526 (+/-0.235) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[679]: 0.489 (+/-0.207) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[680]: 0.575 (+/-0.199) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[681]: 0.414 (+/-0.361) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[682]: 0.390 (+/-0.334) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[683]: 0.535 (+/-0.222) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[684]: 0.526 (+/-0.244) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[685]: 0.481 (+/-0.380) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[686]: 0.474 (+/-0.244) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[687]: 0.373 (+/-0.009) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[688]: 0.516 (+/-0.252) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[689]: 0.495 (+/-0.206) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[690]: 0.470 (+/-0.242) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[691]: 0.456 (+/-0.176) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[692]: 0.476 (+/-0.244) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[693]: 0.521 (+/-0.255) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[694]: 0.624 (+/-0.004) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[695]: 0.629 (+/-0.022) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[696]: 0.570 (+/-0.325) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[697]: 0.643 (+/-0.308) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[698]: 0.612 (+/-0.278) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[699]: 0.427 (+/-0.196) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[700]: 0.524 (+/-0.244) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[701]: 0.524 (+/-0.244) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[702]: 0.624 (+/-0.004) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[703]: 0.575 (+/-0.199) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[704]: 0.526 (+/-0.244) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[705]: 0.547 (+/-0.403) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[706]: 0.477 (+/-0.249) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[707]: 0.540 (+/-0.271) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[708]: 0.714 (+/-0.222) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[709]: 0.648 (+/-0.049) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[710]: 0.544 (+/-0.230) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[711]: 0.629 (+/-0.328) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[712]: 0.526 (+/-0.248) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[713]: 0.596 (+/-0.234) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[714]: 0.526 (+/-0.244) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[715]: 0.575 (+/-0.199) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[716]: 0.479 (+/-0.250) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[717]: 0.596 (+/-0.234) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[718]: 0.533 (+/-0.430) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[719]: 0.427 (+/-0.201) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[720]: 0.894 (+/-0.085) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[721]: 0.822 (+/-0.375) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[722]: 0.871 (+/-0.100) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[723]: 0.838 (+/-0.187) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[724]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[725]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[726]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[727]: 0.573 (+/-0.201) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[728]: 0.427 (+/-0.201) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[729]: 0.476 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[730]: 0.616 (+/-0.294) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[731]: 0.477 (+/-0.240) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[732]: 0.859 (+/-0.119) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[733]: 0.897 (+/-0.064) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[734]: 0.864 (+/-0.077) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[735]: 0.887 (+/-0.115) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[736]: 0.376 (+/-0.004) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[737]: 0.624 (+/-0.004) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[738]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[739]: 0.624 (+/-0.004) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[740]: 0.528 (+/-0.247) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[741]: 0.540 (+/-0.210) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[742]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[743]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[744]: 0.887 (+/-0.093) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[745]: 0.850 (+/-0.096) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[746]: 0.861 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[747]: 0.862 (+/-0.099) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[748]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[749]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[750]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[751]: 0.624 (+/-0.004) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[752]: 0.490 (+/-0.227) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[753]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[754]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[755]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[756]: 0.843 (+/-0.070) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[757]: 0.890 (+/-0.073) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[758]: 0.833 (+/-0.074) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[759]: 0.897 (+/-0.062) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[760]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[761]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[762]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[763]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[764]: 0.634 (+/-0.328) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[765]: 0.573 (+/-0.201) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[766]: 0.425 (+/-0.199) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[767]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[768]: 0.859 (+/-0.052) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[769]: 0.836 (+/-0.207) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[770]: 0.838 (+/-0.190) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[771]: 0.848 (+/-0.081) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[772]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[773]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[774]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[775]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[776]: 0.716 (+/-0.377) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[777]: 0.655 (+/-0.315) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[778]: 0.525 (+/-0.342) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[779]: 0.514 (+/-0.202) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[780]: 0.880 (+/-0.051) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[781]: 0.871 (+/-0.033) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[782]: 0.878 (+/-0.061) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[783]: 0.857 (+/-0.093) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[784]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[785]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[786]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[787]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[788]: 0.587 (+/-0.366) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[789]: 0.575 (+/-0.359) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[790]: 0.531 (+/-0.384) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[791]: 0.427 (+/-0.196) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[792]: 0.885 (+/-0.117) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[793]: 0.873 (+/-0.092) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[794]: 0.873 (+/-0.072) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[795]: 0.862 (+/-0.119) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[796]: 0.376 (+/-0.004) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[797]: 0.474 (+/-0.244) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[798]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[799]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[800]: 0.655 (+/-0.290) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[801]: 0.697 (+/-0.346) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[802]: 0.742 (+/-0.498) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[803]: 0.622 (+/-0.173) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[804]: 0.887 (+/-0.075) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[805]: 0.864 (+/-0.077) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[806]: 0.878 (+/-0.079) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[807]: 0.880 (+/-0.113) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[808]: 0.573 (+/-0.201) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[809]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[810]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[811]: 0.425 (+/-0.199) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[812]: 0.739 (+/-0.355) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[813]: 0.573 (+/-0.376) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[814]: 0.695 (+/-0.298) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[815]: 0.638 (+/-0.339) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[816]: 0.620 (+/-0.194) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[817]: 0.514 (+/-0.236) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[818]: 0.352 (+/-0.455) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[819]: 0.457 (+/-0.339) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[820]: 0.495 (+/-0.123) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[821]: 0.507 (+/-0.172) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[822]: 0.462 (+/-0.260) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[823]: 0.599 (+/-0.189) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[824]: 0.399 (+/-0.078) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[825]: 0.530 (+/-0.359) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[826]: 0.507 (+/-0.297) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[827]: 0.526 (+/-0.304) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[828]: 0.540 (+/-0.417) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[829]: 0.524 (+/-0.231) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[830]: 0.441 (+/-0.147) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[831]: 0.460 (+/-0.204) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[832]: 0.489 (+/-0.180) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[833]: 0.561 (+/-0.119) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[834]: 0.500 (+/-0.213) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[835]: 0.411 (+/-0.126) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[836]: 0.486 (+/-0.376) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[837]: 0.467 (+/-0.401) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[838]: 0.397 (+/-0.170) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[839]: 0.523 (+/-0.320) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[840]: 0.810 (+/-0.191) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[841]: 0.758 (+/-0.144) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[842]: 0.798 (+/-0.169) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[843]: 0.732 (+/-0.106) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[844]: 0.624 (+/-0.004) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[845]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[846]: 0.624 (+/-0.004) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[847]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[848]: 0.476 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[849]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[850]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[851]: 0.524 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[852]: 0.817 (+/-0.131) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[853]: 0.784 (+/-0.162) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[854]: 0.824 (+/-0.237) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[855]: 0.781 (+/-0.179) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[856]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[857]: 0.476 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[858]: 0.476 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[859]: 0.573 (+/-0.201) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[860]: 0.573 (+/-0.201) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[861]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[862]: 0.526 (+/-0.244) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[863]: 0.575 (+/-0.199) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.61        91\n",
      "           1       0.50      1.00      0.67        52\n",
      "\n",
      "    accuracy                           0.64       143\n",
      "   macro avg       0.75      0.72      0.64       143\n",
      "weighted avg       0.82      0.64      0.63       143\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(max_iter=100000, penalty='elasticnet', tol=0.0001)\n",
      "\n",
      "best: dat=N/A, score=0.91092, model=SGDClassifier(alpha=0.0001,loss='hinge',max_iter=100000,penalty='elasticnet',tol=0.0001)\n",
      "\n",
      "OK(grid-search)\n",
      "(\"best: dat=N/A, score=0.91092, model=SGDClassifier(alpha=0.0001,loss='hinge',max_iter=100000,penalty='elasticnet',tol=0.0001)\", SGDClassifier(max_iter=100000, penalty='elasticnet', tol=0.0001))\n"
     ]
    }
   ],
   "source": [
    "# Use some classifiers\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from time import time\n",
    "\n",
    "scores = {}\n",
    "\n",
    "model = SGDClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'perceptron', 'modified_huber', 'squared_error', 'huber'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 100],\n",
    "    'tol' : [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter' : [int(1e+5), int(1e+6)]\n",
    "}\n",
    "\n",
    "scores = doGridSearch(data, model, tuning_parameters)\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 1.45 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\tbest 'f1_micro' score=0.9461012311901505\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.946 (+/-0.050) for {'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[ 1]: 0.946 (+/-0.050) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[ 2]: 0.946 (+/-0.050) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[ 3]: 0.946 (+/-0.050) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[ 4]: 0.946 (+/-0.050) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[ 5]: 0.946 (+/-0.050) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[ 6]: 0.934 (+/-0.035) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[ 7]: 0.934 (+/-0.041) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[ 8]: 0.939 (+/-0.057) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[ 9]: 0.923 (+/-0.038) for {'alpha': 0.01, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[10]: 0.923 (+/-0.038) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[11]: 0.923 (+/-0.052) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[12]: 0.941 (+/-0.033) for {'alpha': 0.1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[13]: 0.941 (+/-0.033) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[14]: 0.941 (+/-0.033) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[15]: 0.941 (+/-0.033) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[16]: 0.941 (+/-0.033) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[17]: 0.941 (+/-0.033) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[18]: 0.934 (+/-0.035) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[19]: 0.934 (+/-0.041) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[20]: 0.939 (+/-0.057) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[21]: 0.923 (+/-0.038) for {'alpha': 0.1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[22]: 0.923 (+/-0.038) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[23]: 0.923 (+/-0.052) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[24]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[25]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[26]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[27]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[28]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[29]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[30]: 0.944 (+/-0.035) for {'alpha': 1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[31]: 0.934 (+/-0.041) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[32]: 0.939 (+/-0.057) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[33]: 0.923 (+/-0.038) for {'alpha': 1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[34]: 0.923 (+/-0.038) for {'alpha': 1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[35]: 0.923 (+/-0.052) for {'alpha': 1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[36]: 0.941 (+/-0.042) for {'alpha': 2, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[37]: 0.941 (+/-0.042) for {'alpha': 2, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[38]: 0.941 (+/-0.042) for {'alpha': 2, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[39]: 0.941 (+/-0.042) for {'alpha': 2, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[40]: 0.941 (+/-0.042) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[41]: 0.941 (+/-0.042) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[42]: 0.946 (+/-0.032) for {'alpha': 2, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[43]: 0.934 (+/-0.041) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[44]: 0.939 (+/-0.057) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[45]: 0.923 (+/-0.038) for {'alpha': 2, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[46]: 0.923 (+/-0.038) for {'alpha': 2, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[47]: 0.923 (+/-0.052) for {'alpha': 2, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[48]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[49]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[50]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[51]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[52]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[53]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[54]: 0.944 (+/-0.031) for {'alpha': 10, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[55]: 0.937 (+/-0.038) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[56]: 0.939 (+/-0.057) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[57]: 0.923 (+/-0.038) for {'alpha': 10, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[58]: 0.923 (+/-0.038) for {'alpha': 10, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[59]: 0.923 (+/-0.052) for {'alpha': 10, 'solver': 'sag', 'tol': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        91\n",
      "           1       1.00      0.94      0.97        52\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.97      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n",
      "\n",
      "CTOR for best model: RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "best: dat=N/A, score=0.94610, model=RidgeClassifier(alpha=0.01,solver='svd',tol=1e-05)\n",
      "\n",
      "OK(grid-search)\n",
      "(\"best: dat=N/A, score=0.94610, model=RidgeClassifier(alpha=0.01,solver='svd',tol=1e-05)\", RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05))\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = 0\n",
    "\n",
    "# Chosen model\n",
    "sgd = SGDClassifier(max_iter=100000, penalty='elasticnet', tol=0.0001)\n",
    "\n",
    "model = RidgeClassifier()\n",
    "tuning_parameters = {\n",
    "        'alpha' : [0.01, 0.1, 1, 2, 10],\n",
    "        'tol' : [0.00001, 0.0001, 0.001],\n",
    "        'solver' : ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "    }\n",
    "\n",
    "scores = doGridSearch(data, model, tuning_parameters)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 0.70 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'max_iter_predict': 100, 'n_restarts_optimizer': 0}\n",
      "\tbest 'f1_micro' score=0.7065116279069767\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tGaussianProcessClassifier()\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.707 (+/-0.094) for {'max_iter_predict': 100, 'n_restarts_optimizer': 0}\n",
      "\t[ 1]: 0.707 (+/-0.094) for {'max_iter_predict': 100, 'n_restarts_optimizer': 2}\n",
      "\t[ 2]: 0.707 (+/-0.094) for {'max_iter_predict': 100, 'n_restarts_optimizer': 5}\n",
      "\t[ 3]: 0.707 (+/-0.094) for {'max_iter_predict': 500, 'n_restarts_optimizer': 0}\n",
      "\t[ 4]: 0.707 (+/-0.094) for {'max_iter_predict': 500, 'n_restarts_optimizer': 2}\n",
      "\t[ 5]: 0.707 (+/-0.094) for {'max_iter_predict': 500, 'n_restarts_optimizer': 5}\n",
      "\t[ 6]: 0.707 (+/-0.094) for {'max_iter_predict': 1000, 'n_restarts_optimizer': 0}\n",
      "\t[ 7]: 0.707 (+/-0.094) for {'max_iter_predict': 1000, 'n_restarts_optimizer': 2}\n",
      "\t[ 8]: 0.707 (+/-0.094) for {'max_iter_predict': 1000, 'n_restarts_optimizer': 5}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82        91\n",
      "           1       0.73      0.46      0.56        52\n",
      "\n",
      "    accuracy                           0.74       143\n",
      "   macro avg       0.74      0.68      0.69       143\n",
      "weighted avg       0.74      0.74      0.72       143\n",
      "\n",
      "\n",
      "CTOR for best model: GaussianProcessClassifier()\n",
      "\n",
      "best: dat=N/A, score=0.70651, model=GaussianProcessClassifier(max_iter_predict=100,n_restarts_optimizer=0)\n",
      "\n",
      "OK(grid-search)\n",
      "('best: dat=N/A, score=0.70651, model=GaussianProcessClassifier(max_iter_predict=100,n_restarts_optimizer=0)', GaussianProcessClassifier())\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "model = GaussianProcessClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'n_restarts_optimizer' : [0, 2, 5],\n",
    "    'max_iter_predict' : [100, 500, 1000]\n",
    "}\n",
    "\n",
    "scores = doGridSearch(data, model, tuning_parameters)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian classifier is clearly very poor choice for this task with a score of 0.723, we will skip it.\n",
    "\n",
    "Next, we'll try a DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 0.98 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\tbest 'f1_micro' score=0.9366073871409029\n",
      "\tbest index=16\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tDecisionTreeClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.911 (+/-0.046) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[ 1]: 0.906 (+/-0.047) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[ 2]: 0.906 (+/-0.047) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[ 3]: 0.906 (+/-0.047) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[ 4]: 0.906 (+/-0.047) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[ 5]: 0.906 (+/-0.047) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[ 6]: 0.906 (+/-0.047) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[ 7]: 0.911 (+/-0.046) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[ 8]: 0.911 (+/-0.046) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[ 9]: 0.906 (+/-0.076) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[10]: 0.911 (+/-0.076) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[11]: 0.906 (+/-0.076) for {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[12]: 0.932 (+/-0.041) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[13]: 0.927 (+/-0.018) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[14]: 0.930 (+/-0.047) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[15]: 0.932 (+/-0.028) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[16]: 0.937 (+/-0.046) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[17]: 0.934 (+/-0.038) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[18]: 0.930 (+/-0.056) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[19]: 0.927 (+/-0.062) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[20]: 0.930 (+/-0.056) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[21]: 0.911 (+/-0.076) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[22]: 0.908 (+/-0.075) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[23]: 0.911 (+/-0.076) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[24]: 0.930 (+/-0.040) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[25]: 0.932 (+/-0.028) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[26]: 0.930 (+/-0.030) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[27]: 0.930 (+/-0.033) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[28]: 0.932 (+/-0.041) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[29]: 0.932 (+/-0.023) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[30]: 0.927 (+/-0.062) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[31]: 0.922 (+/-0.036) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[32]: 0.922 (+/-0.049) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[33]: 0.911 (+/-0.076) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[34]: 0.908 (+/-0.075) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[35]: 0.908 (+/-0.075) for {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[36]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[37]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[38]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[39]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[40]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[41]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[42]: 0.890 (+/-0.052) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[43]: 0.892 (+/-0.045) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[44]: 0.890 (+/-0.052) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[45]: 0.894 (+/-0.044) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[46]: 0.894 (+/-0.044) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[47]: 0.894 (+/-0.044) for {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[48]: 0.911 (+/-0.049) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[49]: 0.920 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[50]: 0.923 (+/-0.024) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[51]: 0.923 (+/-0.038) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[52]: 0.925 (+/-0.019) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[53]: 0.925 (+/-0.024) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[54]: 0.918 (+/-0.071) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[55]: 0.913 (+/-0.068) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[56]: 0.911 (+/-0.067) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[57]: 0.915 (+/-0.052) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[58]: 0.913 (+/-0.055) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[59]: 0.913 (+/-0.055) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[60]: 0.927 (+/-0.046) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[61]: 0.920 (+/-0.035) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[62]: 0.915 (+/-0.031) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[63]: 0.932 (+/-0.035) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[64]: 0.927 (+/-0.028) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[65]: 0.918 (+/-0.042) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[66]: 0.908 (+/-0.069) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[67]: 0.915 (+/-0.067) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[68]: 0.915 (+/-0.069) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[69]: 0.913 (+/-0.055) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[70]: 0.913 (+/-0.055) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[71]: 0.913 (+/-0.055) for {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[72]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[73]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[74]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[75]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[76]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[77]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[78]: 0.890 (+/-0.052) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[79]: 0.890 (+/-0.052) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[80]: 0.892 (+/-0.045) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[81]: 0.894 (+/-0.044) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[82]: 0.894 (+/-0.044) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[83]: 0.894 (+/-0.044) for {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[84]: 0.918 (+/-0.026) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[85]: 0.913 (+/-0.044) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[86]: 0.918 (+/-0.042) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[87]: 0.925 (+/-0.041) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[88]: 0.923 (+/-0.038) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[89]: 0.915 (+/-0.041) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[90]: 0.913 (+/-0.071) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[91]: 0.911 (+/-0.071) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[92]: 0.911 (+/-0.067) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[93]: 0.913 (+/-0.055) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[94]: 0.918 (+/-0.052) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[95]: 0.913 (+/-0.055) for {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\t[96]: 0.918 (+/-0.040) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\t[97]: 0.913 (+/-0.024) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "\t[98]: 0.920 (+/-0.041) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "\t[99]: 0.932 (+/-0.023) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "\t[100]: 0.927 (+/-0.041) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\t[101]: 0.925 (+/-0.044) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "\t[102]: 0.913 (+/-0.074) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\t[103]: 0.911 (+/-0.071) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
      "\t[104]: 0.913 (+/-0.071) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "\t[105]: 0.913 (+/-0.055) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "\t[106]: 0.913 (+/-0.055) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
      "\t[107]: 0.913 (+/-0.055) for {'criterion': 'log_loss', 'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        91\n",
      "           1       0.92      0.92      0.92        52\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.94      0.94       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n",
      "\n",
      "CTOR for best model: DecisionTreeClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5)\n",
      "\n",
      "best: dat=N/A, score=0.93661, model=DecisionTreeClassifier(criterion='gini',max_depth=5,min_samples_leaf=2,min_samples_split=5)\n",
      "\n",
      "OK(grid-search)\n",
      "(\"best: dat=N/A, score=0.93661, model=DecisionTreeClassifier(criterion='gini',max_depth=5,min_samples_leaf=2,min_samples_split=5)\", DecisionTreeClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth' : [2, 5, None],\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "scores = doGridSearch(data, model, tuning_parameters)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=2,\n",
    "                       min_samples_split=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try dummy classifier for a more realistic comparison of score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Classifies EVERYTHING in the most frequent class (here 1 == Malignant tumor diagnosis)\n",
    "\n",
    "def CompareToDummy(data: Data, model):\n",
    "    dummy_clf = DummyClassifier()\n",
    "    dummy_clf.fit(data.x_train, data.y_train)\n",
    "    dummy_y = dummy_clf.predict(data.x_test)\n",
    "\n",
    "    model.fit(data.x_train, data.y_train)\n",
    "    model_y = model.predict(data.x_test)\n",
    "\n",
    "    a_s = accuracy_score(data.y_test, model_y)\n",
    "    c_v_s = cross_val_score(model, data.x_train, data.y_train, cv=3, scoring=\"accuracy\")\n",
    "    # Uses 'jaccard score' function, see: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    a_s_dummy = accuracy_score(data.y_test, dummy_y)\n",
    "    c_v_s_dummy = cross_val_score(dummy_clf, data.x_train, data.y_train, cv = 3, scoring=\"accuracy\")\n",
    "    print(type(model).__name__)\n",
    "    print(\"Model accuracy: \", a_s, \" | Dummy accuracy: \", a_s_dummy)\n",
    "    print(\"Model cvs: \", \"c_v_s\", c_v_s, \" | Dummy cvs: \", c_v_s_dummy, \"\\n\")\n",
    "\n",
    "    return (a_s, c_v_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing cross val score and accuracy of model to dummy classifier scores\n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.8713450292397661  | Dummy accuracy:  0.6432748538011696\n",
      "Model cvs:  c_v_s [0.84962406 0.93984962 0.90909091]  | Dummy cvs:  [0.61654135 0.62406015 0.62121212] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9473684210526315  | Dummy accuracy:  0.6432748538011696\n",
      "Model cvs:  c_v_s [0.93233083 0.96992481 0.95454545]  | Dummy cvs:  [0.61654135 0.62406015 0.62121212] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9415204678362573  | Dummy accuracy:  0.6432748538011696\n",
      "Model cvs:  c_v_s [0.90225564 0.88721805 0.93939394]  | Dummy cvs:  [0.61654135 0.62406015 0.62121212] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.8362573099415205  | Dummy accuracy:  0.6374269005847953\n",
      "Model cvs:  c_v_s [0.90977444 0.87218045 0.87878788]  | Dummy cvs:  [0.62406015 0.62406015 0.62121212] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9649122807017544  | Dummy accuracy:  0.6374269005847953\n",
      "Model cvs:  c_v_s [0.93984962 0.95488722 0.93939394]  | Dummy cvs:  [0.62406015 0.62406015 0.62121212] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.935672514619883  | Dummy accuracy:  0.6374269005847953\n",
      "Model cvs:  c_v_s [0.87969925 0.87969925 0.90151515]  | Dummy cvs:  [0.62406015 0.62406015 0.62121212] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.847953216374269  | Dummy accuracy:  0.5789473684210527\n",
      "Model cvs:  c_v_s [0.93233083 0.90977444 0.84090909]  | Dummy cvs:  [0.64661654 0.64661654 0.65151515] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9473684210526315  | Dummy accuracy:  0.5789473684210527\n",
      "Model cvs:  c_v_s [0.98496241 0.94736842 0.93939394]  | Dummy cvs:  [0.64661654 0.64661654 0.65151515] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9239766081871345  | Dummy accuracy:  0.5789473684210527\n",
      "Model cvs:  c_v_s [0.93984962 0.92481203 0.92424242]  | Dummy cvs:  [0.64661654 0.64661654 0.65151515] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.9064327485380117  | Dummy accuracy:  0.6140350877192983\n",
      "Model cvs:  c_v_s [0.85714286 0.91729323 0.86363636]  | Dummy cvs:  [0.63157895 0.63157895 0.63636364] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9824561403508771  | Dummy accuracy:  0.6140350877192983\n",
      "Model cvs:  c_v_s [0.93233083 0.95488722 0.9469697 ]  | Dummy cvs:  [0.63157895 0.63157895 0.63636364] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9298245614035088  | Dummy accuracy:  0.6140350877192983\n",
      "Model cvs:  c_v_s [0.91729323 0.90225564 0.92424242]  | Dummy cvs:  [0.63157895 0.63157895 0.63636364] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.8947368421052632  | Dummy accuracy:  0.6081871345029239\n",
      "Model cvs:  c_v_s [0.80451128 0.93233083 0.89393939]  | Dummy cvs:  [0.63909774 0.63157895 0.63636364] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9590643274853801  | Dummy accuracy:  0.6081871345029239\n",
      "Model cvs:  c_v_s [0.95488722 0.96240602 0.91666667]  | Dummy cvs:  [0.63909774 0.63157895 0.63636364] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9473684210526315  | Dummy accuracy:  0.6081871345029239\n",
      "Model cvs:  c_v_s [0.96240602 0.93984962 0.91666667]  | Dummy cvs:  [0.63909774 0.63157895 0.63636364] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.8888888888888888  | Dummy accuracy:  0.6198830409356725\n",
      "Model cvs:  c_v_s [0.78195489 0.93233083 0.90909091]  | Dummy cvs:  [0.63157895 0.63157895 0.62878788] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.935672514619883  | Dummy accuracy:  0.6198830409356725\n",
      "Model cvs:  c_v_s [0.98496241 0.93984962 0.96212121]  | Dummy cvs:  [0.63157895 0.63157895 0.62878788] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9415204678362573  | Dummy accuracy:  0.6198830409356725\n",
      "Model cvs:  c_v_s [0.92481203 0.89473684 0.93939394]  | Dummy cvs:  [0.63157895 0.63157895 0.62878788] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.8830409356725146  | Dummy accuracy:  0.6198830409356725\n",
      "Model cvs:  c_v_s [0.86466165 0.81203008 0.71212121]  | Dummy cvs:  [0.63157895 0.63157895 0.62878788] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9473684210526315  | Dummy accuracy:  0.6198830409356725\n",
      "Model cvs:  c_v_s [0.96992481 0.92481203 0.99242424]  | Dummy cvs:  [0.63157895 0.63157895 0.62878788] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9122807017543859  | Dummy accuracy:  0.6198830409356725\n",
      "Model cvs:  c_v_s [0.91729323 0.91729323 0.93939394]  | Dummy cvs:  [0.63157895 0.63157895 0.62878788] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.9064327485380117  | Dummy accuracy:  0.6432748538011696\n",
      "Model cvs:  c_v_s [0.89473684 0.90977444 0.92424242]  | Dummy cvs:  [0.61654135 0.62406015 0.62121212] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9590643274853801  | Dummy accuracy:  0.6432748538011696\n",
      "Model cvs:  c_v_s [0.94736842 0.94736842 0.96212121]  | Dummy cvs:  [0.61654135 0.62406015 0.62121212] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9415204678362573  | Dummy accuracy:  0.6432748538011696\n",
      "Model cvs:  c_v_s [0.93984962 0.93984962 0.90909091]  | Dummy cvs:  [0.61654135 0.62406015 0.62121212] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.9298245614035088  | Dummy accuracy:  0.6081871345029239\n",
      "Model cvs:  c_v_s [0.93233083 0.52631579 0.62121212]  | Dummy cvs:  [0.63909774 0.63157895 0.63636364] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9766081871345029  | Dummy accuracy:  0.6081871345029239\n",
      "Model cvs:  c_v_s [0.96992481 0.93984962 0.93939394]  | Dummy cvs:  [0.63909774 0.63157895 0.63636364] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9298245614035088  | Dummy accuracy:  0.6081871345029239\n",
      "Model cvs:  c_v_s [0.93233083 0.90977444 0.88636364]  | Dummy cvs:  [0.63909774 0.63157895 0.63636364] \n",
      "\n",
      "SGDClassifier\n",
      "Model accuracy:  0.8947368421052632  | Dummy accuracy:  0.5964912280701754\n",
      "Model cvs:  c_v_s [0.93984962 0.92481203 0.86363636]  | Dummy cvs:  [0.63909774 0.63909774 0.64393939] \n",
      "\n",
      "RidgeClassifier\n",
      "Model accuracy:  0.9590643274853801  | Dummy accuracy:  0.5964912280701754\n",
      "Model cvs:  c_v_s [0.96240602 0.95488722 0.93939394]  | Dummy cvs:  [0.63909774 0.63909774 0.64393939] \n",
      "\n",
      "DecisionTreeClassifier\n",
      "Model accuracy:  0.9181286549707602  | Dummy accuracy:  0.5964912280701754\n",
      "Model cvs:  c_v_s [0.93233083 0.87969925 0.96969697]  | Dummy cvs:  [0.63909774 0.63909774 0.64393939] \n",
      "\n",
      "{'sgd': 0.8859649122807018, 'ridge': 0.9578947368421054, 'dtree': 0.9321637426900583}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '0.885965'), Text(0, 0, '0.957895'), Text(0, 0, '0.932164')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "accuracies = {}\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "# Take an average accuracy score from 10 iterations, since they change between fits!\n",
    "\n",
    "print(\"Comparing cross val score and accuracy of model to dummy classifier scores\\n\")\n",
    "sgd_accuracies = []\n",
    "ridge_accuracies = []\n",
    "dtree_accuracies = []\n",
    "for i in range(iterations):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_breast_cancer, y_all, test_size=0.3, shuffle=True)\n",
    "    data = Data(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    scores = CompareToDummy(data, sgd)\n",
    "    sgd_accuracies.append(scores[0])\n",
    "\n",
    "    scores = CompareToDummy(data, ridge)\n",
    "    ridge_accuracies.append(scores[0])\n",
    "\n",
    "    scores = CompareToDummy(data, dtree)\n",
    "    dtree_accuracies.append(scores[0])\n",
    "\n",
    "accuracies[\"sgd\"] = np.mean(sgd_accuracies)\n",
    "accuracies[\"ridge\"] = np.mean(ridge_accuracies)\n",
    "accuracies[\"dtree\"] = np.mean(dtree_accuracies)\n",
    "print(accuracies)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.barh(np.arange(len(accuracies)), list(accuracies.values()))\n",
    "plt.title(\"Classifier accuracy (f1)\")\n",
    "plt.yticks(range(len(accuracies)), list(accuracies.keys()))\n",
    "ax.bar_label(bars)\n",
    "#for i, v in enumerate(accuracies.values()):\n",
    "#    ax.text(i, v+25, \"%d\" %v, ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "display = ConfusionMatrixDisplay.from_estimator(sgd, data.x_test, data.y_test, cmap=plt.cm.Blues)\n",
    "display.ax_.set_title(\"sgd confusion matrix\")\n",
    "plt.savefig(\"images/results/sgd_conf.png\")\n",
    "display_2 = ConfusionMatrixDisplay.from_estimator(ridge, data.x_test, data.y_test, cmap=plt.cm.Blues)\n",
    "display_2.ax_.set_title(\"ridge confusion matrix\")\n",
    "plt.savefig(\"images/results/ridge_conf.png\")\n",
    "display_3 = ConfusionMatrixDisplay.from_estimator(dtree, data.x_test, data.y_test, cmap=plt.cm.Blues)\n",
    "display_3.ax_.set_title(\"decision tree confusion matrix\")\n",
    "plt.savefig(\"images/results/dtree_conf.png\")\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "import matplotlib.image as img\n",
    "img_sgd = img.imread(\"images/results/sgd_conf.png\")\n",
    "img_ridge = img.imread(\"images/results/ridge_conf.png\")\n",
    "img_dtree = img.imread(\"images/results/dtree_conf.png\")\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_sgd)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_ridge)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_dtree)\n",
    "plt.savefig(\"images/results/all_conf.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the algorithm on means and worst separately to see if we can get a better result with different data. For example, can we get better results from only considering the means or the worst features from the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "SEARCH TIME: 4.35 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\tbest 'f1_micro' score=0.9530232558139534\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.953 (+/-0.073) for {'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[ 1]: 0.953 (+/-0.073) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[ 2]: 0.953 (+/-0.073) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[ 3]: 0.953 (+/-0.073) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[ 4]: 0.953 (+/-0.073) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[ 5]: 0.953 (+/-0.073) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[ 6]: 0.951 (+/-0.050) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[ 7]: 0.923 (+/-0.050) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[ 8]: 0.937 (+/-0.049) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[ 9]: 0.925 (+/-0.028) for {'alpha': 0.01, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[10]: 0.925 (+/-0.028) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[11]: 0.911 (+/-0.023) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[12]: 0.946 (+/-0.068) for {'alpha': 0.1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[13]: 0.946 (+/-0.068) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[14]: 0.946 (+/-0.068) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[15]: 0.946 (+/-0.068) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[16]: 0.946 (+/-0.068) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[17]: 0.946 (+/-0.068) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[18]: 0.951 (+/-0.050) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[19]: 0.923 (+/-0.050) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[20]: 0.937 (+/-0.049) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[21]: 0.925 (+/-0.028) for {'alpha': 0.1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[22]: 0.925 (+/-0.028) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[23]: 0.911 (+/-0.023) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[24]: 0.946 (+/-0.068) for {'alpha': 1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[25]: 0.946 (+/-0.068) for {'alpha': 1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[26]: 0.946 (+/-0.068) for {'alpha': 1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[27]: 0.946 (+/-0.068) for {'alpha': 1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[28]: 0.946 (+/-0.068) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[29]: 0.946 (+/-0.068) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[30]: 0.948 (+/-0.059) for {'alpha': 1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[31]: 0.923 (+/-0.050) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[32]: 0.937 (+/-0.049) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[33]: 0.925 (+/-0.028) for {'alpha': 1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[34]: 0.925 (+/-0.028) for {'alpha': 1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[35]: 0.911 (+/-0.023) for {'alpha': 1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[36]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[37]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[38]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[39]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[40]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[41]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[42]: 0.946 (+/-0.068) for {'alpha': 2, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[43]: 0.925 (+/-0.053) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[44]: 0.937 (+/-0.049) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[45]: 0.925 (+/-0.028) for {'alpha': 2, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[46]: 0.925 (+/-0.028) for {'alpha': 2, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[47]: 0.911 (+/-0.023) for {'alpha': 2, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[48]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[49]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[50]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[51]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[52]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[53]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[54]: 0.946 (+/-0.057) for {'alpha': 10, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[55]: 0.930 (+/-0.039) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[56]: 0.937 (+/-0.049) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[57]: 0.925 (+/-0.028) for {'alpha': 10, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[58]: 0.925 (+/-0.028) for {'alpha': 10, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[59]: 0.911 (+/-0.023) for {'alpha': 10, 'solver': 'sag', 'tol': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95        82\n",
      "           1       0.98      0.89      0.93        61\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.95      0.94      0.94       143\n",
      "weighted avg       0.95      0.94      0.94       143\n",
      "\n",
      "\n",
      "CTOR for best model: RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "best: dat=N/A, score=0.95302, model=RidgeClassifier(alpha=0.01,solver='svd',tol=1e-05)\n",
      "\n",
      "OK(grid-search)\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "SEARCH TIME: 0.84 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\tbest 'f1_micro' score=0.9436662106703148\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.944 (+/-0.018) for {'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[ 1]: 0.944 (+/-0.018) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[ 2]: 0.944 (+/-0.018) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[ 3]: 0.944 (+/-0.018) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[ 4]: 0.944 (+/-0.018) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[ 5]: 0.944 (+/-0.018) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[ 6]: 0.934 (+/-0.023) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[ 7]: 0.913 (+/-0.045) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[ 8]: 0.906 (+/-0.053) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[ 9]: 0.909 (+/-0.040) for {'alpha': 0.01, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[10]: 0.909 (+/-0.040) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[11]: 0.911 (+/-0.031) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[12]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[13]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[14]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[15]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[16]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[17]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[18]: 0.937 (+/-0.028) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[19]: 0.913 (+/-0.045) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[20]: 0.906 (+/-0.053) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[21]: 0.909 (+/-0.040) for {'alpha': 0.1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[22]: 0.909 (+/-0.040) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[23]: 0.911 (+/-0.031) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[24]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[25]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[26]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[27]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[28]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[29]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[30]: 0.923 (+/-0.043) for {'alpha': 1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[31]: 0.918 (+/-0.048) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[32]: 0.906 (+/-0.053) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[33]: 0.909 (+/-0.040) for {'alpha': 1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[34]: 0.909 (+/-0.040) for {'alpha': 1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[35]: 0.911 (+/-0.031) for {'alpha': 1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[36]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[37]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[38]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[39]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[40]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[41]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[42]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[43]: 0.918 (+/-0.048) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[44]: 0.906 (+/-0.053) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[45]: 0.909 (+/-0.040) for {'alpha': 2, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[46]: 0.909 (+/-0.040) for {'alpha': 2, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[47]: 0.911 (+/-0.031) for {'alpha': 2, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[48]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[49]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[50]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[51]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[52]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[53]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[54]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[55]: 0.918 (+/-0.053) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[56]: 0.906 (+/-0.053) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[57]: 0.909 (+/-0.040) for {'alpha': 10, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[58]: 0.909 (+/-0.040) for {'alpha': 10, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[59]: 0.911 (+/-0.031) for {'alpha': 10, 'solver': 'sag', 'tol': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93        83\n",
      "           1       0.96      0.83      0.89        60\n",
      "\n",
      "    accuracy                           0.92       143\n",
      "   macro avg       0.93      0.90      0.91       143\n",
      "weighted avg       0.92      0.92      0.92       143\n",
      "\n",
      "\n",
      "CTOR for best model: RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "best: dat=N/A, score=0.94367, model=RidgeClassifier(alpha=0.01,solver='svd',tol=1e-05)\n",
      "\n",
      "OK(grid-search)\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "SEARCH TIME: 0.75 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\tbest 'f1_micro' score=0.9671409028727771\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.967 (+/-0.052) for {'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[ 1]: 0.967 (+/-0.052) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[ 2]: 0.967 (+/-0.052) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[ 3]: 0.967 (+/-0.052) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[ 4]: 0.967 (+/-0.052) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[ 5]: 0.967 (+/-0.052) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[ 6]: 0.955 (+/-0.064) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[ 7]: 0.937 (+/-0.043) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[ 8]: 0.927 (+/-0.034) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[ 9]: 0.930 (+/-0.038) for {'alpha': 0.01, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[10]: 0.930 (+/-0.038) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[11]: 0.918 (+/-0.051) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[12]: 0.965 (+/-0.061) for {'alpha': 0.1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[13]: 0.965 (+/-0.061) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[14]: 0.965 (+/-0.061) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[15]: 0.965 (+/-0.061) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[16]: 0.965 (+/-0.061) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[17]: 0.965 (+/-0.061) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[18]: 0.948 (+/-0.066) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[19]: 0.937 (+/-0.043) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[20]: 0.927 (+/-0.034) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[21]: 0.930 (+/-0.038) for {'alpha': 0.1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[22]: 0.930 (+/-0.038) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[23]: 0.918 (+/-0.051) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[24]: 0.951 (+/-0.045) for {'alpha': 1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[25]: 0.951 (+/-0.045) for {'alpha': 1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[26]: 0.951 (+/-0.045) for {'alpha': 1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[27]: 0.951 (+/-0.045) for {'alpha': 1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[28]: 0.951 (+/-0.045) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[29]: 0.951 (+/-0.045) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[30]: 0.948 (+/-0.043) for {'alpha': 1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[31]: 0.937 (+/-0.043) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[32]: 0.927 (+/-0.034) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[33]: 0.930 (+/-0.038) for {'alpha': 1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[34]: 0.930 (+/-0.038) for {'alpha': 1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[35]: 0.918 (+/-0.051) for {'alpha': 1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[36]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[37]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[38]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[39]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[40]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[41]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[42]: 0.948 (+/-0.043) for {'alpha': 2, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[43]: 0.937 (+/-0.043) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[44]: 0.927 (+/-0.034) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[45]: 0.930 (+/-0.038) for {'alpha': 2, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[46]: 0.930 (+/-0.038) for {'alpha': 2, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[47]: 0.918 (+/-0.051) for {'alpha': 2, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[48]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[49]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[50]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[51]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[52]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[53]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[54]: 0.946 (+/-0.060) for {'alpha': 10, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[55]: 0.937 (+/-0.043) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[56]: 0.927 (+/-0.034) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[57]: 0.930 (+/-0.038) for {'alpha': 10, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[58]: 0.930 (+/-0.038) for {'alpha': 10, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[59]: 0.918 (+/-0.051) for {'alpha': 10, 'solver': 'sag', 'tol': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        90\n",
      "           1       0.98      0.96      0.97        53\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n",
      "\n",
      "CTOR for best model: RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "best: dat=N/A, score=0.96714, model=RidgeClassifier(alpha=0.01,solver='svd',tol=1e-05)\n",
      "\n",
      "OK(grid-search)\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "SEARCH TIME: 0.90 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\tbest 'f1_micro' score=0.7910807113543091\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.791 (+/-0.052) for {'alpha': 0.01, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[ 1]: 0.791 (+/-0.052) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[ 2]: 0.791 (+/-0.052) for {'alpha': 0.01, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[ 3]: 0.791 (+/-0.052) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[ 4]: 0.791 (+/-0.052) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[ 5]: 0.791 (+/-0.052) for {'alpha': 0.01, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[ 6]: 0.789 (+/-0.045) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[ 7]: 0.784 (+/-0.049) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[ 8]: 0.784 (+/-0.063) for {'alpha': 0.01, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[ 9]: 0.784 (+/-0.059) for {'alpha': 0.01, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[10]: 0.784 (+/-0.059) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[11]: 0.784 (+/-0.059) for {'alpha': 0.01, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[12]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[13]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[14]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[15]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[16]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[17]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[18]: 0.782 (+/-0.061) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[19]: 0.782 (+/-0.053) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[20]: 0.784 (+/-0.063) for {'alpha': 0.1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[21]: 0.784 (+/-0.059) for {'alpha': 0.1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[22]: 0.784 (+/-0.059) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[23]: 0.784 (+/-0.059) for {'alpha': 0.1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[24]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[25]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[26]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[27]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[28]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[29]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[30]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[31]: 0.782 (+/-0.054) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[32]: 0.784 (+/-0.063) for {'alpha': 1, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[33]: 0.784 (+/-0.059) for {'alpha': 1, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[34]: 0.784 (+/-0.059) for {'alpha': 1, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[35]: 0.784 (+/-0.059) for {'alpha': 1, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[36]: 0.784 (+/-0.052) for {'alpha': 2, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[37]: 0.784 (+/-0.052) for {'alpha': 2, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[38]: 0.784 (+/-0.052) for {'alpha': 2, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[39]: 0.784 (+/-0.052) for {'alpha': 2, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[40]: 0.784 (+/-0.052) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[41]: 0.784 (+/-0.052) for {'alpha': 2, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[42]: 0.782 (+/-0.043) for {'alpha': 2, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[43]: 0.782 (+/-0.043) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[44]: 0.784 (+/-0.069) for {'alpha': 2, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[45]: 0.784 (+/-0.059) for {'alpha': 2, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[46]: 0.784 (+/-0.059) for {'alpha': 2, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[47]: 0.784 (+/-0.059) for {'alpha': 2, 'solver': 'sag', 'tol': 0.001}\n",
      "\t[48]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'svd', 'tol': 1e-05}\n",
      "\t[49]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'svd', 'tol': 0.0001}\n",
      "\t[50]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'svd', 'tol': 0.001}\n",
      "\t[51]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'cholesky', 'tol': 1e-05}\n",
      "\t[52]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.0001}\n",
      "\t[53]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'cholesky', 'tol': 0.001}\n",
      "\t[54]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'lsqr', 'tol': 1e-05}\n",
      "\t[55]: 0.782 (+/-0.063) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.0001}\n",
      "\t[56]: 0.782 (+/-0.059) for {'alpha': 10, 'solver': 'lsqr', 'tol': 0.001}\n",
      "\t[57]: 0.784 (+/-0.059) for {'alpha': 10, 'solver': 'sag', 'tol': 1e-05}\n",
      "\t[58]: 0.784 (+/-0.059) for {'alpha': 10, 'solver': 'sag', 'tol': 0.0001}\n",
      "\t[59]: 0.784 (+/-0.059) for {'alpha': 10, 'solver': 'sag', 'tol': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91        97\n",
      "           1       0.86      0.70      0.77        46\n",
      "\n",
      "    accuracy                           0.87       143\n",
      "   macro avg       0.87      0.82      0.84       143\n",
      "weighted avg       0.87      0.87      0.86       143\n",
      "\n",
      "\n",
      "CTOR for best model: RidgeClassifier(alpha=0.01, solver='svd', tol=1e-05)\n",
      "\n",
      "best: dat=N/A, score=0.79108, model=RidgeClassifier(alpha=0.01,solver='svd',tol=1e-05)\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "X_train_means, X_test_means, Y_train_means, Y_test_means = train_test_split(data_breast_means, y_all, test_size = 0.25, shuffle=True)\n",
    "X_train_worst, X_test_worst, Y_train_worst, Y_test_worst = train_test_split(data_breast_worst, y_all, test_size = 0.25, shuffle=True)\n",
    "X_train_se, X_test_se, Y_train_se, Y_test_se = train_test_split(data_breast_se, y_all, test_size = 0.25, shuffle=True)\n",
    "\n",
    "data_means = Data(X_train_means, Y_train_means, X_test_means, Y_test_means)\n",
    "data_worst = Data(X_train_worst, Y_train_worst, X_test_worst, Y_test_worst)\n",
    "data_se = Data(X_train_se, Y_train_se, X_test_se, Y_test_se)\n",
    "\n",
    "grid_tuned = gridSearchRidge(data)\n",
    "b, m = trainAndReport(grid_tuned, data)\n",
    "grid_tuned = gridSearchRidge(data_means)\n",
    "b, m = trainAndReport(grid_tuned, data_means)\n",
    "grid_tuned = gridSearchRidge(data_worst)\n",
    "b, m = trainAndReport(grid_tuned, data_worst)\n",
    "grid_tuned = gridSearchRidge(data_se)\n",
    "b, m = trainAndReport(grid_tuned, data_se)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swmal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
