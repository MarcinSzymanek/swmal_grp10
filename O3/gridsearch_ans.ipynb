{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Hyperparameters and Gridsearch \n",
    "\n",
    "Machine learning models have certain global parameters which decide on the inner workings of the model. An example of this could be a degree of polynomial models, or number of neurons or hidden layers in neural network models. Choosing the optimal hyperparameters for machine learning models manually is extremely time consuming, since it would involve a silly amount of trial and error. In this exercise we will delve into optimizing the hyperparameters using GridSearch and RandomizedSearch.\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "\n",
    "The following python code block sets up our functions to load and set up the data, as well as display results of a gridsearch. See detailed explanation in the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(function setup)\n"
     ]
    }
   ],
   "source": [
    "# Explanation:\n",
    "# This block of code loads the data and defines functions which will be used to present results of gridsearch\n",
    "# GetBestModelCTOR() returns a string with a constructor of the model with the best parameters in it\n",
    "# SearchReport() displays the best models name, its best parameters, score and index. It also asserts that the scoring system used is f1_micro.\n",
    "# ClassificationReport() uses the model to predict with the test data supplied in parameters. It then compares the prediction with true values.\n",
    "# TryKerasImport() asserts that keras module is loaded and ready to be used\n",
    "# LoadAndSetupData() loads the data and reshapes it if needed, chosen by the parameter 'mode' - either iris, mnist or moon dataset\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "# Need this to import libitmal on MY Windows machine. Replace with your GITMAL directory or uncomment if you already have gitmal in pythonpath  - Marcin\n",
    "sys.path.append(\"C:\\\\UNI_2023\\\\ml\\\\gitmal\")\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    # This method \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def TryKerasImport(verbose=True):\n",
    "    \n",
    "    kerasok = True\n",
    "    try:\n",
    "        import keras as keras_try\n",
    "    except:\n",
    "        kerasok = False\n",
    "\n",
    "    tensorflowkerasok = True\n",
    "    try:\n",
    "        import tensorflow.keras as tensorflowkeras_try\n",
    "    except:\n",
    "        tensorflowkerasok = False\n",
    "        \n",
    "    ok = kerasok or tensorflowkerasok\n",
    "    \n",
    "    if not ok and verbose:\n",
    "        if not kerasok:\n",
    "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
    "        if not tensorflowkerasok:\n",
    "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
    "\n",
    "    return ok\n",
    "    \n",
    "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch is a method for tuning the hyperparameters of a model automatically, and GridSearchCV is the scikit-learn class that provides this functionality. To use GridSearchCV you simply supply it with the parameters you want it to test, and with values that you want to check. After running GridSearchCV.fit() on a dataset, gridsearch will go through all the possible combination of hyperparameters with the values supplied to it and compare their scoring using a scoring method of your choice. When that is done, the best parameters and the scores will be available in the GridSearchCV object. \n",
    "\n",
    "The following code block performs the actual grid search and displays the results using the functions supplied in the previous block. See the code comments for detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 2.20 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=2\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.962 (+/-0.093) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.371 (+/-0.038) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 4]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 5]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# This is the model type we will test\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")\n",
    "\n",
    "# These are the parameters that we want gridsearch to evaluate\n",
    "# They are setup as a Dict[name, vals] with name always being a string and vals being whatever type the parameter values are\n",
    "# In this particular example we will compare kernels 'linear' and 'rbf' against each other with 'C' (the regularization parameter) values being 0.1, 1 and 10\n",
    "# This means the model will be fit 2*3 times \n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'), \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# This is the number of KFolds that gridsearch's cross-validation strategy will use\n",
    "CV = 5\n",
    "# Don't display any debug informations\n",
    "VERBOSE = 0\n",
    "\n",
    "# Create gridsearch model with hyperparameters tested specified above\n",
    "# n_jobs is number of jobs ran in parallel when fitting the model: -1 uses all available processors according to sklearn's documentation\n",
    "# job is a somewhat ambiguous term so what exactly this means depends on the backend implementation in sklearn\n",
    "\n",
    "# 'f1_micro' scoring method is defined as the micro-averaged harmonic mean of precision and recall.\n",
    "# According to https://www.visobyte.com/2023/05/precision-recall-and-f1-score-in-object-detection-how-are-they-calculated.html#:~:text=The%20F1%20Score%20is%20a%20harmonic%20mean%20of,%2A%20%28Precision%20%2A%20Recall%29%20%2F%20%28Precision%20%2B%20Recall%29\n",
    "# The precision score measures the rate of false positives and the recall score measures how accurately it predicts/detects \n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Find the best parameters and measure the time to do so using X_train, y_train from the iris dataset\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result. Uses previously defined methods to print data about the model. Also runs the best model to predict (X_test, y_test) validating it.\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for our model are C=1 with linear kernel, having a score of 0.97143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "We will now use grid search to tune parameters of a Stochastic Gradient Descent classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 2.64 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\tbest 'f1_micro' score=0.9904761904761905\n",
      "\tbest index=53\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.01, penalty='l1')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.733 (+/-0.230) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[ 1]: 0.800 (+/-0.152) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 2]: 0.771 (+/-0.304) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 3]: 0.781 (+/-0.245) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 4]: 0.895 (+/-0.140) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[ 5]: 0.867 (+/-0.185) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[ 6]: 0.771 (+/-0.332) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[ 7]: 0.886 (+/-0.222) for {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[ 8]: 0.857 (+/-0.209) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[ 9]: 0.867 (+/-0.229) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[10]: 0.752 (+/-0.194) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[11]: 0.771 (+/-0.185) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[12]: 0.867 (+/-0.212) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[13]: 0.724 (+/-0.093) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[14]: 0.886 (+/-0.187) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[15]: 0.905 (+/-0.104) for {'alpha': 0.0001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[16]: 0.790 (+/-0.205) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[17]: 0.743 (+/-0.166) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[18]: 0.829 (+/-0.230) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[19]: 0.752 (+/-0.291) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[20]: 0.867 (+/-0.229) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[21]: 0.857 (+/-0.217) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[22]: 0.838 (+/-0.411) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[23]: 0.800 (+/-0.185) for {'alpha': 0.0001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[24]: 0.895 (+/-0.194) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[25]: 0.790 (+/-0.245) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[26]: 0.895 (+/-0.236) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[27]: 0.838 (+/-0.214) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[28]: 0.933 (+/-0.143) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[29]: 0.933 (+/-0.097) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[30]: 0.952 (+/-0.060) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[31]: 0.933 (+/-0.097) for {'alpha': 0.001, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[32]: 0.695 (+/-0.097) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[33]: 0.848 (+/-0.152) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[34]: 0.895 (+/-0.152) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[35]: 0.876 (+/-0.155) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[36]: 0.952 (+/-0.000) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[37]: 0.952 (+/-0.060) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[38]: 0.943 (+/-0.071) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[39]: 0.876 (+/-0.166) for {'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[40]: 0.886 (+/-0.196) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[41]: 0.800 (+/-0.175) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[42]: 0.876 (+/-0.097) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[43]: 0.848 (+/-0.111) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[44]: 0.943 (+/-0.071) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[45]: 0.962 (+/-0.071) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[46]: 0.952 (+/-0.104) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[47]: 0.933 (+/-0.097) for {'alpha': 0.001, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[48]: 0.838 (+/-0.129) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[49]: 0.781 (+/-0.177) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[50]: 0.743 (+/-0.114) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[51]: 0.810 (+/-0.276) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[52]: 0.962 (+/-0.071) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[53]: 0.990 (+/-0.038) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[54]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[55]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[56]: 0.952 (+/-0.085) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[57]: 0.867 (+/-0.071) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[58]: 0.867 (+/-0.185) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[59]: 0.876 (+/-0.214) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[60]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[61]: 0.990 (+/-0.038) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[62]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[63]: 0.962 (+/-0.071) for {'alpha': 0.01, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[64]: 0.857 (+/-0.241) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[65]: 0.876 (+/-0.129) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[66]: 0.848 (+/-0.203) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[67]: 0.838 (+/-0.245) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[68]: 0.990 (+/-0.038) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[69]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[70]: 0.943 (+/-0.111) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[71]: 0.952 (+/-0.060) for {'alpha': 0.01, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[72]: 0.790 (+/-0.143) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[73]: 0.752 (+/-0.093) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[74]: 0.771 (+/-0.152) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[75]: 0.810 (+/-0.217) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[76]: 0.838 (+/-0.047) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[77]: 0.876 (+/-0.177) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[78]: 0.800 (+/-0.093) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[79]: 0.771 (+/-0.185) for {'alpha': 0.1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[80]: 0.819 (+/-0.229) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[81]: 0.876 (+/-0.205) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[82]: 0.800 (+/-0.175) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[83]: 0.848 (+/-0.244) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[84]: 0.762 (+/-0.159) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[85]: 0.810 (+/-0.170) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[86]: 0.733 (+/-0.047) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[87]: 0.867 (+/-0.164) for {'alpha': 0.1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[88]: 0.848 (+/-0.093) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[89]: 0.857 (+/-0.190) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[90]: 0.771 (+/-0.348) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[91]: 0.800 (+/-0.244) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[92]: 0.914 (+/-0.126) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[93]: 0.933 (+/-0.097) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[94]: 0.886 (+/-0.143) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[95]: 0.905 (+/-0.120) for {'alpha': 0.1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[96]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[97]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[98]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[99]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[100]: 0.438 (+/-0.212) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[101]: 0.514 (+/-0.298) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[102]: 0.562 (+/-0.291) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[103]: 0.505 (+/-0.328) for {'alpha': 1, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[104]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[105]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[106]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[107]: 0.705 (+/-0.071) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[108]: 0.457 (+/-0.222) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[109]: 0.419 (+/-0.203) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[110]: 0.514 (+/-0.309) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[111]: 0.505 (+/-0.344) for {'alpha': 1, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[112]: 0.790 (+/-0.205) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[113]: 0.752 (+/-0.212) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[114]: 0.676 (+/-0.071) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[115]: 0.648 (+/-0.097) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[116]: 0.324 (+/-0.071) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[117]: 0.333 (+/-0.085) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[118]: 0.314 (+/-0.076) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[119]: 0.333 (+/-0.060) for {'alpha': 1, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[120]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[121]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[122]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[123]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[124]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[125]: 0.305 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[126]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[127]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[128]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[129]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[130]: 0.419 (+/-0.203) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[131]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[132]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[133]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[134]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[135]: 0.333 (+/-0.000) for {'alpha': 100, 'loss': 'log_loss', 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[136]: 0.476 (+/-0.356) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[137]: 0.457 (+/-0.273) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[138]: 0.438 (+/-0.279) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[139]: 0.486 (+/-0.304) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[140]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[141]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[142]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[143]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'perceptron', 'penalty': 'l1', 'tol': 0.1}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.94      0.94      0.94        18\n",
      "           2       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.01, penalty='l1')\n",
      "\n",
      "best: dat=iris, score=0.99048, model=SGDClassifier(alpha=0.01,loss='hinge',penalty='l1',tol=0.001)\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "sgd = linear_model.SGDClassifier()\n",
    "\n",
    "# Notice different parameters, check sklearn documentation for SGDClassifier for description of these parameters\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'perceptron'], \n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 100],\n",
    "    'tol' : [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# This part is almost the same as in previous code block, we just swap the model\n",
    "grid_tuned = GridSearchCV(sgd,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model has a staggering score of 0.99048, even better than the SVC model we used previously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/randomsearch.png\" alt=\"WARNING: could not get image from server.\"  style=\"width:350px\" >\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use these default parameters for the random search, similar to the default parameters for the grid search\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    n_iter=20, \n",
    "    random_state=42, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "but with the two new parameters, `n_iter` and `random_state` added. Since the search-type is now random, the `random_state` gives sense, but essential to random search is the new `n_tier` parameter.\n",
    "\n",
    "So: investigate the `n_iter` parameter...in code and write a conceptual explanation  in text.\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minutes, hours, or days.\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "assert False, \"implement a random search for the SGD classifier..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd MNIST Search Quest II\n",
    "\n",
    "Finally, a search-quest competition: __who can find the best model+hyperparameters for the MNIST dataset?__\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the iris _tiny-data_: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for the exhaustive grid search, or use the faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---__except Neural Networks and KNeighborsClassifier!__. \n",
    "\n",
    "* Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale your input data for some models to perform better.\n",
    "\n",
    "* __REMEMBER__, DO NOT USE any Neural Network models. This also means not to use any `Keras` or `Tensorflow` models...since they outperform most other models, and there are also too many examples on the internet to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "```\n",
    "and paste your currently best model into the message box, for ITMAL group 09 like\n",
    "```\n",
    "Grp09: best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "\n",
    "Grp09: CTOR for best model: SGDClassifier(alpha=1.0, average=False, class_weight=None, early_stopping=False,\n",
    "              epsilon=0.1, eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
    "              learning_rate='invscaling', loss='hinge', max_iter=1000,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "              random_state=None, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "```\n",
    "              \n",
    "on Brightspace: \"L09: Regularisering, optimering og søgning\" | \"Qd MNIST Search Quest\"\n",
    "\n",
    "> https://itundervisning.ase.au.dk/itmal_quest/index.php\n",
    "\n",
    "and, check if your score (for MNIST) is better than the currently best score. Republish if you get a better score than your own previously best. Deadline for submission of scores is the same as the deadline for the O3 journal handin.\n",
    "\n",
    "Remember to provide an ITMAL group name manually, so we can identify a winner: the 1. st price is  cake! \n",
    "\n",
    "For the journal hand-in, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data...and note, that the journal will not be accepted unless it contains information about Your results published on the Brightspace 'Search Quest II' page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "assert False, \"participate in the Search Quest---remember to publish your result(s) on Brightspace.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
