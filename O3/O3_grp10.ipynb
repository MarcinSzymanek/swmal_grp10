{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "In this exercise we aim to shed some light on the Convolution Neural Networks by explaining the primary concepts behind CNN such as convolution layers and feature mapping and implement a CNN model to solve the MNIST numbers problem of recognizing numbers from images of handdrawn numbers.\n",
    "\n",
    "### The basics\n",
    "\n",
    "We have previously worked with multilayer perceptron models - these operated by having one or more layers of neurons, and a good way to attempt to understand CNN is to compare them to MLP models. In MLP the input is connected to one or more 'hidden' layers with some weights, and each subsequent hidden is connected to a layer before it. MLP layer consists of a one dimensional array of neurons. CNN, much like multilayer perceptrons, operate on layers connected to each other. The primary difference is that CNN models utilize a 2x2 matrix of neurons instead of a 1d array. This makes them much more fitting for tasks such as image recognition, where the input image can easily be represented as a 2d array of pixels.\n",
    "\n",
    "Another major difference is that in CNN each neuron in a layer is connected to specific subset of last layer called its receptive field. A receptive field is a i x i matrix, and can be thought of as a neurons 'field of vision'. Each neuron performs a convolution operation on its input based on a kernel (filter matrix) of the same size as the receptive fied.\n",
    "\n",
    "<img align = \"middle\" src=\"convolution_layer_geron.png\" width=\"500\"/>\n",
    "  \n",
    "<figcaption align = \"center\"><b>Example of convolution layer. The \"cone\" coming from the top layer is the receptive field of a neuron of size (3x3). </b> <i>A. Geron, Hands on Machine Learning</i></figcaption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CNN to predict MNIST numbers dataset.\n",
    "\n",
    "We will now try to create a new CNN model using Keras framework and tweak it to predict the numbers out of an image with high accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nastr\\anaconda3\\envs\\swmal\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Methods for downloading and plotting the data\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "# mnist 784 dataset has id == 554: https://www.openml.org/search?type=data&status=active&id=554\n",
    "\n",
    "def MNIST_GetDataSet() -> tuple[pd.DataFrame, pd.Series]:\n",
    "    X, y = fetch_openml(version=\"active\", data_id=554, return_X_y=True)\n",
    "    return X, y\n",
    "\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "X, y_true = MNIST_GetDataSet()\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to feed the data into a cnn model, we have to reshape it into 28x28x1 shape, since each image consists of 28x28 pixels. This is because by default the data is flattened and instead of using 28x28 pixels it's shaped as an array with length of 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 28, 28, 1)\n",
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "\n",
    "# Reshape the data\n",
    "X_all = X.to_numpy()\n",
    "X_all = X_all.reshape(70000, 28, 28, 1)\n",
    "print(X_all.shape)\n",
    "X_train = X_all[0:60000, :, :, :]\n",
    "X_test = X_all[60000: 70000, :, :, :]\n",
    "print(X_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model will have to be a classification of a number between 0 and 9. At this step it is necessary to convert the train/test predictions to one-hot encoded values instead of the strings they are now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    0\n",
      "2    4\n",
      "3    1\n",
      "4    9\n",
      "Name: class, dtype: category\n",
      "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# This performs 'one-hot encoding'\n",
    "from keras.utils import to_categorical\n",
    "# Right now y_true is a vector of string with '0' to '9' in them\n",
    "print(y_true[0:5])\n",
    "\n",
    "# X_train = X_train[0:60000]\n",
    "# X_test = X_[60000:70000]\n",
    "# Convert the string to an array of 0s and 1s, where the \"correct\" value is given a 1 and each other value is a 0\n",
    "y_train = to_categorical(y_true[0:60000])\n",
    "y_test = to_categorical(y_true[60000:70000])\n",
    "\n",
    "print(y_train[0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train some cnn models to try to categorize the numbers. Additionally, since some of the fitting will take a long time, let's measure how long they take to train as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.5536 - accuracy: 0.8935 - val_loss: 0.3012 - val_accuracy: 0.9161\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.3114 - accuracy: 0.9123 - val_loss: 0.3014 - val_accuracy: 0.9162\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2972 - accuracy: 0.9169 - val_loss: 0.2824 - val_accuracy: 0.9189\n",
      "time to fit:  43.5616250038147 s\n",
      "count    3.000000\n",
      "mean     0.917067\n",
      "std      0.001589\n",
      "min      0.916100\n",
      "25%      0.916150\n",
      "50%      0.916200\n",
      "75%      0.917550\n",
      "max      0.918900\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from pandas import Series\n",
    "import time\n",
    "\n",
    "def printAccData(acc : dict[str, float]):\n",
    "    valid_acc = Series(fittingLogs.history[\"val_accuracy\"])\n",
    "    print(valid_acc.describe())\n",
    "    return valid_acc.mean()\n",
    "\n",
    "# Model 1: 3 layers, kernel size 1x1\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(16, kernel_size=1, activation='relu', input_shape=(28,28,1)),\n",
    "    Conv2D(16, kernel_size=1, activation='relu', input_shape=(28,28,1)),\n",
    "    Conv2D(16, kernel_size=1, activation='relu', input_shape=(28,28,1)),\n",
    "])\n",
    "\n",
    "model.add(Flatten())\n",
    "# Dense() creates connection to the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start = time.time()\n",
    "fittingLogs = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "end = time.time()\n",
    "print(\"time to fit: \", end-start, \"s\")\n",
    "final = end-start\n",
    "mean1 = printAccData(fittingLogs)\n",
    "m1_res = Series([mean1, final])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try increasing the kernel size to 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.1978 - accuracy: 0.9504 - val_loss: 0.0695 - val_accuracy: 0.9762\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0578 - accuracy: 0.9823 - val_loss: 0.0635 - val_accuracy: 0.9805\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0404 - accuracy: 0.9872 - val_loss: 0.0847 - val_accuracy: 0.9766\n",
      "time to fit:  74.64559316635132 s\n",
      "count    3.000000\n",
      "mean     0.977767\n",
      "std      0.002376\n",
      "min      0.976200\n",
      "25%      0.976400\n",
      "50%      0.976600\n",
      "75%      0.978550\n",
      "max      0.980500\n",
      "dtype: float64\n",
      "count    3.000000\n",
      "mean     0.977767\n",
      "std      0.002376\n",
      "min      0.976200\n",
      "25%      0.976400\n",
      "50%      0.976600\n",
      "75%      0.978550\n",
      "max      0.980500\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape=(28,28,1))),\n",
    "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#train the model\n",
    "start = time.time()\n",
    "fittingLogs = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "end = time.time()\n",
    "print(\"time to fit: \", end-start, \"s\")\n",
    "mean2 = printAccData(fittingLogs)\n",
    "\n",
    "final = end-start\n",
    "mean2 = printAccData(fittingLogs)\n",
    "m2_res = Series([mean2, final])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a noticable improvement in accuracy! We went from 0.917267 average validation accuracy to 0.977233, although the computation time has rougly doubled. Let's try doubling the filter size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.1619 - accuracy: 0.9597 - val_loss: 0.0669 - val_accuracy: 0.9791\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0551 - accuracy: 0.9834 - val_loss: 0.0802 - val_accuracy: 0.9731\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.0772 - val_accuracy: 0.9777\n",
      "time to fit:  135.4416425228119 s\n",
      "count    3.000000\n",
      "mean     0.976633\n",
      "std      0.003139\n",
      "min      0.973100\n",
      "25%      0.975400\n",
      "50%      0.977700\n",
      "75%      0.978400\n",
      "max      0.979100\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1))),\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#train the model\n",
    "start = time.time()\n",
    "fittingLogs = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "end = time.time()\n",
    "print(\"time to fit: \", end-start, \"s\")\n",
    "mean3 = printAccData(fittingLogs)\n",
    "final = end-start\n",
    "m3_res = Series([mean3, final])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! Our accuracy has barely improved while the computation time for fitting, has doubled. Clearly adding more filters did not yield a noticable improvement to our model. The average accuracy and time can be shown below.\n",
    "\n",
    "\n",
    "A useful article about improving the performance of a model can be found here: https://machinelearningmastery.com/improve-deep-learning-performance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'exec time'}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGuCAYAAABC7AYqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3M0lEQVR4nO3de1RVZf7H8c8B9IAXQFRuhkHWeE8NlUEzLRnRzHSyX2qOkuOlUiplRo3yllZUo4gmSjpqNulo1uTPrMGfkmaNeMOxqanMvKRLAzQCEhUU9u+PlqdOQHEUeATfr7X2Wp1nP3uf7z7OeebDsy/HZlmWJQAAAEPcTBcAAACub4QRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQBArRAaGqqHHnrIdBm4AoQRAECNsXPnTs2aNUu5ubmmS0ElsvHbNACAmmLu3LmaPHmyjh49qtDQUKd1hYWFcnNzU506dcwUhyvGzAiuKxcuXFBJSYnpMgBUAbvdThCpoQgj1WzWrFmy2Wz68ssv9Yc//EE+Pj5q2rSppk+fLsuydOLECQ0cOFDe3t4KDAzUvHnzSu2jsLBQM2fO1M033yy73a6QkBBNmTJFhYWFTv1Wrlypu+66S/7+/rLb7WrTpo2WLFlSan+hoaG655579NFHH6lr167y9PTUTTfdpNdee61CxzR37lx169ZNjRs3lpeXl8LDw/Xmm2+W2ff1119X165dVa9ePTVq1Eh33HGH/u///s+pzz//+U/17NlTDRs2lLe3t7p06aI1a9Y41VvWeeFevXqpV69ejtfbt2+XzWbT2rVrNW3aNDVr1kz16tVTfn6+cnJy9Oc//1nt27dXgwYN5O3trX79+unjjz8utd8LFy5o1qxZ+s1vfiNPT08FBQXpvvvu0+HDh2VZlkJDQzVw4MAyt/Px8dHDDz9coc8RMOXkyZP64x//qICAANntdrVt21YrVqxwrD9//rxatWqlVq1a6fz58472nJwcBQUFqVu3biouLpYklZSUKCkpSW3btpWnp6cCAgL08MMP67vvviv1vr/2Xf+5WbNmafLkyZKksLAw2Ww22Ww2HTt2TFLpseHVV1+VzWbTRx99pMcff1xNmzaVr6+vHn74YRUVFSk3N1cjR45Uo0aN1KhRI02ZMkU/P1ngyvHgKlioVjNnzrQkWR07drSGDRtmLV682Orfv78lyUpMTLRatmxpPfroo9bixYut7t27W5KsDz74wLF9cXGx1adPH6tevXrWxIkTrVdeecWKjY21PDw8rIEDBzq9V5cuXayHHnrImj9/vvXyyy9bffr0sSRZixYtcup34403Wi1btrQCAgKsp556ylq0aJF12223WTabzfr0009/9ZhuuOEGa/z48daiRYusxMREq2vXrpYka9OmTU79Zs2aZUmyunXrZv3lL3+xFixYYD344IPW1KlTHX1Wrlxp2Ww2q127dtZzzz1nJScnW2PGjLFGjBjhVG9MTEypOnr27Gn17NnT8Xrbtm2WJKtNmzZWx44drcTERCshIcEqKCiw9u7da7Vo0cJ68sknrVdeecWaPXu21axZM8vHx8c6efKkYx+XLl2yevfubUmyhg4dai1atMhKSEiw7rrrLmvDhg2WZVnW008/bdWpU8f69ttvnep54403LEnWjh07fvUzBEzJzMy0brjhBiskJMSaPXu2tWTJEuvee++1JFnz58939Nu1a5fl7u5uTZo0ydE2dOhQy8vLyzp48KCjbcyYMZaHh4c1duxYKyUlxZo6dapVv359q0uXLlZRUZGjX0W+6z/38ccfW8OGDXPU9re//c3629/+Zp09e9ayrNJjw8qVKx3jbd++fa3k5GRrxIgRliRrypQp1u233249+OCD1uLFi6177rnHkmStWrXK6T0rejy4OoSRanY5jIwbN87RdunSJeuGG26wbDab9cILLzjav/vuO8vLy8vpy/W3v/3NcnNzsz788EOn/aakpFiSrH/961+OtnPnzpV6/+joaOumm25yarvxxhtL/Z9mdna2ZbfbrT/96U+/ekw/f5+ioiKrXbt21l133eVoO3TokOXm5mb9/ve/t4qLi536l5SUWJZlWbm5uVbDhg2tiIgI6/z582X2uVyvK2HkpptuKlXjhQsXStVx9OhRy263W7Nnz3a0rVixwhEUf+5yTQcPHrQkWUuWLHFaf++991qhoaFOtQPXmtGjR1tBQUHWmTNnnNqHDh1q+fj4OH134uPjLTc3N2vHjh3W+vXrLUlWUlKSY/2HH35oSbJWr17ttK/U1FSn9op+18vyl7/8xZJkHT16tNS68sJIdHS0034jIyMtm81mPfLII462y+PwT8eQih4Prh6naQwZM2aM47/d3d3VuXNnWZal0aNHO9p9fX3VsmVLHTlyxNG2fv16tW7dWq1atdKZM2ccy1133SVJ2rZtm6Ovl5eX47/z8vJ05swZ9ezZU0eOHFFeXp5TPW3atFGPHj0cr5s2bVrqvcvz0/f57rvvlJeXpx49emj//v2O9g0bNqikpEQzZsyQm5vz/+xsNpskacuWLfr+++/15JNPytPTs8w+VyImJsapRumHc8uX6yguLta3336rBg0aqGXLlk51v/XWW2rSpIkee+yxUvu9XNNvfvMbRUREaPXq1Y51OTk5+uc//6nhw4dfVe1AVbIsS2+99ZYGDBggy7KcxpTo6Gjl5eU5fR9mzZqltm3bKiYmRuPHj1fPnj31+OOPO9avX79ePj4++t3vfue0r/DwcDVo0MAxPlXVd708o0ePdtpvREREqfH28jj88/G2IseDq+dhuoDrVfPmzZ1e+/j4yNPTU02aNCnV/u233zpeHzp0SJ9//rmaNm1a5n6zs7Md//2vf/1LM2fOVHp6us6dO+fULy8vTz4+PuXWI0mNGjWq0HnRTZs26dlnn9WBAwecrlv56Zf/8OHDcnNzU5s2bcrdz+HDhyVJ7dq1+9X3dEVYWFiptpKSEi1YsECLFy/W0aNHHee7Jalx48ZONbVs2VIeHr/8VRk5cqRiY2P19ddf68Ybb9T69et18eJFjRgxovIOBKhkp0+fVm5urpYuXaqlS5eW2eenY0rdunW1YsUKdenSRZ6enlq5cqXT9/zQoUPKy8uTv7//L+6rqr7r5SlrvJWkkJCQUu0/HfMqejy4eoQRQ9zd3SvUJsnpgqqSkhK1b99eiYmJZfa9/OU6fPiwevfurVatWikxMVEhISGqW7eu3nvvPc2fP7/UHSUVee+yfPjhh7r33nt1xx13aPHixQoKClKdOnW0cuXKX7wQ7WqU95dTcXFxmcfx81kRSXr++ec1ffp0/fGPf9ScOXPk5+cnNzc3TZw48Yruthk6dKgmTZqk1atX66mnntLrr7+uzp07q2XLli7vC6gul/+3/oc//EExMTFl9rn11ludXm/evFnSDxdoHzp0yCnsl5SUyN/f32mW8KfK+yOqqpU3vpXV/vPx9lo8ntqIMFLDtGjRQh9//LF69+79i9OZ77zzjgoLC7Vx40anvwoqe1rxrbfekqenpzZv3iy73e5oX7lyZam6S0pK9Nlnn6ljx45l7qtFixaSpE8//VQ333xzue/ZqFGjMh949PXXX+umm26qUN1vvvmm7rzzTi1fvtypPTc312l2qkWLFtq9e7cuXrz4i7cM+vn5qX///lq9erWGDx+uf/3rX0pKSqpQLYApTZs2VcOGDVVcXKyoqKhf7f+f//xHs2fP1qhRo3TgwAGNGTNGn3zyiWOmoUWLFtq6dau6d+9e5h8Bl1X0u16W6jztWdHjwdXjmpEa5oEHHtDJkye1bNmyUuvOnz+vgoICST8m/p+m/Ly8vFIh4Wq5u7vLZrM5neY4duyYNmzY4NRv0KBBcnNz0+zZs0vNPFyusU+fPmrYsKESEhJ04cKFMvtIPwwQu3btUlFRkaNt06ZNOnHihEt1/3zWZ/369Tp58qRT2+DBg3XmzBktWrSo1D5+vv2IESP02WefafLkyXJ3d9fQoUMrXA9ggru7uwYPHqy33npLn376aan1p0+fdvz3xYsX9dBDDyk4OFgLFizQq6++qqysLE2aNMnR54EHHlBxcbHmzJlTal+XLl1y/BFR0e96WerXry9J1fIE1ooeD64eMyM1zIgRI/TGG2/okUce0bZt29S9e3cVFxfriy++0BtvvKHNmzerc+fO6tOnj+rWrasBAwbo4Ycf1tmzZ7Vs2TL5+/vrm2++qbR6+vfvr8TERPXt21cPPvigsrOzlZycrJtvvln/+c9/HP1uvvlmPf3005ozZ4569Oih++67T3a7XXv37lVwcLASEhLk7e2t+fPna8yYMerSpYsefPBBNWrUSB9//LHOnTunVatWSfrh4t8333xTffv21QMPPKDDhw/r9ddfd/y1VRH33HOP4y+8bt266ZNPPtHq1atLzayMHDlSr732muLi4rRnzx716NFDBQUF2rp1q8aPH+/0fJH+/furcePGWr9+vfr161fueWbgWvLCCy9o27ZtioiI0NixY9WmTRvl5ORo//792rp1q3JyciTJcV1YWlqaGjZsqFtvvVUzZszQtGnTdP/99+vuu+9Wz5499fDDDyshIUEHDhxQnz59VKdOHR06dEjr16/XggULdP/991f4u16W8PBwSdLTTz+toUOHqk6dOhowYIAjpFSmih4PKoGRe3iuY5dv7T19+rRTe0xMjFW/fv1S/Xv27Gm1bdvWqa2oqMh68cUXrbZt21p2u91q1KiRFR4ebj3zzDNWXl6eo9/GjRutW2+91fL09LRCQ0OtF1980XGr6k9vi7vxxhut/v37l/neP73NrTzLly+3brnlFstut1utWrWyVq5c6TjOn1uxYoXVqVMnR909e/a0tmzZ4tRn48aNVrdu3SwvLy/L29vb6tq1q/X3v//dqc+8efOsZs2aWXa73erevbu1b9++cm/tXb9+fak6Lly4YP3pT3+ygoKCLC8vL6t79+5Wenp6mcd87tw56+mnn7bCwsKsOnXqWIGBgdb9999vHT58uNR+x48fb0my1qxZ86ufG3CtyMrKsiZMmGCFhIQ4/jfeu3dva+nSpZZlWVZGRobl4eFhPfbYY07bXbp0yerSpYsVHBxsfffdd472pUuXWuHh4ZaXl5fVsGFDq3379taUKVOsU6dOOW1fke96WebMmWM1a9bMcnNzcxrPyru1d+/evU7buzoOV/R4cOX4bRqgEk2aNEnLly9XZmam6tWrZ7ocAKgRuGYEqCQXLlzQ66+/rsGDBxNEAMAFXDMCXKXs7Gxt3bpVb775pr799ls98cQTpksCgBqFMAJcpc8++0zDhw+Xv7+/Fi5cWO6tywCAsnHNCAAAMIprRgAAgFGEEQAAYFSNuGakpKREp06dUsOGDfkFVMAAy7L0/fffKzg4uNSvLl+rGDcA8yo6dtSIMHLq1KlSv64IoPqdOHFCN9xwg+kyKoRxA7h2/NrYUSPCSMOGDSX9cDDe3t6GqwGuP/n5+QoJCXF8F2sCxg3AvIqOHTUijFyeYvX29mZQAQyq6OmOHTt26C9/+YsyMjL0zTff6O2339agQYPK7PvII4/olVde0fz58zVx4kRHe05Ojh577DG98847cnNz0+DBg7VgwQI1aNDApVoZNwDzfm3sqBknfwHUKAUFBerQoYOSk5N/sd/bb7+tXbt2KTg4uNS64cOH67///a+2bNmiTZs2aceOHRo3blxVlQzAoBoxMwKgZunXr5/69ev3i31Onjypxx57TJs3b1b//v2d1n3++edKTU3V3r171blzZ0nSyy+/rLvvvltz584tM7wAqLlcnhnZsWOHBgwYoODgYNlsNm3YsOFXt9m+fbtuu+022e123XzzzXr11VevoFQAtUVJSYlGjBihyZMnq23btqXWp6eny9fX1xFEJCkqKkpubm7avXt3mfssLCxUfn6+0wKgZnA5jFR0+vWyo0ePqn///rrzzjt14MABTZw4UWPGjNHmzZtdLhZA7fDiiy/Kw8NDjz/+eJnrMzMz5e/v79Tm4eEhPz8/ZWZmlrlNQkKCfHx8HAt30gA1h8unaSoy/fpTKSkpCgsL07x58yRJrVu31kcffaT58+crOjra1bcHUMNlZGRowYIF2r9/f6U+/yM+Pl5xcXGO15ev4gdw7avyC1jT09MVFRXl1BYdHa309PRyt2G6Fai9PvzwQ2VnZ6t58+by8PCQh4eHvv76a/3pT39SaGioJCkwMFDZ2dlO2126dEk5OTkKDAwsc792u91x5wx30AA1S5WHkczMTAUEBDi1BQQEKD8/X+fPny9zG6ZbgdprxIgR+s9//qMDBw44luDgYE2ePNlx+jYyMlK5ubnKyMhwbPf++++rpKREERERpkoHUEWuybtpmG4FarazZ8/qq6++crw+evSoDhw4ID8/PzVv3lyNGzd26l+nTh0FBgaqZcuWkn44ndu3b1+NHTtWKSkpunjxomJjYzV06FDupAFqoSoPI4GBgcrKynJqy8rKkre3t7y8vMrcxm63y263V3VpAKrIvn37dOeddzpeX/7jIiYmpsJ3061evVqxsbHq3bu346FnCxcurIpyARhW5WEkMjJS7733nlPbli1bFBkZWdVvDcCQXr16ybKsCvc/duxYqTY/Pz+tWbOmEqsCcK1y+ZqRs2fPOs7zSj9Ovx4/flzSD6dYRo4c6ej/yCOP6MiRI5oyZYq++OILLV68WG+88YYmTZpUOUcAAABqNJfDyL59+9SpUyd16tRJ0g/Tr506ddKMGTMkSd98840jmEhSWFiY3n33XW3ZskUdOnTQvHnz9Ne//pXbegEAgCTJZrkyl2pIfn6+fHx8lJeXx+16gAE18TtYE2sGapuKfg/5oTwAAGDUNXlrL64doU++a7qEUo690P/XOwFADWB7pvKeQlyZrJnVe9KEMAJUEoIbAFwZTtMAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjPIwXYAJoU++a7qEUo690N90CQAAGMHMCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAKh0O3bs0IABAxQcHCybzaYNGzY41l28eFFTp05V+/btVb9+fQUHB2vkyJE6deqU0z5ycnI0fPhweXt7y9fXV6NHj9bZs2er+UgAVAfCCIBKV1BQoA4dOig5ObnUunPnzmn//v2aPn269u/fr3/84x86ePCg7r33Xqd+w4cP13//+19t2bJFmzZt0o4dOzRu3LjqOgQA1cjDdAEAap9+/fqpX79+Za7z8fHRli1bnNoWLVqkrl276vjx42revLk+//xzpaamau/evercubMk6eWXX9bdd9+tuXPnKjg4uMqPAUD1YWYEgHF5eXmy2Wzy9fWVJKWnp8vX19cRRCQpKipKbm5u2r17d5n7KCwsVH5+vtMCoGYgjAAw6sKFC5o6daqGDRsmb29vSVJmZqb8/f2d+nl4eMjPz0+ZmZll7ichIUE+Pj6OJSQkpMprB1A5CCMAjLl48aIeeOABWZalJUuWXNW+4uPjlZeX51hOnDhRSVUCqGpcMwLAiMtB5Ouvv9b777/vmBWRpMDAQGVnZzv1v3TpknJychQYGFjm/ux2u+x2e5XWDKBqMDMCoNpdDiKHDh3S1q1b1bhxY6f1kZGRys3NVUZGhqPt/fffV0lJiSIiIqq7XABVjJkRAJXu7Nmz+uqrrxyvjx49qgMHDsjPz09BQUG6//77tX//fm3atEnFxcWO60D8/PxUt25dtW7dWn379tXYsWOVkpKiixcvKjY2VkOHDuVOGqAWIowAqHT79u3TnXfe6XgdFxcnSYqJidGsWbO0ceNGSVLHjh2dttu2bZt69eolSVq9erViY2PVu3dvubm5afDgwVq4cGG11A+gehFGAFS6Xr16ybKsctf/0rrL/Pz8tGbNmsosC8A1imtGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARl1RGElOTlZoaKg8PT0VERGhPXv2/GL/pKQktWzZUl5eXgoJCdGkSZN04cKFKyoYAADULi6HkXXr1ikuLk4zZ87U/v371aFDB0VHRys7O7vM/mvWrNGTTz6pmTNn6vPPP9fy5cu1bt06PfXUU1ddPAAAqPlcDiOJiYkaO3asRo0apTZt2iglJUX16tXTihUryuy/c+dOde/eXQ8++KBCQ0PVp08fDRs27FdnUwAAwPXBpTBSVFSkjIwMRUVF/bgDNzdFRUUpPT29zG26deumjIwMR/g4cuSI3nvvPd19993lvk9hYaHy8/OdFgAAUDt5uNL5zJkzKi4uVkBAgFN7QECAvvjiizK3efDBB3XmzBndfvvtsixLly5d0iOPPPKLp2kSEhL0zDPPuFIaAACooar8bprt27fr+eef1+LFi7V//3794x//0Lvvvqs5c+aUu018fLzy8vIcy4kTJ6q6TAAAYIhLMyNNmjSRu7u7srKynNqzsrIUGBhY5jbTp0/XiBEjNGbMGElS+/btVVBQoHHjxunpp5+Wm1vpPGS322W3210pDQAA1FAuzYzUrVtX4eHhSktLc7SVlJQoLS1NkZGRZW5z7ty5UoHD3d1dkmRZlqv1AgCAWsalmRFJiouLU0xMjDp37qyuXbsqKSlJBQUFGjVqlCRp5MiRatasmRISEiRJAwYMUGJiojp16qSIiAh99dVXmj59ugYMGOAIJQAA4PrlchgZMmSITp8+rRkzZigzM1MdO3ZUamqq46LW48ePO82ETJs2TTabTdOmTdPJkyfVtGlTDRgwQM8991zlHQUAAKixXA4jkhQbG6vY2Ngy123fvt35DTw8NHPmTM2cOfNK3goAANRy/DYNAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijACodDt27NCAAQMUHBwsm82mDRs2OK23LEszZsxQUFCQvLy8FBUVpUOHDjn1ycnJ0fDhw+Xt7S1fX1+NHj1aZ8+ercajAFBdCCMAKl1BQYE6dOig5OTkMte/9NJLWrhwoVJSUrR7927Vr19f0dHRunDhgqPP8OHD9d///ldbtmzRpk2btGPHDo0bN666DgFANfIwXQCA2qdfv37q169fmessy1JSUpKmTZumgQMHSpJee+01BQQEaMOGDRo6dKg+//xzpaamau/evercubMk6eWXX9bdd9+tuXPnKjg4uNqOBUDVY2YEQLU6evSoMjMzFRUV5Wjz8fFRRESE0tPTJUnp6eny9fV1BBFJioqKkpubm3bv3l3mfgsLC5Wfn++0AKgZCCMAqlVmZqYkKSAgwKk9ICDAsS4zM1P+/v5O6z08POTn5+fo83MJCQny8fFxLCEhIVVQPYCqQBgBUCvEx8crLy/PsZw4ccJ0SQAqiDACoFoFBgZKkrKyspzas7KyHOsCAwOVnZ3ttP7SpUvKyclx9Pk5u90ub29vpwVAzUAYAVCtwsLCFBgYqLS0NEdbfn6+du/ercjISElSZGSkcnNzlZGR4ejz/vvvq6SkRBEREdVeM4Cqxd00ACrd2bNn9dVXXzleHz16VAcOHJCfn5+aN2+uiRMn6tlnn9Utt9yisLAwTZ8+XcHBwRo0aJAkqXXr1urbt6/Gjh2rlJQUXbx4UbGxsRo6dCh30gC1EGEEQKXbt2+f7rzzTsfruLg4SVJMTIxeffVVTZkyRQUFBRo3bpxyc3N1++23KzU1VZ6eno5tVq9erdjYWPXu3Vtubm4aPHiwFi5cWO3HAqDqEUYAVLpevXrJsqxy19tsNs2ePVuzZ88ut4+fn5/WrFlTFeUBuMZwzQgAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAqCsKI8nJyQoNDZWnp6ciIiK0Z8+eX+yfm5urCRMmKCgoSHa7Xb/5zW/03nvvXVHBAACgdvFwdYN169YpLi5OKSkpioiIUFJSkqKjo3Xw4EH5+/uX6l9UVKTf/e538vf315tvvqlmzZrp66+/lq+vb2XUDwAAajiXw0hiYqLGjh2rUaNGSZJSUlL07rvvasWKFXryySdL9V+xYoVycnK0c+dO1alTR5IUGhp6dVUDAIBaw6XTNEVFRcrIyFBUVNSPO3BzU1RUlNLT08vcZuPGjYqMjNSECRMUEBCgdu3a6fnnn1dxcXG571NYWKj8/HynBQAA1E4uhZEzZ86ouLhYAQEBTu0BAQHKzMwsc5sjR47ozTffVHFxsd577z1Nnz5d8+bN07PPPlvu+yQkJMjHx8exhISEuFImAACoQar8bpqSkhL5+/tr6dKlCg8P15AhQ/T0008rJSWl3G3i4+OVl5fnWE6cOFHVZQIAAENcumakSZMmcnd3V1ZWllN7VlaWAgMDy9wmKChIderUkbu7u6OtdevWyszMVFFRkerWrVtqG7vdLrvd7kppAACghnJpZqRu3boKDw9XWlqao62kpERpaWmKjIwsc5vu3bvrq6++UklJiaPtyy+/VFBQUJlBBAAAXF9cPk0TFxenZcuWadWqVfr888/16KOPqqCgwHF3zciRIxUfH+/o/+ijjyonJ0dPPPGEvvzyS7377rt6/vnnNWHChMo7CgAAUGO5HEaGDBmiuXPnasaMGerYsaMOHDig1NRUx0Wtx48f1zfffOPoHxISos2bN2vv3r269dZb9fjjj+uJJ54o8zZgANeH4uJiTZ8+XWFhYfLy8lKLFi00Z84cWZbl6GNZlmbMmKGgoCB5eXkpKipKhw4dMlg1gKri8nNGJCk2NlaxsbFlrtu+fXuptsjISO3atetK3gpALfTiiy9qyZIlWrVqldq2bat9+/Zp1KhR8vHx0eOPPy5Jeumll7Rw4UKtWrVKYWFhmj59uqKjo/XZZ5/J09PT8BEAqExXFEYA4Grs3LlTAwcOVP/+/SX98CDEv//9746flrAsS0lJSZo2bZoGDhwoSXrttdcUEBCgDRs2aOjQoaX2WVhYqMLCQsdrnk8E1Bz8UB6AatetWzelpaXpyy+/lCR9/PHH+uijj9SvXz9J0tGjR5WZmen0gEUfHx9FRESU+4BFnk8E1FzMjACodk8++aTy8/PVqlUrubu7q7i4WM8995yGDx8uSY6HKLrygMX4+HjFxcU5Xufn5xNIgBqCMAKg2r3xxhtavXq11qxZo7Zt2+rAgQOaOHGigoODFRMTc0X75PlEQM1FGAFQ7SZPnqwnn3zSce1H+/bt9fXXXyshIUExMTGOhyhmZWUpKCjIsV1WVpY6duxoomQAVYhrRgBUu3PnzsnNzXn4cXd3dzwcMSwsTIGBgU4PWMzPz9fu3bvLfcAigJqLmREA1W7AgAF67rnn1Lx5c7Vt21b//ve/lZiYqD/+8Y+SJJvNpokTJ+rZZ5/VLbfc4ri1Nzg4WIMGDTJbPIBKRxgBUO1efvllTZ8+XePHj1d2draCg4P18MMPa8aMGY4+U6ZMUUFBgcaNG6fc3FzdfvvtSk1N5RkjQC1EGAFQ7Ro2bKikpCQlJSWV28dms2n27NmaPXt29RUGwAiuGQEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUdzaCwCoVLZnbKZLKJM10zJdAsrBzAgAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACwIiTJ0/qD3/4gxo3biwvLy+1b99e+/btc6y3LEszZsxQUFCQvLy8FBUVpUOHDhmsGEBVIYwAqHbfffedunfvrjp16uif//ynPvvsM82bN0+NGjVy9HnppZe0cOFCpaSkaPfu3apfv76io6N14cIFg5UDqAoepgsAcP158cUXFRISopUrVzrawsLCHP9tWZaSkpI0bdo0DRw4UJL02muvKSAgQBs2bNDQoUOrvWYAVYeZEQDVbuPGjercubP+53/+R/7+/urUqZOWLVvmWH/06FFlZmYqKirK0ebj46OIiAilp6eXuc/CwkLl5+c7LQBqBsIIgGp35MgRLVmyRLfccos2b96sRx99VI8//rhWrVolScrMzJQkBQQEOG0XEBDgWPdzCQkJ8vHxcSwhISFVexAAKg1hBEC1Kykp0W233abnn39enTp10rhx4zR27FilpKRc8T7j4+OVl5fnWE6cOFGJFQOoSoQRANUuKChIbdq0cWpr3bq1jh8/LkkKDAyUJGVlZTn1ycrKcqz7ObvdLm9vb6cFQM1AGAFQ7bp3766DBw86tX355Ze68cYbJf1wMWtgYKDS0tIc6/Pz87V7925FRkZWa60Aqh530wCodpMmTVK3bt30/PPP64EHHtCePXu0dOlSLV26VJJks9k0ceJEPfvss7rlllsUFham6dOnKzg4WIMGDTJbPIBKRxgBUO26dOmit99+W/Hx8Zo9e7bCwsKUlJSk4cOHO/pMmTJFBQUFGjdunHJzc3X77bcrNTVVnp6eBisHUBUIIwCMuOeee3TPPfeUu95ms2n27NmaPXt2NVYFwASuGQEAAEZdURhJTk5WaGioPD09FRERoT179lRou7Vr18pms3HOFwAAOLgcRtatW6e4uDjNnDlT+/fvV4cOHRQdHa3s7Oxf3O7YsWP685//rB49elxxsQAAoPZxOYwkJiZq7NixGjVqlNq0aaOUlBTVq1dPK1asKHeb4uJiDR8+XM8884xuuummqyoYAADULi6FkaKiImVkZDj9XoSbm5uioqLK/b0ISZo9e7b8/f01evToCr0PvzEBAMD1w6UwcubMGRUXF7v0exEfffSRli9f7vQjWL+G35gAAOD6UaV303z//fcaMWKEli1bpiZNmlR4O35jAgCA64dLzxlp0qSJ3N3dK/x7EYcPH9axY8c0YMAAR1tJSckPb+zhoYMHD6pFixaltrPb7bLb7a6UBgAAaiiXZkbq1q2r8PBwp9+LKCkpUVpaWpm/F9GqVSt98sknOnDggGO59957deedd+rAgQOcfgEAAK4/gTUuLk4xMTHq3LmzunbtqqSkJBUUFGjUqFGSpJEjR6pZs2ZKSEiQp6en2rVr57S9r6+vJJVqBwAA1yeXw8iQIUN0+vRpzZgxQ5mZmerYsaNSU1MdF7UeP35cbm482BUAAFTMFf02TWxsrGJjY8tct3379l/c9tVXX72StwQAALUUUxgAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjPEwXAAA1ge0Zm+kSymTNtEyXAFw1ZkYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAY98ILL8hms2nixImOtgsXLmjChAlq3LixGjRooMGDBysrK8tckQCqDGEEgFF79+7VK6+8oltvvdWpfdKkSXrnnXe0fv16ffDBBzp16pTuu+8+Q1UCqEqEEQDGnD17VsOHD9eyZcvUqFEjR3teXp6WL1+uxMRE3XXXXQoPD9fKlSu1c+dO7dq1y2DFAKoCYQSAMRMmTFD//v0VFRXl1J6RkaGLFy86tbdq1UrNmzdXenp6mfsqLCxUfn6+0wKgZuChZwCMWLt2rfbv36+9e/eWWpeZmam6devK19fXqT0gIECZmZll7i8hIUHPPPNMVZQKoIoxMwKg2p04cUJPPPGEVq9eLU9Pz0rZZ3x8vPLy8hzLiRMnKmW/AKoeYQRAtcvIyFB2drZuu+02eXh4yMPDQx988IEWLlwoDw8PBQQEqKioSLm5uU7bZWVlKTAwsMx92u12eXt7Oy0AagZO0wCodr1799Ynn3zi1DZq1Ci1atVKU6dOVUhIiOrUqaO0tDQNHjxYknTw4EEdP35ckZGRJkoGUIUIIwCqXcOGDdWuXTuntvr166tx48aO9tGjRysuLk5+fn7y9vbWY489psjISP32t781UTKAKkQYAXBNmj9/vtzc3DR48GAVFhYqOjpaixcvNl0WgCpAGAFwTdi+fbvTa09PTyUnJys5OdlMQQCqDRewAgAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKOuKIwkJycrNDRUnp6eioiI0J49e8rtu2zZMvXo0UONGjVSo0aNFBUV9Yv9AQDA9cXlMLJu3TrFxcVp5syZ2r9/vzp06KDo6GhlZ2eX2X/79u0aNmyYtm3bpvT0dIWEhKhPnz46efLkVRcPAABqPpfDSGJiosaOHatRo0apTZs2SklJUb169bRixYoy+69evVrjx49Xx44d1apVK/31r39VSUmJ0tLSrrp4AABQ87kURoqKipSRkaGoqKgfd+DmpqioKKWnp1doH+fOndPFixfl5+dXbp/CwkLl5+c7LQAAoHZyKYycOXNGxcXFCggIcGoPCAhQZmZmhfYxdepUBQcHOwWan0tISJCPj49jCQkJcaVMAABQg1Tr3TQvvPCC1q5dq7fffluenp7l9ouPj1deXp5jOXHiRDVWCQAAqpOHK52bNGkid3d3ZWVlObVnZWUpMDDwF7edO3euXnjhBW3dulW33nrrL/a12+2y2+2ulAYAAGool2ZG6tatq/DwcKeLTy9fjBoZGVnudi+99JLmzJmj1NRUde7c+cqrBQAAtY5LMyOSFBcXp5iYGHXu3Fldu3ZVUlKSCgoKNGrUKEnSyJEj1axZMyUkJEiSXnzxRc2YMUNr1qxRaGio49qSBg0aqEGDBpV4KAAAoCZyOYwMGTJEp0+f1owZM5SZmamOHTsqNTXVcVHr8ePH5eb244TLkiVLVFRUpPvvv99pPzNnztSsWbOurnoAAFDjuRxGJCk2NlaxsbFlrtu+fbvT62PHjl3JWwAAgOsEv00DAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAKh2CQkJ6tKlixo2bCh/f38NGjRIBw8edOpz4cIFTZgwQY0bN1aDBg00ePDgUr8YDqB2IIwAqHYffPCBJkyYoF27dmnLli26ePGi+vTpo4KCAkefSZMm6Z133tH69ev1wQcf6NSpU7rvvvsMVg2gqlzRb9MAwNVITU11ev3qq6/K399fGRkZuuOOO5SXl6fly5drzZo1uuuuuyRJK1euVOvWrbVr1y799re/NVE2gCrCzAgA4/Ly8iRJfn5+kqSMjAxdvHhRUVFRjj6tWrVS8+bNlZ6eXuY+CgsLlZ+f77QAqBkIIwCMKikp0cSJE9W9e3e1a9dOkpSZmam6devK19fXqW9AQIAyMzPL3E9CQoJ8fHwcS0hISFWXDqCSEEYAGDVhwgR9+umnWrt27VXtJz4+Xnl5eY7lxIkTlVQhgKrGNSMAjImNjdWmTZu0Y8cO3XDDDY72wMBAFRUVKTc312l2JCsrS4GBgWXuy263y263V3XJAKoAMyMAqp1lWYqNjdXbb7+t999/X2FhYU7rw8PDVadOHaWlpTnaDh48qOPHjysyMrK6ywVQxZgZAVDtJkyYoDVr1uh///d/1bBhQ8d1ID4+PvLy8pKPj49Gjx6tuLg4+fn5ydvbW4899pgiIyO5kwaohQgjAKrdkiVLJEm9evVyal+5cqUeeughSdL8+fPl5uamwYMHq7CwUNHR0Vq8eHE1VwqgOhBGAFQ7y7J+tY+np6eSk5OVnJxcDRUBMIlrRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUVcURpKTkxUaGipPT09FRERoz549v9h//fr1atWqlTw9PdW+fXu99957V1QsgOuPq+MNgJrH5TCybt06xcXFaebMmdq/f786dOig6OhoZWdnl9l/586dGjZsmEaPHq1///vfGjRokAYNGqRPP/30qosHULu5Ot4AqJlcDiOJiYkaO3asRo0apTZt2iglJUX16tXTihUryuy/YMEC9e3bV5MnT1br1q01Z84c3XbbbVq0aNFVFw+gdnN1vAFQM3m40rmoqEgZGRmKj493tLm5uSkqKkrp6ellbpOenq64uDintujoaG3YsKHc9yksLFRhYaHjdV5eniQpPz/flXLLVVJ4rlL2U5kq69gqG59VxdXmz+ryfizLqpT9VYSr401Vjxu6UDm7qWzX5PeBz6riavlnVdGxw6UwcubMGRUXFysgIMCpPSAgQF988UWZ22RmZpbZPzMzs9z3SUhI0DPPPFOqPSQkxJVyaxSfJNMV1Bx8VhVX2Z/V999/Lx8fn8rdaTlcHW+ux3FDknxeqJ5/j9qAz6riKvuz+rWxw6UwUl3i4+OdZlNKSkqUk5Ojxo0by2azGazsR/n5+QoJCdGJEyfk7e1tupxrGp+Va67Fz8uyLH3//fcKDg42XUq5asK4IV2b/77XKj6rirtWP6uKjh0uhZEmTZrI3d1dWVlZTu1ZWVkKDAwsc5vAwECX+kuS3W6X3W53avP19XWl1Grj7e19Tf3DX8v4rFxzrX1e1TUjcpmr401NGjeka+/f91rGZ1Vx1+JnVZGxw6ULWOvWravw8HClpaU52kpKSpSWlqbIyMgyt4mMjHTqL0lbtmwptz8ASFc23gComVw+TRMXF6eYmBh17txZXbt2VVJSkgoKCjRq1ChJ0siRI9WsWTMlJCRIkp544gn17NlT8+bNU//+/bV27Vrt27dPS5curdwjAVDr/Np4A6B2cDmMDBkyRKdPn9aMGTOUmZmpjh07KjU11XGR2fHjx+Xm9uOES7du3bRmzRpNmzZNTz31lG655RZt2LBB7dq1q7yjMMBut2vmzJmlpoVRGp+Va/i8fvRr401NxL9vxfFZVVxN/6xsVnXeqwcAAPAz/DYNAAAwijACAACMIowAAACjCCMAAMAowggAADDqmnwc/LXozJkzWrFihdLT0x2/qxMYGKhu3brpoYceUtOmTQ1XCOBaxNgB/Dpu7a2AvXv3Kjo6WvXq1VNUVJTjGQdZWVlKS0vTuXPntHnzZnXu3NlwpaiJzp8/r4yMDPn5+alNmzZO6y5cuKA33nhDI0eONFQdrgZjB6pKrRs3LPyqiIgIa9y4cVZJSUmpdSUlJda4ceOs3/72twYqq5mOHz9ujRo1ynQZ14SDBw9aN954o2Wz2Sw3NzfrjjvusE6dOuVYn5mZabm5uRmsEFeDsaNyMXb8oDaOG1wzUgEff/yxJk2aVOYvf9psNk2aNEkHDhyo/sJqqJycHK1atcp0GdeEqVOnql27dsrOztbBgwfVsGFDde/eXcePHzddGioBY0flYuz4QW0cN7hmpAICAwO1Z88etWrVqsz1e/bsqdGPp65sGzdu/MX1R44cqaZKrn07d+7U1q1b1aRJEzVp0kTvvPOOxo8frx49emjbtm2qX7++6RJxFRg7XMPYUTG1cdwgjFTAn//8Z40bN04ZGRnq3bt3qfO+y5Yt09y5cw1Xee0YNGiQbDabrF+4HKmsvxSvR+fPn5eHx49fQ5vNpiVLlig2NlY9e/bUmjVrDFaHq8XY4RrGjoqpleOG6fNENcXatWutiIgIy8PDw7LZbJbNZrM8PDysiIgIa926dabLu6YEBwdbGzZsKHf9v//97xp3PrOqdOnSxXrttdfKXDdhwgTL19eXz6qGY+yoOMaOiqmN4wbXjFTQkCFDtGvXLp07d04nT57UyZMnde7cOe3atUsPPPCA6fKuKeHh4crIyCh3/a/95XM9+f3vf6+///3vZa5btGiRhg0bxmdVwzF2VBxjR8XUxnGDW3tR6T788EMVFBSob9++Za4vKCjQvn371LNnz2quDMC1jLHj+kUYAQAARnGaBgAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBR/w/MR6tyJyu30AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import DataFrame as df\n",
    "from matplotlib import pyplot as plt\n",
    "results_score = Series([m1_res.iloc[0], m2_res.iloc[0], m3_res.iloc[0]])\n",
    "results_time = Series([m1_res.iloc[1],m2_res.iloc[1], m3_res.iloc[1]])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "x_labels = [\"1x1 kernel\", \"3x3 kernel, 16\", \"3x3 kernel, 32\"]\n",
    "results_score.plot.bar(ax = axes[0], title=\"mean accuracy\", )\n",
    "results_time.plot.bar(ax = axes[1], color=\"green\", title=\"exec time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error\n",
    "\n",
    "This exercise will touch the concepts of generalization error with the starting point being from the figure below.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/dl_generalization_error.png\" alt=\"WARNING: could not get image from server.\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "### Qa) On Generalization Error\n",
    "\n",
    "##### Training/generalization error\n",
    "The training error is the error from the model training on training data.\n",
    "<br>\n",
    "The generalization error is the error from the model being testet on data is hasn't seen during traning. It indicates how well the model can generalize.\n",
    "\n",
    "##### Underfit/overfit zone\n",
    "The left of the graph represent the underfitting area, while the right side represent the overfitting area. In the underfitting area the model is too simple to capture underlying patterns in the data and cannot generalize well. <br>\n",
    "The overfitting area the model is too complex and this results in the model fitting to noise in the training data, making it perform poorly on unseen data. \n",
    "\n",
    "##### Optimal Capacity\n",
    "The optimal capacity is the point where the generalization is lowest. It is the ideal level of model complexity and is the sweet spot between capturing the underlying patterns in the data and avoiding overfitting which results in a model that performs best on unseen data. \n",
    "\n",
    "##### Generalization Gap\n",
    "The gap refers to the difference bewteen the training error and the generalization error. It represent how well the model generalizes to unseen data. A Larger generalization gap indicates poorer genealization performance.\n",
    "\n",
    "##### X-axis and Y-axes\n",
    "The x-axis represent capacity, which measures the complecity of the model. The bigger number of capacity the more complex the model is. <br>\n",
    "The y-axis represent the error which measures the model's performance, with lower value indicating better performance as the error is lower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb A MSE-Epoch/Error Plot\n",
    "\n",
    "##### What is an epoch?\n",
    "It is one pass through a training set during the training of a model. In the example below it represents one iteration over the traning data points during the SGD process. \n",
    "\n",
    "##### Code and description of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (25, 1)\n",
      "X_val  .shape= (25, 1)\n",
      "y_train.shape= (25,)\n",
      "y_val  .shape= (25,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Run code: Qb(part I)\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def GenerateData():\n",
    "    m = 100\n",
    "    X = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split( \\\n",
    "        X[:50], y[:50].ravel(), \\\n",
    "        test_size=0.5, \\\n",
    "        random_state=10)\n",
    "\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"X_val  .shape=\",X_val.shape)\n",
    "print(\"y_train.shape=\",y_train.shape)\n",
    "print(\"y_val  .shape=\",y_val.shape)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled   = poly_scaler.transform(X_val)\n",
    "\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, \"b.\", label=\"All X-y Data\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18, )\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n",
    "\n",
    "print('OK')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 is about generating some synthetic data for the polynomial regression, which happens in the GenerateData() function. The data is then splot into taining and validation sets. In a pipeline a polynomial featurea are created using PolynomaialFeatures with a degree of 90, which makes the model cabeable of capturing very complex patterns. And a standard scaler is also used in the pipeline. Lastly it prints the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...n_epochs= 500\n",
      "  epoch=   0, mse_train=11.85, mse_val=14.58\n",
      "  epoch=   1, mse_train=11.51, mse_val=14.10\n",
      "  epoch=   2, mse_train=11.15, mse_val=13.60\n",
      "  epoch=   3, mse_train=10.81, mse_val=13.13\n",
      "  epoch=   4, mse_train=10.49, mse_val=12.70\n",
      "  epoch=   5, mse_train=10.18, mse_val=12.30\n",
      "  epoch=   6, mse_train=9.88, mse_val=11.92\n",
      "  epoch=   7, mse_train=9.60, mse_val=11.56\n",
      "  epoch=   8, mse_train=9.33, mse_val=11.23\n",
      "  epoch=   9, mse_train=9.07, mse_val=10.91\n",
      "  epoch=  10, mse_train=8.82, mse_val=10.62\n",
      "  epoch=  11, mse_train=8.59, mse_val=10.34\n",
      "  epoch=  12, mse_train=8.36, mse_val=10.07\n",
      "  epoch=  13, mse_train=8.14, mse_val=9.82\n",
      "  epoch=  14, mse_train=7.93, mse_val=9.57\n",
      "  epoch=  15, mse_train=7.72, mse_val=9.34\n",
      "  epoch=  16, mse_train=7.53, mse_val=9.12\n",
      "  epoch=  17, mse_train=7.34, mse_val=8.91\n",
      "  epoch=  18, mse_train=7.16, mse_val=8.71\n",
      "  epoch=  19, mse_train=6.98, mse_val=8.52\n",
      "  epoch=  20, mse_train=6.81, mse_val=8.33\n",
      "  epoch=  21, mse_train=6.65, mse_val=8.15\n",
      "  epoch=  22, mse_train=6.49, mse_val=7.98\n",
      "  epoch=  23, mse_train=6.34, mse_val=7.81\n",
      "  epoch=  24, mse_train=6.19, mse_val=7.65\n",
      "  epoch=  25, mse_train=6.05, mse_val=7.49\n",
      "  epoch=  26, mse_train=5.91, mse_val=7.34\n",
      "  epoch=  27, mse_train=5.77, mse_val=7.20\n",
      "  epoch=  28, mse_train=5.64, mse_val=7.06\n",
      "  epoch=  29, mse_train=5.52, mse_val=6.92\n",
      "  epoch=  30, mse_train=5.40, mse_val=6.79\n",
      "  epoch=  31, mse_train=5.28, mse_val=6.66\n",
      "  epoch=  32, mse_train=5.16, mse_val=6.54\n",
      "  epoch=  33, mse_train=5.05, mse_val=6.42\n",
      "  epoch=  34, mse_train=4.94, mse_val=6.30\n",
      "  epoch=  35, mse_train=4.84, mse_val=6.18\n",
      "  epoch=  36, mse_train=4.73, mse_val=6.07\n",
      "  epoch=  37, mse_train=4.63, mse_val=5.97\n",
      "  epoch=  38, mse_train=4.54, mse_val=5.86\n",
      "  epoch=  39, mse_train=4.44, mse_val=5.76\n",
      "  epoch=  40, mse_train=4.35, mse_val=5.66\n",
      "  epoch=  41, mse_train=4.26, mse_val=5.56\n",
      "  epoch=  42, mse_train=4.17, mse_val=5.47\n",
      "  epoch=  43, mse_train=4.09, mse_val=5.37\n",
      "  epoch=  44, mse_train=4.01, mse_val=5.28\n",
      "  epoch=  45, mse_train=3.93, mse_val=5.20\n",
      "  epoch=  46, mse_train=3.85, mse_val=5.11\n",
      "  epoch=  47, mse_train=3.77, mse_val=5.03\n",
      "  epoch=  48, mse_train=3.70, mse_val=4.95\n",
      "  epoch=  49, mse_train=3.63, mse_val=4.87\n",
      "  epoch=  50, mse_train=3.56, mse_val=4.79\n",
      "  epoch=  51, mse_train=3.49, mse_val=4.71\n",
      "  epoch=  52, mse_train=3.42, mse_val=4.64\n",
      "  epoch=  53, mse_train=3.36, mse_val=4.57\n",
      "  epoch=  54, mse_train=3.29, mse_val=4.49\n",
      "  epoch=  55, mse_train=3.23, mse_val=4.43\n",
      "  epoch=  56, mse_train=3.17, mse_val=4.36\n",
      "  epoch=  57, mse_train=3.11, mse_val=4.29\n",
      "  epoch=  58, mse_train=3.06, mse_val=4.23\n",
      "  epoch=  59, mse_train=3.00, mse_val=4.16\n",
      "  epoch=  60, mse_train=2.95, mse_val=4.10\n",
      "  epoch=  61, mse_train=2.89, mse_val=4.04\n",
      "  epoch=  62, mse_train=2.84, mse_val=3.98\n",
      "  epoch=  63, mse_train=2.79, mse_val=3.93\n",
      "  epoch=  64, mse_train=2.74, mse_val=3.87\n",
      "  epoch=  65, mse_train=2.70, mse_val=3.81\n",
      "  epoch=  66, mse_train=2.65, mse_val=3.76\n",
      "  epoch=  67, mse_train=2.60, mse_val=3.71\n",
      "  epoch=  68, mse_train=2.56, mse_val=3.66\n",
      "  epoch=  69, mse_train=2.52, mse_val=3.60\n",
      "  epoch=  70, mse_train=2.47, mse_val=3.56\n",
      "  epoch=  71, mse_train=2.43, mse_val=3.51\n",
      "  epoch=  72, mse_train=2.39, mse_val=3.46\n",
      "  epoch=  73, mse_train=2.35, mse_val=3.41\n",
      "  epoch=  74, mse_train=2.31, mse_val=3.37\n",
      "  epoch=  75, mse_train=2.28, mse_val=3.32\n",
      "  epoch=  76, mse_train=2.24, mse_val=3.28\n",
      "  epoch=  77, mse_train=2.20, mse_val=3.24\n",
      "  epoch=  78, mse_train=2.17, mse_val=3.20\n",
      "  epoch=  79, mse_train=2.14, mse_val=3.15\n",
      "  epoch=  80, mse_train=2.10, mse_val=3.12\n",
      "  epoch=  81, mse_train=2.07, mse_val=3.08\n",
      "  epoch=  82, mse_train=2.04, mse_val=3.04\n",
      "  epoch=  83, mse_train=2.01, mse_val=3.00\n",
      "  epoch=  84, mse_train=1.98, mse_val=2.96\n",
      "  epoch=  85, mse_train=1.95, mse_val=2.93\n",
      "  epoch=  86, mse_train=1.92, mse_val=2.89\n",
      "  epoch=  87, mse_train=1.89, mse_val=2.86\n",
      "  epoch=  88, mse_train=1.86, mse_val=2.82\n",
      "  epoch=  89, mse_train=1.84, mse_val=2.79\n",
      "  epoch=  90, mse_train=1.81, mse_val=2.76\n",
      "  epoch=  91, mse_train=1.79, mse_val=2.73\n",
      "  epoch=  92, mse_train=1.76, mse_val=2.70\n",
      "  epoch=  93, mse_train=1.74, mse_val=2.67\n",
      "  epoch=  94, mse_train=1.71, mse_val=2.64\n",
      "  epoch=  95, mse_train=1.69, mse_val=2.61\n",
      "  epoch=  96, mse_train=1.67, mse_val=2.58\n",
      "  epoch=  97, mse_train=1.65, mse_val=2.55\n",
      "  epoch=  98, mse_train=1.62, mse_val=2.52\n",
      "  epoch=  99, mse_train=1.60, mse_val=2.50\n",
      "  epoch= 100, mse_train=1.58, mse_val=2.47\n",
      "  epoch= 101, mse_train=1.56, mse_val=2.45\n",
      "  epoch= 102, mse_train=1.54, mse_val=2.42\n",
      "  epoch= 103, mse_train=1.52, mse_val=2.40\n",
      "  epoch= 104, mse_train=1.50, mse_val=2.37\n",
      "  epoch= 105, mse_train=1.49, mse_val=2.35\n",
      "  epoch= 106, mse_train=1.47, mse_val=2.33\n",
      "  epoch= 107, mse_train=1.45, mse_val=2.30\n",
      "  epoch= 108, mse_train=1.43, mse_val=2.28\n",
      "  epoch= 109, mse_train=1.42, mse_val=2.26\n",
      "  epoch= 110, mse_train=1.40, mse_val=2.24\n",
      "  epoch= 111, mse_train=1.38, mse_val=2.22\n",
      "  epoch= 112, mse_train=1.37, mse_val=2.20\n",
      "  epoch= 113, mse_train=1.35, mse_val=2.18\n",
      "  epoch= 114, mse_train=1.34, mse_val=2.16\n",
      "  epoch= 115, mse_train=1.32, mse_val=2.14\n",
      "  epoch= 116, mse_train=1.31, mse_val=2.12\n",
      "  epoch= 117, mse_train=1.30, mse_val=2.10\n",
      "  epoch= 118, mse_train=1.28, mse_val=2.08\n",
      "  epoch= 119, mse_train=1.27, mse_val=2.06\n",
      "  epoch= 120, mse_train=1.26, mse_val=2.05\n",
      "  epoch= 121, mse_train=1.24, mse_val=2.03\n",
      "  epoch= 122, mse_train=1.23, mse_val=2.01\n",
      "  epoch= 123, mse_train=1.22, mse_val=2.00\n",
      "  epoch= 124, mse_train=1.21, mse_val=1.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 125, mse_train=1.19, mse_val=1.97\n",
      "  epoch= 126, mse_train=1.18, mse_val=1.95\n",
      "  epoch= 127, mse_train=1.17, mse_val=1.94\n",
      "  epoch= 128, mse_train=1.16, mse_val=1.92\n",
      "  epoch= 129, mse_train=1.15, mse_val=1.91\n",
      "  epoch= 130, mse_train=1.14, mse_val=1.89\n",
      "  epoch= 131, mse_train=1.13, mse_val=1.88\n",
      "  epoch= 132, mse_train=1.12, mse_val=1.87\n",
      "  epoch= 133, mse_train=1.11, mse_val=1.85\n",
      "  epoch= 134, mse_train=1.10, mse_val=1.84\n",
      "  epoch= 135, mse_train=1.09, mse_val=1.83\n",
      "  epoch= 136, mse_train=1.08, mse_val=1.81\n",
      "  epoch= 137, mse_train=1.07, mse_val=1.80\n",
      "  epoch= 138, mse_train=1.06, mse_val=1.79\n",
      "  epoch= 139, mse_train=1.06, mse_val=1.78\n",
      "  epoch= 140, mse_train=1.05, mse_val=1.77\n",
      "  epoch= 141, mse_train=1.04, mse_val=1.76\n",
      "  epoch= 142, mse_train=1.03, mse_val=1.74\n",
      "  epoch= 143, mse_train=1.02, mse_val=1.73\n",
      "  epoch= 144, mse_train=1.02, mse_val=1.72\n",
      "  epoch= 145, mse_train=1.01, mse_val=1.71\n",
      "  epoch= 146, mse_train=1.00, mse_val=1.70\n",
      "  epoch= 147, mse_train=0.99, mse_val=1.69\n",
      "  epoch= 148, mse_train=0.99, mse_val=1.68\n",
      "  epoch= 149, mse_train=0.98, mse_val=1.67\n",
      "  epoch= 150, mse_train=0.97, mse_val=1.67\n",
      "  epoch= 151, mse_train=0.97, mse_val=1.66\n",
      "  epoch= 152, mse_train=0.96, mse_val=1.65\n",
      "  epoch= 153, mse_train=0.95, mse_val=1.64\n",
      "  epoch= 154, mse_train=0.95, mse_val=1.63\n",
      "  epoch= 155, mse_train=0.94, mse_val=1.62\n",
      "  epoch= 156, mse_train=0.93, mse_val=1.61\n",
      "  epoch= 157, mse_train=0.93, mse_val=1.61\n",
      "  epoch= 158, mse_train=0.92, mse_val=1.60\n",
      "  epoch= 159, mse_train=0.92, mse_val=1.59\n",
      "  epoch= 160, mse_train=0.91, mse_val=1.58\n",
      "  epoch= 161, mse_train=0.91, mse_val=1.58\n",
      "  epoch= 162, mse_train=0.90, mse_val=1.57\n",
      "  epoch= 163, mse_train=0.90, mse_val=1.56\n",
      "  epoch= 164, mse_train=0.89, mse_val=1.56\n",
      "  epoch= 165, mse_train=0.89, mse_val=1.55\n",
      "  epoch= 166, mse_train=0.88, mse_val=1.54\n",
      "  epoch= 167, mse_train=0.88, mse_val=1.54\n",
      "  epoch= 168, mse_train=0.87, mse_val=1.53\n",
      "  epoch= 169, mse_train=0.87, mse_val=1.52\n",
      "  epoch= 170, mse_train=0.86, mse_val=1.52\n",
      "  epoch= 171, mse_train=0.86, mse_val=1.51\n",
      "  epoch= 172, mse_train=0.85, mse_val=1.51\n",
      "  epoch= 173, mse_train=0.85, mse_val=1.50\n",
      "  epoch= 174, mse_train=0.84, mse_val=1.50\n",
      "  epoch= 175, mse_train=0.84, mse_val=1.49\n",
      "  epoch= 176, mse_train=0.84, mse_val=1.49\n",
      "  epoch= 177, mse_train=0.83, mse_val=1.48\n",
      "  epoch= 178, mse_train=0.83, mse_val=1.48\n",
      "  epoch= 179, mse_train=0.82, mse_val=1.47\n",
      "  epoch= 180, mse_train=0.82, mse_val=1.47\n",
      "  epoch= 181, mse_train=0.82, mse_val=1.46\n",
      "  epoch= 182, mse_train=0.81, mse_val=1.46\n",
      "  epoch= 183, mse_train=0.81, mse_val=1.45\n",
      "  epoch= 184, mse_train=0.81, mse_val=1.45\n",
      "  epoch= 185, mse_train=0.80, mse_val=1.45\n",
      "  epoch= 186, mse_train=0.80, mse_val=1.44\n",
      "  epoch= 187, mse_train=0.80, mse_val=1.44\n",
      "  epoch= 188, mse_train=0.79, mse_val=1.43\n",
      "  epoch= 189, mse_train=0.79, mse_val=1.43\n",
      "  epoch= 190, mse_train=0.79, mse_val=1.43\n",
      "  epoch= 191, mse_train=0.78, mse_val=1.42\n",
      "  epoch= 192, mse_train=0.78, mse_val=1.42\n",
      "  epoch= 193, mse_train=0.78, mse_val=1.42\n",
      "  epoch= 194, mse_train=0.77, mse_val=1.41\n",
      "  epoch= 195, mse_train=0.77, mse_val=1.41\n",
      "  epoch= 196, mse_train=0.77, mse_val=1.41\n",
      "  epoch= 197, mse_train=0.77, mse_val=1.40\n",
      "  epoch= 198, mse_train=0.76, mse_val=1.40\n",
      "  epoch= 199, mse_train=0.76, mse_val=1.40\n",
      "  epoch= 200, mse_train=0.76, mse_val=1.40\n",
      "  epoch= 201, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 202, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 203, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 204, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 205, mse_train=0.74, mse_val=1.39\n",
      "  epoch= 206, mse_train=0.74, mse_val=1.38\n",
      "  epoch= 207, mse_train=0.74, mse_val=1.38\n",
      "  epoch= 208, mse_train=0.74, mse_val=1.38\n",
      "  epoch= 209, mse_train=0.73, mse_val=1.38\n",
      "  epoch= 210, mse_train=0.73, mse_val=1.38\n",
      "  epoch= 211, mse_train=0.73, mse_val=1.37\n",
      "  epoch= 212, mse_train=0.73, mse_val=1.37\n",
      "  epoch= 213, mse_train=0.73, mse_val=1.37\n",
      "  epoch= 214, mse_train=0.72, mse_val=1.37\n",
      "  epoch= 215, mse_train=0.72, mse_val=1.37\n",
      "  epoch= 216, mse_train=0.72, mse_val=1.37\n",
      "  epoch= 217, mse_train=0.72, mse_val=1.36\n",
      "  epoch= 218, mse_train=0.72, mse_val=1.36\n",
      "  epoch= 219, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 220, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 221, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 222, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 223, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 224, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 225, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 226, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 227, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 228, mse_train=0.70, mse_val=1.35\n",
      "  epoch= 229, mse_train=0.70, mse_val=1.35\n",
      "  epoch= 230, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 231, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 232, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 233, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 234, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 235, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 236, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 237, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 238, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 239, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 240, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 241, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 242, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 243, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 244, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 245, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 246, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 247, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 248, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 249, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 250, mse_train=0.66, mse_val=1.35\n",
      "  epoch= 251, mse_train=0.66, mse_val=1.35\n",
      "  epoch= 252, mse_train=0.66, mse_val=1.35\n",
      "  epoch= 253, mse_train=0.66, mse_val=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 254, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 255, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 256, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 257, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 258, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 259, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 260, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 261, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 262, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 263, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 264, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 265, mse_train=0.65, mse_val=1.37\n",
      "  epoch= 266, mse_train=0.65, mse_val=1.37\n",
      "  epoch= 267, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 268, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 269, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 270, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 271, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 272, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 273, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 274, mse_train=0.64, mse_val=1.38\n",
      "  epoch= 275, mse_train=0.64, mse_val=1.38\n",
      "  epoch= 276, mse_train=0.64, mse_val=1.38\n",
      "  epoch= 277, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 278, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 279, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 280, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 281, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 282, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 283, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 284, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 285, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 286, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 287, mse_train=0.63, mse_val=1.40\n",
      "  epoch= 288, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 289, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 290, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 291, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 292, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 293, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 294, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 295, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 296, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 297, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 298, mse_train=0.62, mse_val=1.42\n",
      "  epoch= 299, mse_train=0.62, mse_val=1.42\n",
      "  epoch= 300, mse_train=0.61, mse_val=1.42\n",
      "  epoch= 301, mse_train=0.61, mse_val=1.42\n",
      "  epoch= 302, mse_train=0.61, mse_val=1.42\n",
      "  epoch= 303, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 304, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 305, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 306, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 307, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 308, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 309, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 310, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 311, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 312, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 313, mse_train=0.61, mse_val=1.45\n",
      "  epoch= 314, mse_train=0.60, mse_val=1.45\n",
      "  epoch= 315, mse_train=0.60, mse_val=1.45\n",
      "  epoch= 316, mse_train=0.60, mse_val=1.45\n",
      "  epoch= 317, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 318, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 319, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 320, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 321, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 322, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 323, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 324, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 325, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 326, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 327, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 328, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 329, mse_train=0.60, mse_val=1.49\n",
      "  epoch= 330, mse_train=0.59, mse_val=1.49\n",
      "  epoch= 331, mse_train=0.59, mse_val=1.49\n",
      "  epoch= 332, mse_train=0.59, mse_val=1.49\n",
      "  epoch= 333, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 334, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 335, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 336, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 337, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 338, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 339, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 340, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 341, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 342, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 343, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 344, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 345, mse_train=0.59, mse_val=1.53\n",
      "  epoch= 346, mse_train=0.59, mse_val=1.53\n",
      "  epoch= 347, mse_train=0.59, mse_val=1.53\n",
      "  epoch= 348, mse_train=0.58, mse_val=1.53\n",
      "  epoch= 349, mse_train=0.58, mse_val=1.54\n",
      "  epoch= 350, mse_train=0.58, mse_val=1.54\n",
      "  epoch= 351, mse_train=0.58, mse_val=1.54\n",
      "  epoch= 352, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 353, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 354, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 355, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 356, mse_train=0.58, mse_val=1.56\n",
      "  epoch= 357, mse_train=0.58, mse_val=1.56\n",
      "  epoch= 358, mse_train=0.58, mse_val=1.56\n",
      "  epoch= 359, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 360, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 361, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 362, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 363, mse_train=0.58, mse_val=1.58\n",
      "  epoch= 364, mse_train=0.58, mse_val=1.58\n",
      "  epoch= 365, mse_train=0.58, mse_val=1.58\n",
      "  epoch= 366, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 367, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 368, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 369, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 370, mse_train=0.57, mse_val=1.60\n",
      "  epoch= 371, mse_train=0.57, mse_val=1.60\n",
      "  epoch= 372, mse_train=0.57, mse_val=1.60\n",
      "  epoch= 373, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 374, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 375, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 376, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 377, mse_train=0.57, mse_val=1.62\n",
      "  epoch= 378, mse_train=0.57, mse_val=1.62\n",
      "  epoch= 379, mse_train=0.57, mse_val=1.62\n",
      "  epoch= 380, mse_train=0.57, mse_val=1.63\n",
      "  epoch= 381, mse_train=0.57, mse_val=1.63\n",
      "  epoch= 382, mse_train=0.57, mse_val=1.63\n",
      "  epoch= 383, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 384, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 385, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 386, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 387, mse_train=0.57, mse_val=1.65\n",
      "  epoch= 388, mse_train=0.57, mse_val=1.65\n",
      "  epoch= 389, mse_train=0.57, mse_val=1.65\n",
      "  epoch= 390, mse_train=0.57, mse_val=1.66\n",
      "  epoch= 391, mse_train=0.57, mse_val=1.66\n",
      "  epoch= 392, mse_train=0.57, mse_val=1.66\n",
      "  epoch= 393, mse_train=0.57, mse_val=1.67\n",
      "  epoch= 394, mse_train=0.57, mse_val=1.67\n",
      "  epoch= 395, mse_train=0.56, mse_val=1.67\n",
      "  epoch= 396, mse_train=0.56, mse_val=1.67\n",
      "  epoch= 397, mse_train=0.56, mse_val=1.68\n",
      "  epoch= 398, mse_train=0.56, mse_val=1.68\n",
      "  epoch= 399, mse_train=0.56, mse_val=1.68\n",
      "  epoch= 400, mse_train=0.56, mse_val=1.69\n",
      "  epoch= 401, mse_train=0.56, mse_val=1.69\n",
      "  epoch= 402, mse_train=0.56, mse_val=1.69\n",
      "  epoch= 403, mse_train=0.56, mse_val=1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 404, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 405, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 406, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 407, mse_train=0.56, mse_val=1.71\n",
      "  epoch= 408, mse_train=0.56, mse_val=1.71\n",
      "  epoch= 409, mse_train=0.56, mse_val=1.71\n",
      "  epoch= 410, mse_train=0.56, mse_val=1.72\n",
      "  epoch= 411, mse_train=0.56, mse_val=1.72\n",
      "  epoch= 412, mse_train=0.56, mse_val=1.72\n",
      "  epoch= 413, mse_train=0.56, mse_val=1.73\n",
      "  epoch= 414, mse_train=0.56, mse_val=1.73\n",
      "  epoch= 415, mse_train=0.56, mse_val=1.73\n",
      "  epoch= 416, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 417, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 418, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 419, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 420, mse_train=0.56, mse_val=1.75\n",
      "  epoch= 421, mse_train=0.56, mse_val=1.75\n",
      "  epoch= 422, mse_train=0.56, mse_val=1.75\n",
      "  epoch= 423, mse_train=0.56, mse_val=1.76\n",
      "  epoch= 424, mse_train=0.56, mse_val=1.76\n",
      "  epoch= 425, mse_train=0.55, mse_val=1.76\n",
      "  epoch= 426, mse_train=0.55, mse_val=1.77\n",
      "  epoch= 427, mse_train=0.55, mse_val=1.77\n",
      "  epoch= 428, mse_train=0.55, mse_val=1.77\n",
      "  epoch= 429, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 430, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 431, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 432, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 433, mse_train=0.55, mse_val=1.79\n",
      "  epoch= 434, mse_train=0.55, mse_val=1.79\n",
      "  epoch= 435, mse_train=0.55, mse_val=1.79\n",
      "  epoch= 436, mse_train=0.55, mse_val=1.80\n",
      "  epoch= 437, mse_train=0.55, mse_val=1.80\n",
      "  epoch= 438, mse_train=0.55, mse_val=1.80\n",
      "  epoch= 439, mse_train=0.55, mse_val=1.81\n",
      "  epoch= 440, mse_train=0.55, mse_val=1.81\n",
      "  epoch= 441, mse_train=0.55, mse_val=1.81\n",
      "  epoch= 442, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 443, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 444, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 445, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 446, mse_train=0.55, mse_val=1.83\n",
      "  epoch= 447, mse_train=0.55, mse_val=1.83\n",
      "  epoch= 448, mse_train=0.55, mse_val=1.83\n",
      "  epoch= 449, mse_train=0.55, mse_val=1.84\n",
      "  epoch= 450, mse_train=0.55, mse_val=1.84\n",
      "  epoch= 451, mse_train=0.55, mse_val=1.84\n",
      "  epoch= 452, mse_train=0.55, mse_val=1.85\n",
      "  epoch= 453, mse_train=0.55, mse_val=1.85\n",
      "  epoch= 454, mse_train=0.55, mse_val=1.85\n",
      "  epoch= 455, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 456, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 457, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 458, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 459, mse_train=0.55, mse_val=1.87\n",
      "  epoch= 460, mse_train=0.55, mse_val=1.87\n",
      "  epoch= 461, mse_train=0.55, mse_val=1.87\n",
      "  epoch= 462, mse_train=0.55, mse_val=1.88\n",
      "  epoch= 463, mse_train=0.54, mse_val=1.88\n",
      "  epoch= 464, mse_train=0.54, mse_val=1.88\n",
      "  epoch= 465, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 466, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 467, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 468, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 469, mse_train=0.54, mse_val=1.90\n",
      "  epoch= 470, mse_train=0.54, mse_val=1.90\n",
      "  epoch= 471, mse_train=0.54, mse_val=1.90\n",
      "  epoch= 472, mse_train=0.54, mse_val=1.91\n",
      "  epoch= 473, mse_train=0.54, mse_val=1.91\n",
      "  epoch= 474, mse_train=0.54, mse_val=1.91\n",
      "  epoch= 475, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 476, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 477, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 478, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 479, mse_train=0.54, mse_val=1.93\n",
      "  epoch= 480, mse_train=0.54, mse_val=1.93\n",
      "  epoch= 481, mse_train=0.54, mse_val=1.93\n",
      "  epoch= 482, mse_train=0.54, mse_val=1.94\n",
      "  epoch= 483, mse_train=0.54, mse_val=1.94\n",
      "  epoch= 484, mse_train=0.54, mse_val=1.94\n",
      "  epoch= 485, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 486, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 487, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 488, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 489, mse_train=0.54, mse_val=1.96\n",
      "  epoch= 490, mse_train=0.54, mse_val=1.96\n",
      "  epoch= 491, mse_train=0.54, mse_val=1.96\n",
      "  epoch= 492, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 493, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 494, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 495, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 496, mse_train=0.54, mse_val=1.98\n",
      "  epoch= 497, mse_train=0.54, mse_val=1.98\n",
      "  epoch= 498, mse_train=0.54, mse_val=1.98\n",
      "  epoch= 499, mse_train=0.54, mse_val=1.99\n",
      "OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ninat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run code: Qb(part II)\n",
    "\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(\"Training...n_epochs=\",n_epochs)\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    sgd_reg = SGDRegressor(max_iter=1,\n",
    "                           penalty=None,\n",
    "                           eta0=0.0005,\n",
    "                           warm_start=True,\n",
    "                           early_stopping=False,\n",
    "                           learning_rate=\"constant\",\n",
    "                           tol=0,\n",
    "                           random_state=42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict   = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train=mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val  =mean_squared_error(y_val  , y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors  .append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"  epoch={epoch:4d}, mse_train={mse_train:4.2f}, mse_val={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = Train(X_train_poly_scaled, y_train, X_val_poly_scaled, y_val, n_epochs, True)\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 is about training the model. The Train function performs stochastic gradient descent with some parameters. The function iterates over number of epochs. Every iteration/epoch updates the model and calculates the MSE for both the training and validation set. The MSE values are added to train_errors and val_errors. \n",
    "<br>\n",
    "mse_train is the MSE of the model's predictions on the training dataset. It measures how well the model fits the training data.\n",
    "<br>\n",
    "mse_val on the other hand is the MSE of the models' prediction og the validation dataset. It measures how well the model generalizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Run code: Qb(part III)\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(train_errors), \"b--\", linewidth=2, label=\"Training set\")\n",
    "plt.plot(np.sqrt(val_errors), \"g-\", linewidth=3, label=\"Validation set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 3 the code finds the epoch with the lowest validation RMSE using np.argmin(val_errors) and calculates the corresponding RMSE value. The rest of the code is about plotting the training and validation RMSE. This plot visualizes how the RMSE changes over epochs during training. \n",
    "From the plot it is seen that the best epoch is between 200 and 300. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc)  Early Stopping\n",
    "\n",
    "Early stopping is a technique used to prevent overfitting. The idea is to monitor a models performance on a validation set during training and stop training when the performance starts to degrade. This helps to find a balance between model complexity and generalization performance. \n",
    "\n",
    "To implement an early stopping to the polynomal regression there should be an varable that holds the validation error, and stops when this starts to increase instead of descrease. This could be implementet with an if else statement.\n",
    "<br>\n",
    "``` \n",
    "    if val_error < best_val_error:\n",
    "        best_val_error = val_error  \n",
    "        best_epoch = epoch \n",
    "    else:\n",
    "        break\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Explain the Polynomial RMSE-Capacity plot\n",
    "\n",
    "\n",
    "What does the x-axis _Capacity_ and y-axis _RMSE_ represent?\n",
    "\n",
    "Try increasing the model capacity. What happens when you do plots for `degrees` larger than around 10? Relate this with what you found via Qa+b in `capacity_under_overfitting.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating...degrees= range(1, 8)\n",
      "  degree=   1, rmse_training=0.48, rmse_cv=0.64\n",
      "  degree=   2, rmse_training=0.17, rmse_cv=0.24\n",
      "  degree=   3, rmse_training=0.11, rmse_cv=0.14\n",
      "  degree=   4, rmse_training=0.11, rmse_cv=0.21\n",
      "  degree=   5, rmse_training=0.10, rmse_cv=0.31\n",
      "  degree=   6, rmse_training=0.10, rmse_cv=0.34\n",
      "  degree=   7, rmse_training=0.10, rmse_cv=0.44\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Run and review this code\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData():\n",
    "    n_samples = 30\n",
    "    #degrees = [1, 4, 15]\n",
    "    degrees = range(1,8)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y, degrees\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, degrees  = GenerateData()\n",
    "\n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "capacities, rmses_training, rmses_validation= [], [], []\n",
    "for i in range(len(degrees)):\n",
    "    d=degrees[i]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    \n",
    "    Z = X[:, np.newaxis]\n",
    "    pipeline.fit(Z, y)\n",
    "    \n",
    "    p = pipeline.predict(Z)\n",
    "    train_rms = mean_squared_error(y,p)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, Z, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = -scores.mean()\n",
    "    \n",
    "    rmse_training=sqrt(train_rms)\n",
    "    rmse_validation=sqrt(score_mean)\n",
    "    \n",
    "    print(f\"  degree={d:4d}, rmse_training={rmse_training:4.2f}, rmse_cv={rmse_validation:4.2f}\")\n",
    "    \n",
    "    capacities      .append(d)\n",
    "    rmses_training  .append(rmse_training)\n",
    "    rmses_validation.append(rmse_validation)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(capacities, rmses_training,  \"b--\", linewidth=2, label=\"training RMSE\")\n",
    "plt.plot(capacities, rmses_validation,\"g-\",  linewidth=2, label=\"validation RMSE\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Capacity\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE-Capacity plot illustrates the relationship between the models capacity and the Root Mean Square Error for both the training data and validation data.<br>\n",
    "As the models capacity is low it it too simple to capture the underlying patterns in the data, and thereby the error is high in both dataset indicating underfitting. <br>\n",
    "Beyond capacity 3 the models capacity becomes too hight and it starts fitting the noise in the training data, causing the validation error to rise. This is an indication of overfitting. <br>\n",
    "At capacity 3 the model validation error is at its lowest indication that it is capable of capturing the patterns in the data and is best at genealizing at this point. \n",
    "This plot is a great example of the plot at the start of the exercise as it shows the underfitting, overfitting and sweetspot very clearly. \n",
    "<br>\n",
    "If we try to plot with 15 degress as an example, we get this result:\n",
    "<br>\n",
    "![image.png](attachment:image.png)\n",
    "<br>\n",
    "It is clearly shown that there is way too many degrees and the result is an overfitted model. It doesn't deviate from the training data and captures every noise, which result in very bad generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model capacity and under/overfitting\n",
    "\n",
    "We will explore what capacity is, and what relationshipt there is between capacity and under/overfitting. \n",
    "\n",
    "### Qa) Explain the polynomial fitting via code review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating...degrees= [1, 4, 15]\n",
      "  degree=   1, score_mean=-0.41,  PolynomialFeatures(degree=1, include_bias=False)\n",
      "    CV sub-scores:  mean = -0.41,  std = 0.43\n",
      "      CV fold 0  =>  score = -1.2\n",
      "      CV fold 1  =>  score = -0.2\n",
      "      CV fold 2  =>  score = -0.044\n",
      "      CV fold 3  =>  score = -0.36\n",
      "      CV fold 4  =>  score = -0.28\n",
      "      CV fold 5  =>  score = -0.3\n",
      "      CV fold 6  =>  score = -0.18\n",
      "      CV fold 7  =>  score = -0.0086\n",
      "      CV fold 8  =>  score = -0.25\n",
      "      CV fold 9  =>  score = -1.3\n",
      "  degree=   4, score_mean=-0.04,  PolynomialFeatures(degree=4, include_bias=False)\n",
      "    CV sub-scores:  mean = -0.043,  std = 0.071\n",
      "      CV fold 0  =>  score = -0.25\n",
      "      CV fold 1  =>  score = -0.042\n",
      "      CV fold 2  =>  score = -0.027\n",
      "      CV fold 3  =>  score = -0.029\n",
      "      CV fold 4  =>  score = -0.0049\n",
      "      CV fold 5  =>  score = -0.0049\n",
      "      CV fold 6  =>  score = -0.019\n",
      "      CV fold 7  =>  score = -0.038\n",
      "      CV fold 8  =>  score = -0.012\n",
      "      CV fold 9  =>  score = -0.0029\n",
      "  degree=  15, score_mean=-182493841.77,  PolynomialFeatures(degree=15, include_bias=False)\n",
      "    CV sub-scores:  mean = -1.8e+08,  std = 5.5e+08\n",
      "      CV fold 0  =>  score = -1.8e+09\n",
      "      CV fold 1  =>  score = -3.4e+04\n",
      "      CV fold 2  =>  score = -0.0051\n",
      "      CV fold 3  =>  score = -0.007\n",
      "      CV fold 4  =>  score = -0.0092\n",
      "      CV fold 5  =>  score = -0.069\n",
      "      CV fold 6  =>  score = -0.051\n",
      "      CV fold 7  =>  score = -0.079\n",
      "      CV fold 8  =>  score = -0.074\n",
      "      CV fold 9  =>  score = -3.5e+03\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: code review \n",
    "\n",
    "# NOTE: code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData(n_samples = 30):\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = GenerateData()\n",
    "degrees = [1, 4, 15]\n",
    "    \n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    \n",
    "    score_mean = scores.mean()\n",
    "    print(f\"  degree={degrees[i]:4d}, score_mean={score_mean:4.2f},  {polynomial_features}\")   \n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    y_pred = pipeline.predict(X_test[:, np.newaxis])\n",
    "    \n",
    "    # Plotting details\n",
    "    plt.plot(X_test, y_pred          , label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nScore(-MSE) = {:.2e}(+/- {:.2e})\".format(degrees[i], scores.mean(), scores.std()))\n",
    "    \n",
    "    # CEF: loop added, prints each score per CV-fold. \n",
    "    #      NOTICE the sub-means when degree=15!\n",
    "    print(f\"    CV sub-scores:  mean = {scores.mean():.2},  std = {scores.std():.2}\")\n",
    "    for i in range(len(scores)):\n",
    "        print(f\"      CV fold {i}  =>  score = {scores[i]:.2}\")\n",
    "        \n",
    "plt.show()\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Code review\n",
    "The code above performs a polynomial fitting and demonstrates the effects of different polynomial degrees on the model's performance. \n",
    "<br>\n",
    "First the true_fun represent the true function that fits the data.\n",
    "The GenerateData function creates data points by using the true function and adding some noise to it. The number of samples/data points is 30.\n",
    "The code then uses linear regression models with 3 different polynomial features, with degrees (1,4,15) to fit the models to the generated dataset. <br>\n",
    "For each degree it: <br> \n",
    "Creates a pipeline with polynomial_features and linear_regression.  <br>\n",
    "Fit the pipeline to the training data. <br>\n",
    "Evaluates the model using cross-validation with 10 folds. <br>\n",
    "Calculates the mean of the negative MSE.  <br>\n",
    "Generates predictions for a Range of X values (X_test) using the trained mode. <br>\n",
    "Lastly it plots the results.\n",
    "\n",
    "The plot show the 3 different polynomial degrees, each trying to fit the model to the data. The left one, linear regression, clearly doesnt fit the data very well ie. underfitting, and is missing the patterns in the data. The right one is overfitting, which add patterns to the noise and makes the model perform bad at generalization. The middle one is almost a perfect fit as it is almost laying on top of the true function. This means that a degree of 4 is the best fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Explain the capacity and under/overfitting concept\n",
    "\n",
    "What happens when the polynomial degree is low/medium/high with respect to under/overfitting  concepts? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Capacity refers to the models ability to represent complex relationships in the data. The more degrees of freedom or parameters a model has, the higher its capacity. \n",
    "\n",
    "When a model has low capacity, it means it is too simple to capture underlying patterns in data. It underfit the data, resulting in a model that cannot follow the true function. Underfit models have a high training error and high validation error. \n",
    "\n",
    "When a model has high capacity, it means it is too complex and can fit the training data too well, and unintentionally captures noise and outliers which means it is overfitting. High complexity leads to bas generalization. Overfit models have low training error but high validation error. \n",
    "\n",
    "With a low-degree polynomial, the model is too simple to capture the patterns as seen in the left plot. <br>\n",
    "With a hight-degree, the model is too complex and overfits the training data, as seen in the right plot. <br>\n",
    "With medium-degree the model has an appropiate level of complexity to capture underlying patterns without being sensitive to noise. This is seen in the middle plot. \n",
    "\n",
    "The choice of degree is crucial in determining wether the model underfits, goodfit or overfit the data. The plot illustrates this very good. Finding the right balance in model complexity is key to well-performing machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Score method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"neg_mean_squared_error\" is used in scikit-learn to follow a convention where scoring functions are treated as utility functions to be minimized, rather than cost functions to be maximized. The \"neg\" prefix signifies that the scoring function should be minimized. If it is set to mean_squared_error, an exception is raised with InvalidParameterError.\n",
    "\n",
    "The degree 15 model have an extremly low MSE score, which indicates better fit. But it overfits the training data and fits every data point perfectly. This just means that MSE is not enough to tell if a model is good or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Hyperparameters and Gridsearch \n",
    "\n",
    "Machine learning models have certain global parameters which decide on the inner workings of the model. An example of this could be a degree of polynomial models, or number of neurons or hidden layers in neural network models. Choosing the optimal hyperparameters for machine learning models manually is extremely time consuming, since it would involve a silly amount of trial and error. In this exercise we will delve into optimizing the hyperparameters using GridSearch and RandomizedSearch.\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "\n",
    "The following python code block sets up our functions to load and set up the data, as well as display results of a gridsearch. See detailed explanation in the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(function setup, hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: importing 'keras' failed\n",
      "WARNING: importing 'tensorflow.keras' failed\n"
     ]
    }
   ],
   "source": [
    "# Explanation:\n",
    "# This block of code loads the data and defines functions which will be used to present results of gridsearch\n",
    "# GetBestModelCTOR() returns a string with a constructor of the model with the best parameters in it\n",
    "# SearchReport() displays the best models name, its best parameters, score and index. It also asserts that the scoring system used is f1_micro.\n",
    "# ClassificationReport() uses the model to predict with the test data supplied in parameters. It then compares the prediction with true values.\n",
    "# TryKerasImport() asserts that keras module is loaded and ready to be used\n",
    "# LoadAndSetupData() loads the data and reshapes it if needed, chosen by the parameter 'mode' - either iris, mnist or moon dataset\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "# Need this to import libitmal on MY Windows machine. Replace with your GITMAL directory or uncomment if you already have gitmal in pythonpath  - Marcin\n",
    "sys.path.append(\"C:\\\\UNI_2023\\\\ml\\\\gitmal\")\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model):\n",
    "\n",
    "    # This method\n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"\n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'\n",
    "            return ret_str\n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')'\n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "\n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "\n",
    "    global currmode\n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"\n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_\n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print()\n",
    "\n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)\n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "\n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "\n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "\n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "\n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def TryKerasImport(verbose=True):\n",
    "\n",
    "    kerasok = True\n",
    "    try:\n",
    "        import keras as keras_try\n",
    "    except:\n",
    "        kerasok = False\n",
    "\n",
    "    tensorflowkerasok = True\n",
    "    try:\n",
    "        import tensorflow.keras as tensorflowkeras_try\n",
    "    except:\n",
    "        tensorflowkerasok = False\n",
    "\n",
    "    ok = kerasok or tensorflowkerasok\n",
    "\n",
    "    if not ok and verbose:\n",
    "        if not kerasok:\n",
    "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
    "        if not tensorflowkerasok:\n",
    "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
    "\n",
    "    return ok\n",
    "\n",
    "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch is a method for tuning the hyperparameters of a model automatically, and GridSearchCV is the scikit-learn class that provides this functionality. To use GridSearchCV you simply supply it with the parameters you want it to test, and with values that you want to check. After running GridSearchCV.fit() on a dataset, gridsearch will go through all the possible combination of hyperparameters with the values supplied to it and compare their scoring using a scoring method of your choice. When that is done, the best parameters and the scores will be available in the GridSearchCV object. \n",
    "\n",
    "The following code block performs the actual grid search and displays the results using the functions supplied in the previous block. See the code comments for detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 1.98 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=2\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.962 (+/-0.093) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.371 (+/-0.038) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 4]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 5]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# This is the model type we will test\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")\n",
    "\n",
    "# These are the parameters that we want gridsearch to evaluate\n",
    "# They are setup as a Dict[name, vals] with name always being a string and vals being whatever type the parameter values are\n",
    "# In this particular example we will compare kernels 'linear' and 'rbf' against each other with 'C' (the regularization parameter) values being 0.1, 1 and 10\n",
    "# This means the model will be fit 2*3 times\n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'),\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# This is the number of KFolds that gridsearch's cross-validation strategy will use\n",
    "CV = 5\n",
    "# Don't display any debug informations\n",
    "VERBOSE = 0\n",
    "\n",
    "# Create gridsearch model with hyperparameters tested specified above\n",
    "# n_jobs is number of jobs ran in parallel when fitting the model: -1 uses all available processors according to sklearn's documentation\n",
    "# job is a somewhat ambiguous term so what exactly this means depends on the backend implementation in sklearn\n",
    "\n",
    "# 'f1_micro' scoring method is defined as the micro-averaged harmonic mean of precision and recall.\n",
    "# According to https://www.visobyte.com/2023/05/precision-recall-and-f1-score-in-object-detection-how-are-they-calculated.html#:~:text=The%20F1%20Score%20is%20a%20harmonic%20mean%20of,%2A%20%28Precision%20%2A%20Recall%29%20%2F%20%28Precision%20%2B%20Recall%29\n",
    "# The precision score measures the rate of false positives and the recall score measures how accurately it predicts/detects\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Find the best parameters and measure the time to do so using X_train, y_train from the iris dataset\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result. Uses previously defined methods to print data about the model. Also runs the best model to predict (X_test, y_test) validating it.\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for our model are C=1 with linear kernel, having a score of 0.97143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "We will now use grid search to tune parameters of a Stochastic Gradient Descent classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 120.97 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\tbest 'f1_micro' score=0.9904761904761905\n",
      "\tbest index=222\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.001, loss='modified_huber', max_iter=100000, penalty='l1',\n",
      "              tol=0.01)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.790 (+/-0.411) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[ 1]: 0.686 (+/-0.273) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 2]: 0.800 (+/-0.194) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 3]: 0.771 (+/-0.220) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 4]: 0.819 (+/-0.279) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[ 5]: 0.952 (+/-0.085) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[ 6]: 0.800 (+/-0.265) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[ 7]: 0.781 (+/-0.187) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[ 8]: 0.781 (+/-0.305) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[ 9]: 0.790 (+/-0.230) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[10]: 0.876 (+/-0.187) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[11]: 0.800 (+/-0.152) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[12]: 0.676 (+/-0.338) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[13]: 0.800 (+/-0.229) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[14]: 0.781 (+/-0.245) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[15]: 0.667 (+/-0.434) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[16]: 0.829 (+/-0.196) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[17]: 0.905 (+/-0.200) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[18]: 0.781 (+/-0.143) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[19]: 0.895 (+/-0.185) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[20]: 0.876 (+/-0.245) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[21]: 0.810 (+/-0.170) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[22]: 0.819 (+/-0.152) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[23]: 0.829 (+/-0.222) for {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[24]: 0.762 (+/-0.248) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[25]: 0.743 (+/-0.339) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[26]: 0.733 (+/-0.420) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[27]: 0.733 (+/-0.129) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[28]: 0.867 (+/-0.229) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[29]: 0.819 (+/-0.315) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[30]: 0.752 (+/-0.315) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[31]: 0.810 (+/-0.200) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[32]: 0.819 (+/-0.194) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[33]: 0.695 (+/-0.398) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[34]: 0.810 (+/-0.200) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[35]: 0.781 (+/-0.166) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[36]: 0.781 (+/-0.245) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[37]: 0.619 (+/-0.386) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[38]: 0.857 (+/-0.181) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[39]: 0.743 (+/-0.187) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[40]: 0.914 (+/-0.164) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[41]: 0.848 (+/-0.258) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[42]: 0.819 (+/-0.164) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[43]: 0.867 (+/-0.220) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[44]: 0.733 (+/-0.222) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[45]: 0.752 (+/-0.251) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[46]: 0.905 (+/-0.209) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[47]: 0.667 (+/-0.545) for {'alpha': 0.0001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[48]: 0.848 (+/-0.203) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[49]: 0.800 (+/-0.126) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[50]: 0.752 (+/-0.272) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[51]: 0.914 (+/-0.203) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[52]: 0.838 (+/-0.245) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[53]: 0.781 (+/-0.177) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[54]: 0.790 (+/-0.230) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[55]: 0.857 (+/-0.241) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[56]: 0.810 (+/-0.200) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[57]: 0.771 (+/-0.185) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[58]: 0.819 (+/-0.126) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[59]: 0.781 (+/-0.205) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[60]: 0.876 (+/-0.205) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[61]: 0.848 (+/-0.220) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[62]: 0.733 (+/-0.245) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[63]: 0.648 (+/-0.273) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[64]: 0.838 (+/-0.155) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[65]: 0.857 (+/-0.085) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[66]: 0.895 (+/-0.194) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[67]: 0.895 (+/-0.164) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[68]: 0.781 (+/-0.214) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[69]: 0.724 (+/-0.505) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[70]: 0.743 (+/-0.260) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[71]: 0.781 (+/-0.253) for {'alpha': 0.0001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[72]: 0.886 (+/-0.230) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[73]: 0.667 (+/-0.120) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[74]: 0.733 (+/-0.253) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[75]: 0.629 (+/-0.304) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[76]: 0.876 (+/-0.222) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[77]: 0.733 (+/-0.214) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[78]: 0.771 (+/-0.315) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[79]: 0.895 (+/-0.111) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[80]: 0.762 (+/-0.135) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[81]: 0.781 (+/-0.328) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[82]: 0.800 (+/-0.353) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[83]: 0.743 (+/-0.114) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[84]: 0.790 (+/-0.196) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[85]: 0.829 (+/-0.196) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[86]: 0.857 (+/-0.209) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[87]: 0.800 (+/-0.185) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[88]: 0.819 (+/-0.244) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[89]: 0.914 (+/-0.164) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[90]: 0.867 (+/-0.203) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[91]: 0.838 (+/-0.196) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[92]: 0.743 (+/-0.230) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[93]: 0.733 (+/-0.177) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[94]: 0.705 (+/-0.363) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[95]: 0.924 (+/-0.097) for {'alpha': 0.0001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[96]: 0.352 (+/-0.097) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[97]: 0.400 (+/-0.354) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[98]: 0.495 (+/-0.322) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[99]: 0.352 (+/-0.076) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[100]: 0.343 (+/-0.229) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[101]: 0.314 (+/-0.076) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[102]: 0.305 (+/-0.129) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[103]: 0.257 (+/-0.293) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[104]: 0.429 (+/-0.263) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[105]: 0.343 (+/-0.071) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[106]: 0.305 (+/-0.076) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[107]: 0.400 (+/-0.316) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[108]: 0.314 (+/-0.076) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[109]: 0.324 (+/-0.071) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[110]: 0.362 (+/-0.047) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[111]: 0.295 (+/-0.203) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[112]: 0.295 (+/-0.038) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[113]: 0.324 (+/-0.111) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[114]: 0.314 (+/-0.047) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[115]: 0.305 (+/-0.166) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[116]: 0.352 (+/-0.143) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[117]: 0.362 (+/-0.047) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[118]: 0.286 (+/-0.289) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[119]: 0.381 (+/-0.241) for {'alpha': 0.0001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[120]: 0.448 (+/-0.245) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[121]: 0.467 (+/-0.368) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[122]: 0.400 (+/-0.222) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[123]: 0.267 (+/-0.273) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[124]: 0.381 (+/-0.289) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[125]: 0.352 (+/-0.097) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[126]: 0.438 (+/-0.220) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[127]: 0.333 (+/-0.060) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[128]: 0.448 (+/-0.267) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[129]: 0.410 (+/-0.267) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[130]: 0.457 (+/-0.280) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[131]: 0.276 (+/-0.236) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[132]: 0.438 (+/-0.309) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[133]: 0.429 (+/-0.313) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[134]: 0.381 (+/-0.200) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[135]: 0.429 (+/-0.313) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[136]: 0.390 (+/-0.185) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[137]: 0.419 (+/-0.291) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[138]: 0.257 (+/-0.280) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[139]: 0.429 (+/-0.289) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[140]: 0.410 (+/-0.214) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[141]: 0.381 (+/-0.135) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[142]: 0.448 (+/-0.322) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[143]: 0.448 (+/-0.364) for {'alpha': 0.0001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[144]: 0.876 (+/-0.177) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[145]: 0.848 (+/-0.236) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[146]: 0.752 (+/-0.164) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[147]: 0.876 (+/-0.214) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[148]: 0.952 (+/-0.085) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[149]: 0.943 (+/-0.111) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[150]: 0.895 (+/-0.140) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[151]: 0.962 (+/-0.071) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[152]: 0.933 (+/-0.097) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[153]: 0.790 (+/-0.230) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[154]: 0.771 (+/-0.229) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[155]: 0.886 (+/-0.196) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[156]: 0.895 (+/-0.229) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[157]: 0.800 (+/-0.212) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[158]: 0.819 (+/-0.265) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[159]: 0.867 (+/-0.236) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[160]: 0.952 (+/-0.060) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[161]: 0.876 (+/-0.196) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[162]: 0.933 (+/-0.129) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[163]: 0.943 (+/-0.071) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[164]: 0.781 (+/-0.177) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[165]: 0.876 (+/-0.230) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[166]: 0.933 (+/-0.097) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[167]: 0.857 (+/-0.104) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[168]: 0.867 (+/-0.220) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[169]: 0.771 (+/-0.194) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[170]: 0.857 (+/-0.217) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[171]: 0.857 (+/-0.269) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[172]: 0.886 (+/-0.114) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[173]: 0.924 (+/-0.097) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[174]: 0.952 (+/-0.060) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[175]: 0.924 (+/-0.114) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[176]: 0.933 (+/-0.114) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[177]: 0.838 (+/-0.333) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[178]: 0.905 (+/-0.289) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[179]: 0.867 (+/-0.152) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[180]: 0.790 (+/-0.453) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[181]: 0.848 (+/-0.265) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[182]: 0.867 (+/-0.236) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[183]: 0.848 (+/-0.164) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[184]: 0.962 (+/-0.071) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[185]: 0.943 (+/-0.071) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[186]: 0.933 (+/-0.047) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[187]: 0.971 (+/-0.076) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[188]: 0.800 (+/-0.229) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[189]: 0.867 (+/-0.220) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[190]: 0.876 (+/-0.214) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[191]: 0.781 (+/-0.245) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[192]: 0.781 (+/-0.214) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[193]: 0.838 (+/-0.196) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[194]: 0.800 (+/-0.298) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[195]: 0.733 (+/-0.196) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[196]: 0.952 (+/-0.104) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[197]: 0.943 (+/-0.071) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[198]: 0.962 (+/-0.111) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[199]: 0.924 (+/-0.114) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[200]: 0.829 (+/-0.230) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[201]: 0.905 (+/-0.159) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[202]: 0.857 (+/-0.217) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[203]: 0.914 (+/-0.212) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[204]: 0.876 (+/-0.177) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[205]: 0.857 (+/-0.248) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[206]: 0.800 (+/-0.152) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[207]: 0.838 (+/-0.205) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[208]: 0.971 (+/-0.047) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[209]: 0.943 (+/-0.038) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[210]: 0.933 (+/-0.047) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[211]: 0.924 (+/-0.214) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[212]: 0.867 (+/-0.203) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[213]: 0.867 (+/-0.212) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[214]: 0.952 (+/-0.060) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[215]: 0.848 (+/-0.164) for {'alpha': 0.001, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[216]: 0.819 (+/-0.212) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[217]: 0.829 (+/-0.322) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[218]: 0.876 (+/-0.177) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[219]: 0.867 (+/-0.111) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[220]: 0.962 (+/-0.071) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[221]: 0.924 (+/-0.155) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[222]: 0.990 (+/-0.038) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[223]: 0.971 (+/-0.047) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[224]: 0.876 (+/-0.267) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[225]: 0.867 (+/-0.185) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[226]: 0.857 (+/-0.256) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[227]: 0.790 (+/-0.155) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[228]: 0.829 (+/-0.267) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[229]: 0.838 (+/-0.205) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[230]: 0.857 (+/-0.276) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[231]: 0.857 (+/-0.241) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[232]: 0.924 (+/-0.129) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[233]: 0.933 (+/-0.047) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[234]: 0.895 (+/-0.140) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[235]: 0.952 (+/-0.000) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[236]: 0.810 (+/-0.200) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[237]: 0.905 (+/-0.200) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[238]: 0.943 (+/-0.111) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[239]: 0.876 (+/-0.267) for {'alpha': 0.001, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[240]: 0.333 (+/-0.085) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[241]: 0.333 (+/-0.085) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[242]: 0.324 (+/-0.423) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[243]: 0.314 (+/-0.155) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[244]: 0.276 (+/-0.285) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[245]: 0.286 (+/-0.104) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[246]: 0.210 (+/-0.322) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[247]: 0.276 (+/-0.285) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[248]: 0.410 (+/-0.293) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[249]: 0.248 (+/-0.251) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[250]: 0.505 (+/-0.322) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[251]: 0.324 (+/-0.220) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[252]: 0.400 (+/-0.273) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[253]: 0.200 (+/-0.332) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[254]: 0.343 (+/-0.071) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[255]: 0.276 (+/-0.236) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[256]: 0.267 (+/-0.222) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[257]: 0.295 (+/-0.298) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[258]: 0.314 (+/-0.076) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[259]: 0.333 (+/-0.085) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[260]: 0.257 (+/-0.260) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[261]: 0.352 (+/-0.097) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[262]: 0.314 (+/-0.047) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[263]: 0.343 (+/-0.071) for {'alpha': 0.001, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[264]: 0.638 (+/-0.129) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[265]: 0.600 (+/-0.322) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[266]: 0.467 (+/-0.338) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[267]: 0.400 (+/-0.273) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[268]: 0.552 (+/-0.344) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[269]: 0.495 (+/-0.322) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[270]: 0.667 (+/-0.341) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[271]: 0.486 (+/-0.304) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[272]: 0.524 (+/-0.330) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[273]: 0.600 (+/-0.273) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[274]: 0.571 (+/-0.319) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[275]: 0.362 (+/-0.177) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[276]: 0.619 (+/-0.301) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[277]: 0.733 (+/-0.311) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[278]: 0.543 (+/-0.311) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[279]: 0.448 (+/-0.411) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[280]: 0.543 (+/-0.311) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[281]: 0.505 (+/-0.393) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[282]: 0.562 (+/-0.415) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[283]: 0.571 (+/-0.486) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[284]: 0.638 (+/-0.557) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[285]: 0.457 (+/-0.424) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[286]: 0.505 (+/-0.230) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[287]: 0.381 (+/-0.159) for {'alpha': 0.001, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[288]: 0.914 (+/-0.111) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[289]: 0.905 (+/-0.104) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[290]: 0.838 (+/-0.214) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[291]: 0.781 (+/-0.205) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[292]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[293]: 0.990 (+/-0.038) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[294]: 0.971 (+/-0.076) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[295]: 0.962 (+/-0.038) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[296]: 0.838 (+/-0.196) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[297]: 0.886 (+/-0.076) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[298]: 0.914 (+/-0.111) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[299]: 0.838 (+/-0.205) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[300]: 0.924 (+/-0.097) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[301]: 0.895 (+/-0.194) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[302]: 0.876 (+/-0.177) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[303]: 0.867 (+/-0.229) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[304]: 0.952 (+/-0.085) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[305]: 0.962 (+/-0.038) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[306]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[307]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[308]: 0.724 (+/-0.038) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[309]: 0.790 (+/-0.230) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[310]: 0.838 (+/-0.230) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[311]: 0.781 (+/-0.238) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[312]: 0.876 (+/-0.143) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[313]: 0.886 (+/-0.177) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[314]: 0.848 (+/-0.185) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[315]: 0.838 (+/-0.245) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[316]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[317]: 0.962 (+/-0.093) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[318]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[319]: 0.952 (+/-0.085) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[320]: 0.914 (+/-0.071) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[321]: 0.876 (+/-0.245) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[322]: 0.924 (+/-0.196) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[323]: 0.876 (+/-0.214) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[324]: 0.857 (+/-0.159) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[325]: 0.886 (+/-0.230) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[326]: 0.876 (+/-0.097) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[327]: 0.952 (+/-0.060) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[328]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[329]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[330]: 0.962 (+/-0.071) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[331]: 0.962 (+/-0.038) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[332]: 0.895 (+/-0.152) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[333]: 0.905 (+/-0.159) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[334]: 0.829 (+/-0.230) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[335]: 0.876 (+/-0.230) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[336]: 0.810 (+/-0.276) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[337]: 0.905 (+/-0.181) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[338]: 0.810 (+/-0.159) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[339]: 0.848 (+/-0.220) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[340]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[341]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[342]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[343]: 0.952 (+/-0.085) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[344]: 0.905 (+/-0.170) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[345]: 0.895 (+/-0.140) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[346]: 0.867 (+/-0.212) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[347]: 0.905 (+/-0.241) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[348]: 0.838 (+/-0.177) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[349]: 0.886 (+/-0.129) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[350]: 0.848 (+/-0.203) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[351]: 0.848 (+/-0.265) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[352]: 0.933 (+/-0.076) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[353]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[354]: 0.952 (+/-0.104) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[355]: 0.933 (+/-0.143) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[356]: 0.819 (+/-0.258) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[357]: 0.886 (+/-0.260) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[358]: 0.857 (+/-0.181) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[359]: 0.800 (+/-0.272) for {'alpha': 0.01, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[360]: 0.838 (+/-0.143) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[361]: 0.724 (+/-0.272) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[362]: 0.829 (+/-0.349) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[363]: 0.848 (+/-0.353) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[364]: 0.981 (+/-0.047) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[365]: 0.952 (+/-0.060) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[366]: 0.962 (+/-0.038) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[367]: 0.952 (+/-0.060) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[368]: 0.933 (+/-0.047) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[369]: 0.895 (+/-0.126) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[370]: 0.829 (+/-0.230) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[371]: 0.819 (+/-0.304) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[372]: 0.800 (+/-0.140) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[373]: 0.838 (+/-0.222) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[374]: 0.924 (+/-0.129) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[375]: 0.829 (+/-0.166) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[376]: 0.971 (+/-0.076) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[377]: 0.962 (+/-0.071) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[378]: 0.962 (+/-0.038) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[379]: 0.971 (+/-0.047) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[380]: 0.962 (+/-0.111) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[381]: 0.876 (+/-0.205) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[382]: 0.838 (+/-0.369) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[383]: 0.952 (+/-0.085) for {'alpha': 0.01, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[384]: 0.352 (+/-0.129) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[385]: 0.371 (+/-0.152) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[386]: 0.352 (+/-0.114) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[387]: 0.343 (+/-0.038) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[388]: 0.324 (+/-0.111) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[389]: 0.371 (+/-0.126) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[390]: 0.362 (+/-0.097) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[391]: 0.229 (+/-0.093) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[392]: 0.314 (+/-0.230) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[393]: 0.257 (+/-0.143) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[394]: 0.314 (+/-0.076) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[395]: 0.381 (+/-0.135) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[396]: 0.371 (+/-0.265) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[397]: 0.400 (+/-0.129) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[398]: 0.410 (+/-0.129) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[399]: 0.381 (+/-0.060) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[400]: 0.352 (+/-0.273) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[401]: 0.333 (+/-0.217) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[402]: 0.314 (+/-0.196) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[403]: 0.305 (+/-0.205) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[404]: 0.333 (+/-0.060) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[405]: 0.343 (+/-0.126) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[406]: 0.333 (+/-0.190) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[407]: 0.305 (+/-0.177) for {'alpha': 0.01, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[408]: 0.676 (+/-0.071) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[409]: 0.714 (+/-0.104) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[410]: 0.657 (+/-0.140) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[411]: 0.686 (+/-0.097) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[412]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[413]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[414]: 0.648 (+/-0.177) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[415]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[416]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[417]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[418]: 0.733 (+/-0.076) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[419]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[420]: 0.743 (+/-0.166) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[421]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[422]: 0.724 (+/-0.140) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[423]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[424]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[425]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[426]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[427]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[428]: 0.695 (+/-0.047) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[429]: 0.733 (+/-0.177) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[430]: 0.705 (+/-0.071) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[431]: 0.657 (+/-0.236) for {'alpha': 0.01, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[432]: 0.781 (+/-0.177) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[433]: 0.800 (+/-0.194) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[434]: 0.752 (+/-0.164) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[435]: 0.800 (+/-0.220) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[436]: 0.752 (+/-0.126) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[437]: 0.838 (+/-0.143) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[438]: 0.857 (+/-0.104) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[439]: 0.743 (+/-0.155) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[440]: 0.781 (+/-0.114) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[441]: 0.733 (+/-0.097) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[442]: 0.771 (+/-0.203) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[443]: 0.714 (+/-0.135) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[444]: 0.876 (+/-0.205) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[445]: 0.829 (+/-0.253) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[446]: 0.810 (+/-0.233) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[447]: 0.867 (+/-0.291) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[448]: 0.867 (+/-0.194) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[449]: 0.829 (+/-0.155) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[450]: 0.790 (+/-0.129) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[451]: 0.848 (+/-0.175) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[452]: 0.714 (+/-0.060) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[453]: 0.743 (+/-0.047) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[454]: 0.781 (+/-0.177) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[455]: 0.705 (+/-0.071) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[456]: 0.810 (+/-0.248) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[457]: 0.790 (+/-0.129) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[458]: 0.800 (+/-0.220) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[459]: 0.752 (+/-0.212) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[460]: 0.762 (+/-0.200) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[461]: 0.752 (+/-0.126) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[462]: 0.857 (+/-0.241) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[463]: 0.743 (+/-0.129) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[464]: 0.714 (+/-0.060) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[465]: 0.790 (+/-0.143) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[466]: 0.752 (+/-0.093) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[467]: 0.771 (+/-0.194) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[468]: 0.762 (+/-0.060) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[469]: 0.924 (+/-0.222) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[470]: 0.790 (+/-0.230) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[471]: 0.771 (+/-0.152) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[472]: 0.829 (+/-0.205) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[473]: 0.762 (+/-0.104) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[474]: 0.790 (+/-0.177) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[475]: 0.848 (+/-0.203) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[476]: 0.781 (+/-0.214) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[477]: 0.829 (+/-0.196) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[478]: 0.714 (+/-0.085) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[479]: 0.743 (+/-0.177) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[480]: 0.838 (+/-0.214) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[481]: 0.867 (+/-0.251) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[482]: 0.876 (+/-0.097) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[483]: 0.829 (+/-0.196) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[484]: 0.895 (+/-0.071) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[485]: 0.886 (+/-0.245) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[486]: 0.886 (+/-0.047) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[487]: 0.895 (+/-0.126) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[488]: 0.781 (+/-0.155) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[489]: 0.829 (+/-0.097) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[490]: 0.838 (+/-0.143) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[491]: 0.790 (+/-0.245) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[492]: 0.781 (+/-0.129) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[493]: 0.857 (+/-0.085) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[494]: 0.762 (+/-0.269) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[495]: 0.848 (+/-0.164) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[496]: 0.895 (+/-0.244) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[497]: 0.933 (+/-0.114) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[498]: 0.848 (+/-0.236) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[499]: 0.867 (+/-0.194) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[500]: 0.886 (+/-0.155) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[501]: 0.943 (+/-0.093) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[502]: 0.800 (+/-0.140) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[503]: 0.771 (+/-0.203) for {'alpha': 0.1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[504]: 0.838 (+/-0.238) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[505]: 0.914 (+/-0.152) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[506]: 0.895 (+/-0.194) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[507]: 0.829 (+/-0.155) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[508]: 0.962 (+/-0.038) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[509]: 0.962 (+/-0.071) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[510]: 0.990 (+/-0.038) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[511]: 0.971 (+/-0.047) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[512]: 0.857 (+/-0.233) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[513]: 0.867 (+/-0.140) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[514]: 0.876 (+/-0.230) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[515]: 0.895 (+/-0.126) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[516]: 0.943 (+/-0.071) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[517]: 0.924 (+/-0.097) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[518]: 0.848 (+/-0.194) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[519]: 0.952 (+/-0.148) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[520]: 0.952 (+/-0.085) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[521]: 0.971 (+/-0.047) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[522]: 0.981 (+/-0.047) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[523]: 0.971 (+/-0.047) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[524]: 0.933 (+/-0.143) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[525]: 0.895 (+/-0.152) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[526]: 0.933 (+/-0.097) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[527]: 0.924 (+/-0.114) for {'alpha': 0.1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[528]: 0.324 (+/-0.111) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[529]: 0.314 (+/-0.214) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[530]: 0.400 (+/-0.155) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[531]: 0.324 (+/-0.093) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[532]: 0.324 (+/-0.285) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[533]: 0.410 (+/-0.166) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[534]: 0.343 (+/-0.203) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[535]: 0.276 (+/-0.111) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[536]: 0.286 (+/-0.241) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[537]: 0.286 (+/-0.148) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[538]: 0.267 (+/-0.129) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[539]: 0.371 (+/-0.164) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[540]: 0.381 (+/-0.200) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[541]: 0.295 (+/-0.071) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[542]: 0.381 (+/-0.200) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[543]: 0.371 (+/-0.111) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[544]: 0.333 (+/-0.269) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[545]: 0.267 (+/-0.230) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[546]: 0.419 (+/-0.071) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[547]: 0.305 (+/-0.129) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[548]: 0.371 (+/-0.071) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[549]: 0.371 (+/-0.258) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[550]: 0.305 (+/-0.177) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[551]: 0.305 (+/-0.187) for {'alpha': 0.1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[552]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[553]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[554]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[555]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[556]: 0.562 (+/-0.212) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[557]: 0.486 (+/-0.291) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[558]: 0.486 (+/-0.265) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[559]: 0.495 (+/-0.299) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[560]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[561]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[562]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[563]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[564]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[565]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[566]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[567]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[568]: 0.524 (+/-0.181) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[569]: 0.390 (+/-0.203) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[570]: 0.524 (+/-0.241) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[571]: 0.562 (+/-0.244) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[572]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[573]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[574]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[575]: 0.695 (+/-0.047) for {'alpha': 0.1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[576]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[577]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[578]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[579]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[580]: 0.600 (+/-0.322) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[581]: 0.467 (+/-0.265) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[582]: 0.657 (+/-0.140) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[583]: 0.619 (+/-0.233) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[584]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[585]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[586]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[587]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[588]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[589]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[590]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[591]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[592]: 0.419 (+/-0.304) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[593]: 0.514 (+/-0.343) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[594]: 0.457 (+/-0.349) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[595]: 0.495 (+/-0.293) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[596]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[597]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[598]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[599]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[600]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[601]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[602]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[603]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[604]: 0.581 (+/-0.332) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[605]: 0.362 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[606]: 0.371 (+/-0.038) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[607]: 0.438 (+/-0.279) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[608]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[609]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[610]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[611]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[612]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[613]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[614]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[615]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[616]: 0.495 (+/-0.322) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[617]: 0.429 (+/-0.241) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[618]: 0.562 (+/-0.338) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[619]: 0.438 (+/-0.279) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[620]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[621]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[622]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[623]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[624]: 0.810 (+/-0.248) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[625]: 0.848 (+/-0.194) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[626]: 0.743 (+/-0.222) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[627]: 0.771 (+/-0.152) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[628]: 0.324 (+/-0.071) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[629]: 0.343 (+/-0.071) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[630]: 0.333 (+/-0.085) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[631]: 0.333 (+/-0.060) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[632]: 0.867 (+/-0.348) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[633]: 0.781 (+/-0.333) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[634]: 0.686 (+/-0.076) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[635]: 0.676 (+/-0.244) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[636]: 0.752 (+/-0.140) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[637]: 0.829 (+/-0.245) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[638]: 0.790 (+/-0.205) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[639]: 0.762 (+/-0.301) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[640]: 0.324 (+/-0.071) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[641]: 0.343 (+/-0.071) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[642]: 0.333 (+/-0.060) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[643]: 0.352 (+/-0.047) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[644]: 0.714 (+/-0.233) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[645]: 0.848 (+/-0.140) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[646]: 0.648 (+/-0.230) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[647]: 0.600 (+/-0.379) for {'alpha': 1, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[648]: 0.752 (+/-0.111) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[649]: 0.790 (+/-0.205) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[650]: 0.867 (+/-0.203) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[651]: 0.829 (+/-0.222) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[652]: 0.714 (+/-0.104) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[653]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[654]: 0.733 (+/-0.129) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[655]: 0.705 (+/-0.038) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[656]: 0.743 (+/-0.114) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[657]: 0.762 (+/-0.085) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[658]: 0.762 (+/-0.120) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[659]: 0.733 (+/-0.114) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[660]: 0.714 (+/-0.104) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[661]: 0.771 (+/-0.236) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[662]: 0.781 (+/-0.177) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[663]: 0.743 (+/-0.166) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[664]: 0.714 (+/-0.104) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[665]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[666]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[667]: 0.733 (+/-0.177) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[668]: 0.790 (+/-0.177) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[669]: 0.733 (+/-0.097) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[670]: 0.771 (+/-0.152) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[671]: 0.705 (+/-0.038) for {'alpha': 1, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[672]: 0.457 (+/-0.260) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[673]: 0.371 (+/-0.285) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[674]: 0.229 (+/-0.236) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[675]: 0.400 (+/-0.416) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[676]: 0.257 (+/-0.177) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[677]: 0.381 (+/-0.233) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[678]: 0.267 (+/-0.260) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[679]: 0.333 (+/-0.170) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[680]: 0.371 (+/-0.338) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[681]: 0.352 (+/-0.253) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[682]: 0.371 (+/-0.236) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[683]: 0.295 (+/-0.212) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[684]: 0.333 (+/-0.276) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[685]: 0.495 (+/-0.196) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[686]: 0.295 (+/-0.298) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[687]: 0.438 (+/-0.265) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[688]: 0.410 (+/-0.316) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[689]: 0.371 (+/-0.220) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[690]: 0.257 (+/-0.260) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[691]: 0.295 (+/-0.212) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[692]: 0.343 (+/-0.343) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[693]: 0.267 (+/-0.196) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[694]: 0.200 (+/-0.236) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[695]: 0.314 (+/-0.177) for {'alpha': 1, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[696]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[697]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[698]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[699]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[700]: 0.352 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[701]: 0.362 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[702]: 0.333 (+/-0.085) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[703]: 0.343 (+/-0.071) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[704]: 0.371 (+/-0.038) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[705]: 0.333 (+/-0.085) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[706]: 0.371 (+/-0.038) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[707]: 0.371 (+/-0.038) for {'alpha': 1, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[708]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[709]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[710]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[711]: 0.695 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[712]: 0.324 (+/-0.071) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[713]: 0.333 (+/-0.085) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[714]: 0.305 (+/-0.047) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[715]: 0.333 (+/-0.060) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[716]: 0.352 (+/-0.076) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[717]: 0.371 (+/-0.038) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[718]: 0.352 (+/-0.076) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[719]: 0.371 (+/-0.038) for {'alpha': 1, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[720]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[721]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[722]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[723]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[724]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[725]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[726]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[727]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[728]: 0.333 (+/-0.000) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[729]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[730]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[731]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[732]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[733]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[734]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[735]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[736]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[737]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[738]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[739]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[740]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[741]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[742]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[743]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'hinge', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[744]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[745]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[746]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[747]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[748]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[749]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[750]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[751]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[752]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[753]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[754]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[755]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[756]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[757]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[758]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[759]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[760]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[761]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[762]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[763]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[764]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[765]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[766]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[767]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'log_loss', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[768]: 0.448 (+/-0.222) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[769]: 0.419 (+/-0.298) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[770]: 0.467 (+/-0.258) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[771]: 0.448 (+/-0.155) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[772]: 0.305 (+/-0.047) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[773]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[774]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[775]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[776]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[777]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[778]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[779]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[780]: 0.429 (+/-0.241) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[781]: 0.371 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[782]: 0.457 (+/-0.214) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[783]: 0.400 (+/-0.177) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[784]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[785]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[786]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[787]: 0.343 (+/-0.038) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[788]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[789]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[790]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[791]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'perceptron', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[792]: 0.467 (+/-0.229) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[793]: 0.467 (+/-0.229) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[794]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[795]: 0.400 (+/-0.230) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[796]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[797]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[798]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[799]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[800]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[801]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[802]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[803]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[804]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[805]: 0.381 (+/-0.000) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[806]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[807]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[808]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[809]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[810]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[811]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[812]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[813]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[814]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[815]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'modified_huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[816]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[817]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[818]: 0.419 (+/-0.203) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[819]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[820]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[821]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[822]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[823]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[824]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[825]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[826]: 0.333 (+/-0.085) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[827]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[828]: 0.400 (+/-0.129) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[829]: 0.400 (+/-0.230) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[830]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[831]: 0.419 (+/-0.203) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[832]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[833]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[834]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[835]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[836]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[837]: 0.343 (+/-0.038) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[838]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[839]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'squared_error', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[840]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[841]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[842]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[843]: 0.381 (+/-0.104) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[844]: 0.352 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[845]: 0.324 (+/-0.038) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[846]: 0.305 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[847]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[848]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[849]: 0.324 (+/-0.071) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[850]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[851]: 0.314 (+/-0.076) for {'alpha': 100, 'loss': 'huber', 'max_iter': 100000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\t[852]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "\t[853]: 0.371 (+/-0.038) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[854]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[855]: 0.343 (+/-0.071) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[856]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "\t[857]: 0.333 (+/-0.060) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[858]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[859]: 0.305 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[860]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "\t[861]: 0.362 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.001}\n",
      "\t[862]: 0.352 (+/-0.076) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.01}\n",
      "\t[863]: 0.314 (+/-0.047) for {'alpha': 100, 'loss': 'huber', 'max_iter': 1000000, 'penalty': 'elasticnet', 'tol': 0.1}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.001, loss='modified_huber', max_iter=100000, penalty='l1',\n",
      "              tol=0.01)\n",
      "\n",
      "best: dat=iris, score=0.99048, model=SGDClassifier(alpha=0.001,loss='modified_huber',max_iter=100000,penalty='l1',tol=0.01)\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "sgd = linear_model.SGDClassifier()\n",
    "\n",
    "# Notice different parameters, check sklearn documentation for SGDClassifier for description of these parameters\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss', 'perceptron', 'modified_huber', 'squared_error', 'huber'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 100],\n",
    "    'tol' : [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter' : [int(1e+5), int(1e+6)]\n",
    "}\n",
    "\n",
    "# This part is almost the same as in previous code block, we just swap the model\n",
    "grid_tuned = GridSearchCV(sgd,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model has a staggering score of 0.99048, even better than the SVC model we used previously! It did take a few minutes though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Another method of finding the optimal parameters is to uze RandomizedSearchCV. In contrast to GridSearchCV it only chooses a few randomly selected samples of each hyperparameter provided, controlled by its own parameter 'n_iter'. This makes it potentially faster then GridSearchCV, but the results might not give the best scores. \n",
    "\n",
    "Below is an implementation and a test run of a RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 0.12 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'tol': 0.01, 'penalty': 'l1', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 0.001}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.001, loss='perceptron', max_iter=100000, penalty='l1',\n",
      "              tol=0.01)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.971 (+/-0.114) for {'tol': 0.01, 'penalty': 'l1', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 0.001}\n",
      "\t[ 1]: 0.352 (+/-0.076) for {'tol': 0.0001, 'penalty': 'l2', 'max_iter': 100000, 'loss': 'hinge', 'alpha': 100}\n",
      "\t[ 2]: 0.743 (+/-0.129) for {'tol': 0.1, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 1}\n",
      "\t[ 3]: 0.886 (+/-0.196) for {'tol': 0.001, 'penalty': 'elasticnet', 'max_iter': 1000000, 'loss': 'hinge', 'alpha': 0.01}\n",
      "\t[ 4]: 0.686 (+/-0.322) for {'tol': 0.1, 'penalty': 'elasticnet', 'max_iter': 1000000, 'loss': 'hinge', 'alpha': 0.0001}\n",
      "\t[ 5]: 0.895 (+/-0.111) for {'tol': 0.1, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'hinge', 'alpha': 0.01}\n",
      "\t[ 6]: 0.524 (+/-0.301) for {'tol': 0.001, 'penalty': 'l1', 'max_iter': 1000000, 'loss': 'hinge', 'alpha': 1}\n",
      "\t[ 7]: 0.924 (+/-0.097) for {'tol': 0.01, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'hinge', 'alpha': 0.01}\n",
      "\t[ 8]: 0.905 (+/-0.120) for {'tol': 0.001, 'penalty': 'l1', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 0.1}\n",
      "\t[ 9]: 0.752 (+/-0.164) for {'tol': 0.001, 'penalty': 'l2', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 1}\n",
      "\t[10]: 0.333 (+/-0.085) for {'tol': 0.001, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'squared_error', 'alpha': 0.01}\n",
      "\t[11]: 0.362 (+/-0.047) for {'tol': 0.01, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 100}\n",
      "\t[12]: 0.343 (+/-0.071) for {'tol': 0.1, 'penalty': 'l1', 'max_iter': 100000, 'loss': 'perceptron', 'alpha': 1}\n",
      "\t[13]: 0.829 (+/-0.196) for {'tol': 0.0001, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'log_loss', 'alpha': 0.1}\n",
      "\t[14]: 0.286 (+/-0.319) for {'tol': 0.01, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'squared_error', 'alpha': 1}\n",
      "\t[15]: 0.362 (+/-0.047) for {'tol': 0.001, 'penalty': 'l2', 'max_iter': 100000, 'loss': 'log_loss', 'alpha': 100}\n",
      "\t[16]: 0.838 (+/-0.177) for {'tol': 0.1, 'penalty': 'l1', 'max_iter': 1000000, 'loss': 'perceptron', 'alpha': 0.0001}\n",
      "\t[17]: 0.352 (+/-0.076) for {'tol': 0.001, 'penalty': 'elasticnet', 'max_iter': 100000, 'loss': 'huber', 'alpha': 100}\n",
      "\t[18]: 0.333 (+/-0.085) for {'tol': 0.0001, 'penalty': 'elasticnet', 'max_iter': 1000000, 'loss': 'huber', 'alpha': 1}\n",
      "\t[19]: 0.371 (+/-0.164) for {'tol': 0.0001, 'penalty': 'l2', 'max_iter': 100000, 'loss': 'squared_error', 'alpha': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.89      0.94        18\n",
      "           2       0.85      1.00      0.92        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.001, loss='perceptron', max_iter=100000, penalty='l1',\n",
      "              tol=0.01)\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SGDClassifier(alpha=0.001,loss='perceptron',max_iter=100000,penalty='l1',tol=0.01)\n",
      "\n",
      "OK(random-search)\n"
     ]
    }
   ],
   "source": [
    "random_tuned = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    tuning_parameters,\n",
    "    # Pick up to 20 different samples\n",
    "    n_iter=20,\n",
    "    # same state should give same rng distribution\n",
    "    random_state=42,\n",
    "    cv=CV,\n",
    "    scoring='f1_micro',\n",
    "    verbose=VERBOSE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "random_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(random_tuned, X_test, y_test, t)\n",
    "print('OK(random-search)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output that only 20 different combinations of parameters were tested, and the best parameters were not the same as in GridSearchCV test. The f1_score is also lower at 0.914 instead of 0.990. This could potentially be improved by adding more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd MNIST Search Quest II\n",
    "\n",
    "\n",
    "It's time to embark on an epic adventure of finding the best model for the MNIST dataset. Here we will use gridsearch on several models to attempt to find the best model according to the f1_micro scoring metric. The best scores across different groups will then be compared to each other. May the best man win!\n",
    "\n",
    "In our first try, we'd like to test how fast the gridsearch performs, so we're gonna use a GridSearchCV with our existing SGDClassifier. Since complete GridSearch would take forever, we will only select a few parameters from each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: mnist..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\UNI_2023\\ml\\gitmal\\libitmal\\dataloaders.py:65: UserWarning: MNIST_GetDataSet(): failed to import and load data in load_mode 'tensorflow.keras', proceding to next mode..\n",
      "  warnings.warn(\"MNIST_GetDataSet(): failed to import and load data in load_mode 'tensorflow.keras', proceding to next mode..\")\n",
      "C:\\UNI_2023\\ml\\gitmal\\libitmal\\dataloaders.py:77: UserWarning: MNIST_GetDataSet(): failed to import and load data in load_mode 'keras', proceding to next mode..\n",
      "  warnings.warn(\"MNIST_GetDataSet(): failed to import and load data in load_mode 'keras', proceding to next mode..\")\n",
      "c:\\Users\\nastr\\anaconda3\\envs\\swmal\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "C:\\UNI_2023\\ml\\gitmal\\libitmal\\dataloaders.py:88: UserWarning: MNIST_GetDataSet(): fetch openml mode is slow and uses 'float64' instead of 'uint8'\n",
      "  warnings.warn(\"MNIST_GetDataSet(): fetch openml mode is slow and uses 'float64' instead of 'uint8'\")\n",
      "C:\\UNI_2023\\ml\\gitmal\\libitmal\\dataloaders.py:92: UserWarning: MNIST_GetDataSet(): fetch openml mode converts y from '<class 'str'>' to 'uint8'\n",
      "  warnings.warn(f\"MNIST_GetDataSet(): fetch openml mode converts y from '{type(y[0])}' to 'uint8'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  org. data:  X.shape      =(70000;  784), y.shape      =(70000)\n",
      "  train data: X_train.shape=(49000;  784), y_train.shape=(49000)\n",
      "  test data:  X_test.shape =(21000;  784), y_test.shape =(21000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load mnist data separate block, since this can take extra time\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 17656.98 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\tbest 'f1_micro' score=0.8937959183673468\n",
      "\tbest index=17\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.001, loss='log_loss', max_iter=100, penalty='l1', tol=0.1)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.868 (+/-0.017) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 1]: 0.864 (+/-0.027) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 2]: 0.882 (+/-0.012) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 3]: 0.893 (+/-0.008) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[ 4]: 0.892 (+/-0.007) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[ 5]: 0.891 (+/-0.003) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[ 6]: 0.869 (+/-0.024) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 7]: 0.876 (+/-0.032) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 8]: 0.879 (+/-0.018) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 9]: 0.890 (+/-0.008) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[10]: 0.890 (+/-0.012) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[11]: 0.891 (+/-0.008) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[12]: 0.872 (+/-0.015) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[13]: 0.874 (+/-0.020) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[14]: 0.881 (+/-0.011) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[15]: 0.892 (+/-0.006) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[16]: 0.891 (+/-0.003) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[17]: 0.894 (+/-0.007) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[18]: 0.881 (+/-0.007) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[19]: 0.873 (+/-0.049) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[20]: 0.880 (+/-0.017) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[21]: 0.890 (+/-0.009) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[22]: 0.888 (+/-0.006) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[23]: 0.888 (+/-0.002) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[24]: 0.865 (+/-0.032) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[25]: 0.877 (+/-0.014) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[26]: 0.878 (+/-0.035) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[27]: 0.881 (+/-0.010) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[28]: 0.877 (+/-0.003) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[29]: 0.880 (+/-0.007) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[30]: 0.877 (+/-0.028) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[31]: 0.872 (+/-0.029) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[32]: 0.876 (+/-0.016) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[33]: 0.881 (+/-0.012) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[34]: 0.885 (+/-0.004) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[35]: 0.887 (+/-0.007) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[36]: 0.875 (+/-0.014) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[37]: 0.872 (+/-0.025) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[38]: 0.875 (+/-0.018) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[39]: 0.885 (+/-0.006) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[40]: 0.884 (+/-0.004) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[41]: 0.881 (+/-0.006) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[42]: 0.866 (+/-0.029) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[43]: 0.866 (+/-0.050) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[44]: 0.878 (+/-0.021) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[45]: 0.886 (+/-0.004) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[46]: 0.883 (+/-0.009) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[47]: 0.884 (+/-0.009) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[48]: 0.876 (+/-0.027) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[49]: 0.875 (+/-0.026) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[50]: 0.869 (+/-0.017) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[51]: 0.822 (+/-0.006) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[52]: 0.822 (+/-0.013) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[53]: 0.821 (+/-0.011) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[54]: 0.885 (+/-0.018) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[55]: 0.876 (+/-0.024) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[56]: 0.885 (+/-0.014) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[57]: 0.834 (+/-0.007) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[58]: 0.834 (+/-0.016) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[59]: 0.832 (+/-0.018) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[60]: 0.880 (+/-0.017) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[61]: 0.880 (+/-0.023) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[62]: 0.875 (+/-0.030) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[63]: 0.839 (+/-0.026) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[64]: 0.841 (+/-0.022) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[65]: 0.844 (+/-0.020) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[66]: 0.884 (+/-0.025) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[67]: 0.880 (+/-0.014) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[68]: 0.871 (+/-0.012) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[69]: 0.850 (+/-0.011) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[70]: 0.845 (+/-0.016) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[71]: 0.847 (+/-0.015) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      2077\n",
      "           1       0.96      0.96      0.96      2385\n",
      "           2       0.91      0.89      0.90      2115\n",
      "           3       0.90      0.85      0.87      2117\n",
      "           4       0.90      0.91      0.91      2004\n",
      "           5       0.85      0.81      0.83      1900\n",
      "           6       0.94      0.93      0.94      2045\n",
      "           7       0.94      0.90      0.92      2189\n",
      "           8       0.73      0.87      0.79      2042\n",
      "           9       0.88      0.85      0.86      2126\n",
      "\n",
      "    accuracy                           0.89     21000\n",
      "   macro avg       0.90      0.89      0.89     21000\n",
      "weighted avg       0.90      0.89      0.89     21000\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.001, loss='log_loss', max_iter=100, penalty='l1', tol=0.1)\n",
      "\n",
      "best: dat=mnist, score=0.89380, model=SGDClassifier(alpha=0.001,loss='log_loss',max_iter=100,penalty='l1',tol=0.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nastr\\anaconda3\\envs\\swmal\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Notice different parameters, check sklearn documentation for SGDClassifier for description of these parameters\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log_loss'],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'alpha' : [0.001, 0.01, 0.1],\n",
    "    'tol' : [0.001, 0.01, 0.1],\n",
    "    'max_iter' : [int(1e+2), int(1e+3)]\n",
    "}\n",
    "\n",
    "# This part is almost the same as in previous code block, we just swap the model\n",
    "grid_tuned = GridSearchCV(sgd,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A score of 0.8938 isn't terrible but it surely can be improved upon. Let's try a model which hasn't been explored during the lectures: RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nastr\\anaconda3\\envs\\swmal\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 36 is smaller than n_iter=50. Running 36 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "SEARCH TIME: 5070.60 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\tbest 'f1_micro' score=0.8937959183673468\n",
      "\tbest index=17\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.001, loss='log_loss', max_iter=100, penalty='l1', tol=0.1)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.868 (+/-0.017) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 1]: 0.864 (+/-0.027) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 2]: 0.882 (+/-0.012) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 3]: 0.893 (+/-0.008) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[ 4]: 0.892 (+/-0.007) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[ 5]: 0.891 (+/-0.003) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[ 6]: 0.869 (+/-0.024) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[ 7]: 0.876 (+/-0.032) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[ 8]: 0.879 (+/-0.018) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[ 9]: 0.890 (+/-0.008) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[10]: 0.890 (+/-0.012) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[11]: 0.891 (+/-0.008) for {'alpha': 0.001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[12]: 0.872 (+/-0.015) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[13]: 0.874 (+/-0.020) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[14]: 0.881 (+/-0.011) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[15]: 0.892 (+/-0.006) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[16]: 0.891 (+/-0.003) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[17]: 0.894 (+/-0.007) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[18]: 0.881 (+/-0.007) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[19]: 0.873 (+/-0.049) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[20]: 0.880 (+/-0.017) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[21]: 0.890 (+/-0.009) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[22]: 0.888 (+/-0.006) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[23]: 0.888 (+/-0.002) for {'alpha': 0.001, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[24]: 0.865 (+/-0.032) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[25]: 0.877 (+/-0.014) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[26]: 0.878 (+/-0.035) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[27]: 0.881 (+/-0.010) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[28]: 0.877 (+/-0.003) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[29]: 0.880 (+/-0.007) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[30]: 0.877 (+/-0.028) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[31]: 0.872 (+/-0.029) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[32]: 0.876 (+/-0.016) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[33]: 0.881 (+/-0.012) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[34]: 0.885 (+/-0.004) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[35]: 0.887 (+/-0.007) for {'alpha': 0.01, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[36]: 0.875 (+/-0.014) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[37]: 0.872 (+/-0.025) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[38]: 0.875 (+/-0.018) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[39]: 0.885 (+/-0.006) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[40]: 0.884 (+/-0.004) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[41]: 0.881 (+/-0.006) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[42]: 0.866 (+/-0.029) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[43]: 0.866 (+/-0.050) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[44]: 0.878 (+/-0.021) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[45]: 0.886 (+/-0.004) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[46]: 0.883 (+/-0.009) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[47]: 0.884 (+/-0.009) for {'alpha': 0.01, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[48]: 0.876 (+/-0.027) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[49]: 0.875 (+/-0.026) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[50]: 0.869 (+/-0.017) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[51]: 0.822 (+/-0.006) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[52]: 0.822 (+/-0.013) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[53]: 0.821 (+/-0.011) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[54]: 0.885 (+/-0.018) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[55]: 0.876 (+/-0.024) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[56]: 0.885 (+/-0.014) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[57]: 0.834 (+/-0.007) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[58]: 0.834 (+/-0.016) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[59]: 0.832 (+/-0.018) for {'alpha': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[60]: 0.880 (+/-0.017) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[61]: 0.880 (+/-0.023) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[62]: 0.875 (+/-0.030) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[63]: 0.839 (+/-0.026) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[64]: 0.841 (+/-0.022) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[65]: 0.844 (+/-0.020) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 100, 'penalty': 'l1', 'tol': 0.1}\n",
      "\t[66]: 0.884 (+/-0.025) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\t[67]: 0.880 (+/-0.014) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.01}\n",
      "\t[68]: 0.871 (+/-0.012) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.1}\n",
      "\t[69]: 0.850 (+/-0.011) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.001}\n",
      "\t[70]: 0.845 (+/-0.016) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.01}\n",
      "\t[71]: 0.847 (+/-0.015) for {'alpha': 0.1, 'loss': 'log_loss', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.1}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      2077\n",
      "           1       0.96      0.96      0.96      2385\n",
      "           2       0.91      0.89      0.90      2115\n",
      "           3       0.90      0.85      0.87      2117\n",
      "           4       0.90      0.91      0.91      2004\n",
      "           5       0.85      0.81      0.83      1900\n",
      "           6       0.94      0.93      0.94      2045\n",
      "           7       0.94      0.90      0.92      2189\n",
      "           8       0.73      0.87      0.79      2042\n",
      "           9       0.88      0.85      0.86      2126\n",
      "\n",
      "    accuracy                           0.89     21000\n",
      "   macro avg       0.90      0.89      0.89     21000\n",
      "weighted avg       0.90      0.89      0.89     21000\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.001, loss='log_loss', max_iter=100, penalty='l1', tol=0.1)\n",
      "\n",
      "best: dat=mnist, score=0.89380, model=SGDClassifier(alpha=0.001,loss='log_loss',max_iter=100,penalty='l1',tol=0.1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forest gridsearch!\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'n_estimators' : [50, 100, 1000],\n",
    "    'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth' : [None, 2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_tuned = GridSearchCV(forest,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let this code run on the gpucluster server and got the following results:\n",
    "\n",
    "![random forest results](mnist_quest/RandomForest.png)\n",
    "\n",
    "  \n",
    "With a score of 0.96863 we feel this is a pretty good result! Nonetheless, we might be able to optimize it. The search above was done using RandomizedSearchCV. Let's try choosing a couple parameters close to our result and performing a GridSearch with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "SEARCH TIME: 4508.03 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'min_samples_split': 2, 'n_estimators': 1500}\n",
      "\tbest 'f1_micro' score=0.9686530612244898\n",
      "\tbest index=1\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tRandomForestClassifier(n_estimators=1500)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.968 (+/-0.003) for {'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\t[ 1]: 0.969 (+/-0.003) for {'min_samples_split': 2, 'n_estimators': 1500}\n",
      "\t[ 2]: 0.968 (+/-0.003) for {'min_samples_split': 2, 'n_estimators': 2000}\n",
      "\t[ 3]: 0.968 (+/-0.003) for {'min_samples_split': 3, 'n_estimators': 1000}\n",
      "\t[ 4]: 0.968 (+/-0.003) for {'min_samples_split': 3, 'n_estimators': 1500}\n",
      "\t[ 5]: 0.968 (+/-0.003) for {'min_samples_split': 3, 'n_estimators': 2000}\n",
      "\t[ 6]: 0.968 (+/-0.003) for {'min_samples_split': 5, 'n_estimators': 1000}\n",
      "\t[ 7]: 0.968 (+/-0.002) for {'min_samples_split': 5, 'n_estimators': 1500}\n",
      "\t[ 8]: 0.967 (+/-0.003) for {'min_samples_split': 5, 'n_estimators': 2000}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2077\n",
      "           1       0.99      0.99      0.99      2385\n",
      "           2       0.96      0.98      0.97      2115\n",
      "           3       0.97      0.95      0.96      2117\n",
      "           4       0.97      0.97      0.97      2004\n",
      "           5       0.97      0.96      0.97      1900\n",
      "           6       0.97      0.98      0.98      2045\n",
      "           7       0.97      0.96      0.97      2189\n",
      "           8       0.96      0.96      0.96      2042\n",
      "           9       0.96      0.95      0.96      2126\n",
      "\n",
      "    accuracy                           0.97     21000\n",
      "   macro avg       0.97      0.97      0.97     21000\n",
      "weighted avg       0.97      0.97      0.97     21000\n",
      "\n",
      "\n",
      "CTOR for best model: RandomForestClassifier(n_estimators=1500)\n",
      "\n",
      "best: dat=mnist, score=0.96865, model=RandomForestClassifier(min_samples_split=2,n_estimators=1500)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'n_estimators' : [1000, 1500, 2000],\n",
    "    'min_samples_split' : [2, 3, 5]\n",
    "}\n",
    "\n",
    "grid_tuned = GridSearchCV(forest,\n",
    "                          tuning_parameters,\n",
    "                          cv=5,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=2,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not seem to improve our score by much. In our final test we will test the DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "SEARCH TIME: 40.16 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\tbest 'f1_micro' score=0.8317346938775512\n",
      "\tbest index=24\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tDecisionTreeClassifier(criterion='entropy', max_features='sqrt')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.819 (+/-0.005) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[ 1]: 0.821 (+/-0.015) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[ 2]: 0.824 (+/-0.014) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[ 3]: 0.815 (+/-0.010) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[ 4]: 0.823 (+/-0.013) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[ 5]: 0.819 (+/-0.006) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[ 6]: 0.819 (+/-0.006) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[ 7]: 0.821 (+/-0.007) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[ 8]: 0.818 (+/-0.013) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[ 9]: 0.817 (+/-0.009) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[10]: 0.821 (+/-0.010) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[11]: 0.822 (+/-0.014) for {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[12]: 0.778 (+/-0.007) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[13]: 0.777 (+/-0.010) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[14]: 0.773 (+/-0.011) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[15]: 0.769 (+/-0.009) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[16]: 0.776 (+/-0.006) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[17]: 0.772 (+/-0.014) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[18]: 0.779 (+/-0.008) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[19]: 0.773 (+/-0.015) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[20]: 0.770 (+/-0.006) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[21]: 0.780 (+/-0.011) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[22]: 0.778 (+/-0.019) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[23]: 0.773 (+/-0.013) for {'criterion': 'gini', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[24]: 0.832 (+/-0.007) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[25]: 0.830 (+/-0.005) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[26]: 0.824 (+/-0.007) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[27]: 0.826 (+/-0.006) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[28]: 0.826 (+/-0.010) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[29]: 0.828 (+/-0.014) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[30]: 0.826 (+/-0.012) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[31]: 0.828 (+/-0.008) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[32]: 0.825 (+/-0.004) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[33]: 0.829 (+/-0.009) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[34]: 0.831 (+/-0.006) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[35]: 0.827 (+/-0.006) for {'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[36]: 0.782 (+/-0.011) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[37]: 0.778 (+/-0.005) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[38]: 0.781 (+/-0.009) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[39]: 0.776 (+/-0.005) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[40]: 0.778 (+/-0.015) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[41]: 0.780 (+/-0.004) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[42]: 0.780 (+/-0.013) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[43]: 0.777 (+/-0.011) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[44]: 0.777 (+/-0.010) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\t[45]: 0.778 (+/-0.012) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\t[46]: 0.776 (+/-0.017) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 3, 'splitter': 'best'}\n",
      "\t[47]: 0.773 (+/-0.015) for {'criterion': 'entropy', 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5, 'splitter': 'best'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      2077\n",
      "           1       0.94      0.95      0.95      2385\n",
      "           2       0.84      0.83      0.84      2115\n",
      "           3       0.79      0.78      0.79      2117\n",
      "           4       0.81      0.84      0.83      2004\n",
      "           5       0.79      0.77      0.78      1900\n",
      "           6       0.88      0.89      0.88      2045\n",
      "           7       0.86      0.86      0.86      2189\n",
      "           8       0.80      0.79      0.80      2042\n",
      "           9       0.78      0.79      0.79      2126\n",
      "\n",
      "    accuracy                           0.85     21000\n",
      "   macro avg       0.84      0.84      0.84     21000\n",
      "weighted avg       0.84      0.85      0.85     21000\n",
      "\n",
      "\n",
      "CTOR for best model: DecisionTreeClassifier(criterion='entropy', max_features='sqrt')\n",
      "\n",
      "best: dat=mnist, score=0.83173, model=DecisionTreeClassifier(criterion='entropy',max_features='sqrt',min_samples_leaf=1,min_samples_split=2,splitter='best')\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'splitter' : ['best'],\n",
    "    'min_samples_split' : [2, 3, 5],\n",
    "    'min_samples_leaf' : [1, 2, 3, 4],\n",
    "    'max_features' : ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_tuned = GridSearchCV(tree,\n",
    "                          tuning_parameters,\n",
    "                          cv=5,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=2,\n",
    "                          n_jobs=6)\n",
    "\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this score is even worse than the scores we had before. We'll have to stick with our RandomTreeClassifier, with a score of 0.986."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Advanced CNN using Roboflow\n",
    "\n",
    "In this exercise we explore object detection machine learning algorithms, using Roboflow and YOLOv8 API. YOLOv8 is an api developed by Ultralytics, which is designed for CNN models. It is a successor of YOLOv3 and YOLOv5, and has proven itself to be a powerful framework for tasks such as object detection and classification. Because of this we will use it for the most glorious purpose imaginable: detecting cats! And if you hate cats, we can use it to crop them out of the pictures...\n",
    "\n",
    "## Creating a dataset to work with\n",
    "\n",
    "In order to detect cats properly, one must first create a dataset of images which the model can work with. We will start with cloning an appropriate project from https://universe.roboflow.com/ . For this project we chose a project which already has 50 images to work with, and luckily for us they are annotated. Cloning the images is quite straightforward - you select the project to clone from, choose the images to clone and your Roboflow workspace/project where the images will be placed.\n",
    "\n",
    "![cloning in roboflow](cnn2_Imgs/roboflow_cloning.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the images which we have cloned are already annotated: there are bounding boxes around the cats. This is helpful, because it will spare us the time to go through each of these images individually and annotate them ourselves. However, to illustrate how such process is done we decided to upload a few images of our own, and annotate them.\n",
    "\n",
    "![roboflow annotations](cnn2_imgs/roboflow_annotation.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate a new version of the dataset. This step performs the Train/Test split and some preprocessing on the images, notably by downsizing them since ML models tend to work better on smaller pixel resolution images.\n",
    "\n",
    "![roboflow version generation](cnn2_imgs/roboflow_version.png)\n",
    "\n",
    "This also creates a code snippet that downloads the generated dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo CLI and SDK intro\n",
    "\n",
    "To make sure that everything is setup correctly we will go through the tutorial on https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/ . This also introduces us briefly to YOLO SDK in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  'model' is missing. Using default 'model=yolov8n.pt'.\n",
      "WARNING  'source' is missing. Using default 'source=/home/swmal10e23/.local/lib/python3.9/site-packages/ultralytics/assets'.\n",
      "Ultralytics YOLOv8.0.209  Python-3.9.12 torch-1.12.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12043MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "image 1/2 /home/swmal10e23/.local/lib/python3.9/site-packages/ultralytics/assets/bus.jpg: 640x480 3 persons, 1 bus, 7.5ms\n",
      "image 2/2 /home/swmal10e23/.local/lib/python3.9/site-packages/ultralytics/assets/zidane.jpg: 384x640 2 persons, 8.1ms\n",
      "Speed: 1.0ms preprocess, 7.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      " Learn more at https://docs.ultralytics.com/modes/track\n"
     ]
    }
   ],
   "source": [
    "# Checks that we have access to the GPU\n",
    "! nvidia-smi\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "# Check that yolo is installed properly by running this CLI command. If no exception is thrown, we are golden.\n",
    "! yolo mode=\"track\"\n",
    "# YOLO installs some missing dependencies on its own when running via CLI! Neat!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  103k  100  103k    0     0  1135k      0 --:--:-- --:--:-- --:--:-- 1138k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" rule for type \"image/png\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1l\u001b>4;1H\u001b[2J\u001b[?47l\u001b8 D)ownload, or C)ancel             \u001b[22;32H\u001b[m\u001b[m                                                              \u001b[2;1H                                                                                \u001b[3;1H                                                                                \u001b[4;1H                                                                                \u001b[5;1H                                                                                \u001b[6;1H                                                                                \u001b[7;1H                                                                                \u001b[8;1H                                                                                \u001b[9;1H                                                                                \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                              \u001b[4h\u001b[37m\u001b[40m \u001b[4l\u001b[H\u001b[m\u001b[m\u001b[37m\u001b[40m\u001b[m\u001b[m\u001b[21B\u001b[33m\u001b[44m\u001b[1mGetting file://localhost/tmp/tmpeufyigfr.PNG                                    \u001b[22;45H\u001b[m\u001b[m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://media.roboflow.com/notebooks/examples/dog.jpeg locally at dog.jpeg\n",
      "image 1/1 /home/swmal10e23/swmal_grp10/O3/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 8.0ms\n",
      "Speed: 0.9ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "# test SDK\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "# Download and show the dog picture\n",
    "! curl https://media.roboflow.com/notebooks/examples/dog.jpeg -o ./dog.jpeg\n",
    "img = Image.open(\"./dog.jpeg\")\n",
    "img.show()\n",
    "\n",
    "# Test on the tutorial example with a dog. pt = model is pretrained\n",
    "model = YOLO('yolov8n.pt')\n",
    "results = model.predict(source=\"https://media.roboflow.com/notebooks/examples/dog.jpeg\", conf=0.25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm found even the blurred car in the picture, which I myself have missed. Quite impressive.\n",
    "\n",
    "It's time to train and test a YOLO model on our cat dataset. The first step is to load the data from Roboflow.\n",
    "\n",
    "## Loading and training on data from Roboflow\n",
    "\n",
    "We will use the code generated earlier by Roboflow to download the data from it. It uses roboflows own package and api to download data securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.0.209, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in CatFinder-1 to yolov8:: 100%|| 2722/2722 [00:00<00:00, 3947.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to CatFinder-1 in yolov8:: 100%|| 122/122 [00:00<00:00, 167.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed roboflow package:\n",
    "# ! pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"RcblOwaZETwq19a8fyov\")\n",
    "project = rf.workspace(\"swmal10\").project(\"catfinder-hh2e7\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "\n",
    "# You must change the paths in project/data.yaml : all paths are relative to data.yaml directory, but roboflow mistakingly generates paths with project dir... and test path goes back for no reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import platform\n",
    "newmodel = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Find the directory to the data.yaml file. We need to change its contents because Roboflow\n",
    "# puts wrong paths in data.yaml. This might be due to incorrect versions? Roboflow\n",
    "# complains about yolo lib being too new...\n",
    "\n",
    "\n",
    "\n",
    "def fix_my_yaml(path):\n",
    "    print(path)\n",
    "    # Replace the incorrect paths in .yaml\n",
    "    lines = []\n",
    "    with open (path) as f:\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "            # print(line)\n",
    "\n",
    "    with open (datapath, \"w\") as f:\n",
    "        for s in lines:\n",
    "            if \"../test\" in s:\n",
    "                s = s.replace(\"../\", \"\")\n",
    "                f.write(s)\n",
    "            elif \"train:\" in s:\n",
    "                s = s.replace(\"CatFinder-1/\", \"\")\n",
    "                f.write(s)\n",
    "            elif \"val:\" in s:\n",
    "                s = s.replace(\"CatFinder-1/\", \"\")\n",
    "                f.write(s)\n",
    "            else:\n",
    "                f.write(s)\n",
    "    print(\"Fixed data.yaml in \", datapath)\n",
    "\n",
    "# Don't run on Windows\n",
    "if platform.system() == \"Linux\":\n",
    "    ! find / -iname \"CatFinder-1\"  2>&1 | grep -v \"Permission denied\" | grep -v \"Invalid argument\" > path.txt\n",
    "    with open(\"path.txt\") as f: s = f.readline()[:-1] # -1 because find returns \\n at the end\n",
    "    os.remove(\"path.txt\")\n",
    "    datapath = s + \"/data.yaml\"\n",
    "    fix_my_yaml(datapath)\n",
    "    res = newmodel.train(data=datapath)\n",
    "\n",
    "elif platform.system() == \"Windows\":\n",
    "    # Please insert your own path and make sure the .yaml file is in order\n",
    "    res = newmodel.train(data=\"C:\\\\UNI_2023\\\\ml\\\\swmal_grp10\\\\O3\\\\CatFinder-1\\\\data.yaml\")\n",
    "\n",
    "# The more epochs, the better: I run 100 on my desktop but 10 on my laptop\n",
    "# If on gpu cluster/some other server you can increase epochs\n",
    "\n",
    "\n",
    "# This crashed on my computer every time for some reason...\n",
    "# The reason was OBVIOUSLY one of the dependencies. Probably matplotlib or scipy (called by seaborn, but seaborn was not the problem)\n",
    "# Solved by upgrading via python -m pip install <package> --upgrade\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results are stored in runs/train/ directory and include labeled images as well as a lot of statistical data about the models performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"RcblOwaZETwq19a8fyov\")\n",
    "project = rf.workspace(\"swmal10\").project(\"catfinder-hh2e7\")\n",
    "dataset = project.version(2).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads the dataset into CatFinder-num directory. Note that default roboflow installation has data.yaml file with incorrect paths and need to be changed.  \n",
    "test: ../test/images -> test: test/images  \n",
    "train: CatFinder-1/train/images -> train: train/images  \n",
    "val: CatFinder-1/valid/images -> val: valid/images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.209  Python-3.9.12 torch-1.12.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12043MiB)\n",
      "WARNING  Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/swmal10e23/swmal_grp10/O3/CatFinder-2/data.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train6\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/swmal10e23/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1babfa85ac144a1b7b250c448c1e4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/755k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train6', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/swmal10e23/swmal_grp10/O3/CatFinder-2/train/labels... 318 images, 0 backgrounds, 0 corrupt: 100%|| 318/318 [00:01<00:00, 278.96it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/swmal10e23/swmal_grp10/O3/CatFinder-2/train/labels.cache\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 207, len(boxes) = 663. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/swmal10e23/swmal_grp10/O3/CatFinder-2/valid/labels... 36 images, 0 backgrounds, 0 corrupt: 100%|| 36/36 [00:00<00:00, 340.44it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/swmal10e23/swmal_grp10/O3/CatFinder-2/valid/labels.cache\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 55. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to runs/detect/train6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GrouperView' object has no attribute 'join'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train6\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100      2.28G      1.122      2.633      1.455         40        640: 100%|| 20/20 [00:02<00:00,  7.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  3.05it/s]\n",
      "                   all         36         55      0.782        0.2      0.488      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      2/100      2.36G       1.12      1.761      1.483         49        640: 100%|| 20/20 [00:02<00:00,  9.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 14.25it/s]\n",
      "                   all         36         55      0.403      0.327      0.398      0.201\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      3/100      2.33G      1.128      1.694      1.478         44        640: 100%|| 20/20 [00:02<00:00,  9.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 14.67it/s]\n",
      "                   all         36         55       0.36      0.369      0.229      0.079\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      4/100      2.36G      1.163      1.663      1.483         39        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 14.69it/s]\n",
      "                   all         36         55      0.294      0.236      0.236     0.0825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      5/100      2.33G      1.167       1.55      1.466         55        640: 100%|| 20/20 [00:02<00:00,  9.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 15.04it/s]\n",
      "                   all         36         55      0.231      0.364      0.165     0.0536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      6/100      2.33G      1.177      1.507      1.468         47        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 15.40it/s]\n",
      "                   all         36         55       0.42      0.255      0.191      0.065\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      7/100      2.33G      1.119      1.387      1.389         59        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.88it/s]\n",
      "                   all         36         55      0.552        0.2      0.237     0.0673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      8/100      2.31G      1.053      1.337      1.386         58        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.82it/s]\n",
      "                   all         36         55      0.092      0.164     0.0649     0.0133\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      9/100      2.33G      1.066      1.328      1.403         51        640: 100%|| 20/20 [00:01<00:00, 10.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.87it/s]\n",
      "                   all         36         55      0.498      0.491      0.426      0.156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     10/100      2.36G      1.017      1.207      1.352         52        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.90it/s]\n",
      "                   all         36         55      0.369      0.473      0.311      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     11/100      2.33G     0.9965      1.227      1.364         42        640: 100%|| 20/20 [00:01<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.12it/s]\n",
      "                   all         36         55      0.487      0.436      0.405      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     12/100      2.32G     0.9716       1.14      1.332         46        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.02it/s]\n",
      "                   all         36         55      0.367      0.382      0.275     0.0965\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     13/100      2.31G     0.9085      1.067       1.26         44        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.39it/s]\n",
      "                   all         36         55      0.309      0.382      0.206     0.0539\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     14/100      2.36G     0.9077      1.045      1.248         43        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.59it/s]\n",
      "                   all         36         55      0.367      0.418       0.28     0.0985\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     15/100      2.35G     0.8935      1.011      1.268         51        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.20it/s]\n",
      "                   all         36         55      0.602      0.441      0.506      0.178\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     16/100      2.33G     0.8654     0.9272      1.231         63        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.03it/s]\n",
      "                   all         36         55      0.661      0.509      0.518      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     17/100      2.32G     0.8457     0.9324      1.232         79        640: 100%|| 20/20 [00:01<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.15it/s]\n",
      "                   all         36         55      0.258        0.4      0.226     0.0904\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     18/100      2.35G     0.8452     0.9214      1.214         58        640: 100%|| 20/20 [00:02<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.02it/s]\n",
      "                   all         36         55      0.534      0.562      0.505      0.265\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     19/100      2.35G     0.7716     0.8602      1.166         66        640: 100%|| 20/20 [00:01<00:00, 10.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.38it/s]\n",
      "                   all         36         55      0.451      0.473      0.474      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     20/100      2.33G      0.769     0.8375      1.197         61        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.36it/s]\n",
      "                   all         36         55      0.491      0.436      0.431      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     21/100      2.36G     0.7517     0.8093      1.168         42        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.40it/s]\n",
      "                   all         36         55      0.527      0.506      0.445      0.158\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     22/100      2.33G     0.7407     0.8042      1.149         41        640: 100%|| 20/20 [00:01<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.96it/s]\n",
      "                   all         36         55      0.627      0.545      0.568      0.282\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     23/100      2.36G     0.7075     0.7906      1.151         37        640: 100%|| 20/20 [00:01<00:00, 10.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.46it/s]\n",
      "                   all         36         55      0.674      0.418      0.495      0.249\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     24/100      2.36G     0.7599     0.8277      1.186         72        640: 100%|| 20/20 [00:02<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.07it/s]\n",
      "                   all         36         55      0.661       0.39      0.468      0.206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     25/100      2.33G      0.731     0.7673      1.158         62        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.92it/s]\n",
      "                   all         36         55       0.63      0.436      0.467      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     26/100      2.33G     0.6909     0.7445       1.14         58        640: 100%|| 20/20 [00:01<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.27it/s]\n",
      "                   all         36         55      0.812        0.4      0.549      0.276\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     27/100      2.33G     0.6804     0.7082      1.103         48        640: 100%|| 20/20 [00:02<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.42it/s]\n",
      "                   all         36         55      0.614      0.545      0.623       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     28/100      2.36G     0.6366     0.6427      1.083         75        640: 100%|| 20/20 [00:01<00:00, 10.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.55it/s]\n",
      "                   all         36         55      0.821      0.502      0.584      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     29/100      2.33G     0.6705     0.6817      1.094         55        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.53it/s]\n",
      "                   all         36         55      0.698      0.505      0.587        0.3\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     30/100      2.34G     0.6288     0.6583      1.099         89        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.46it/s]\n",
      "                   all         36         55      0.601        0.6      0.576       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     31/100      2.35G     0.6083     0.6572      1.084         68        640: 100%|| 20/20 [00:01<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.37it/s]\n",
      "                   all         36         55      0.569      0.618       0.59      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     32/100      2.33G     0.6223     0.6308      1.079         59        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.37it/s]\n",
      "                   all         36         55      0.634      0.597      0.643      0.276\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     33/100      2.31G     0.6235     0.6275      1.076         52        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.56it/s]\n",
      "                   all         36         55      0.813      0.564      0.735      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     34/100      2.33G     0.5884     0.6144      1.059         63        640: 100%|| 20/20 [00:02<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.38it/s]\n",
      "                   all         36         55      0.534      0.473      0.473      0.213\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     35/100      2.33G     0.5906     0.5846      1.052         68        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.60it/s]\n",
      "                   all         36         55      0.471       0.55      0.424      0.171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     36/100      2.33G     0.6046     0.6139      1.072         55        640: 100%|| 20/20 [00:02<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.42it/s]\n",
      "                   all         36         55      0.679        0.5      0.559      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     37/100      2.31G     0.5689     0.5589      1.051         64        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.46it/s]\n",
      "                   all         36         55      0.833      0.509      0.641      0.344\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     38/100      2.36G     0.5496     0.5444      1.051         40        640: 100%|| 20/20 [00:02<00:00,  9.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.31it/s]\n",
      "                   all         36         55      0.808        0.6      0.659      0.366\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     39/100      2.36G     0.5493     0.5582       1.05         65        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.59it/s]\n",
      "                   all         36         55       0.81      0.709       0.76      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     40/100      2.36G     0.5398     0.5432      1.035         53        640: 100%|| 20/20 [00:01<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.54it/s]\n",
      "                   all         36         55      0.671      0.519      0.497      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     41/100      2.35G     0.5691     0.5931      1.043         57        640: 100%|| 20/20 [00:02<00:00,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.47it/s]\n",
      "                   all         36         55      0.709      0.575      0.659      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     42/100      2.36G     0.5586     0.5653      1.028         51        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.55it/s]\n",
      "                   all         36         55       0.75       0.49       0.63      0.326\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     43/100      2.33G     0.5489     0.5585      1.038         43        640: 100%|| 20/20 [00:02<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.18it/s]\n",
      "                   all         36         55       0.64      0.473      0.522      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     44/100      2.34G     0.5206     0.5203      1.027         55        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.47it/s]\n",
      "                   all         36         55      0.585      0.564       0.55      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     45/100      2.33G       0.52     0.5353      1.027         54        640: 100%|| 20/20 [00:02<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.06it/s]\n",
      "                   all         36         55      0.742      0.382      0.472      0.192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     46/100      2.33G     0.5091      0.533      1.021         78        640: 100%|| 20/20 [00:02<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.39it/s]\n",
      "                   all         36         55      0.763      0.527      0.618      0.287\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     47/100      2.33G     0.5097     0.5269      1.015         73        640: 100%|| 20/20 [00:02<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.37it/s]\n",
      "                   all         36         55       0.75       0.49      0.547      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     48/100      2.33G     0.4855      0.516      1.005         66        640: 100%|| 20/20 [00:02<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.38it/s]\n",
      "                   all         36         55      0.867      0.582      0.695      0.271\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     49/100      2.36G     0.4808     0.4836      1.002         78        640: 100%|| 20/20 [00:01<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.89it/s]\n",
      "                   all         36         55      0.912      0.527      0.682      0.283\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     50/100      2.36G     0.4945     0.4834      1.011         86        640: 100%|| 20/20 [00:02<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.57it/s]\n",
      "                   all         36         55      0.757      0.582      0.653      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     51/100      2.36G     0.4787     0.4716      1.002         54        640: 100%|| 20/20 [00:01<00:00, 10.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.53it/s]\n",
      "                   all         36         55      0.532        0.6      0.519      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     52/100      2.33G     0.4566     0.4669      0.988         49        640: 100%|| 20/20 [00:02<00:00,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.55it/s]\n",
      "                   all         36         55        0.8      0.655      0.704      0.361\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     53/100      2.34G     0.4429     0.4497     0.9878         55        640: 100%|| 20/20 [00:01<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.60it/s]\n",
      "                   all         36         55      0.752      0.673      0.746      0.354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     54/100      2.36G     0.4567     0.4611     0.9921         59        640: 100%|| 20/20 [00:02<00:00,  9.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.31it/s]\n",
      "                   all         36         55      0.589      0.496      0.511      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     55/100      2.36G     0.4441     0.4607     0.9823         63        640: 100%|| 20/20 [00:02<00:00,  9.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.73it/s]\n",
      "                   all         36         55       0.66      0.545      0.624      0.318\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     56/100      2.35G     0.4572     0.4491      0.987         71        640: 100%|| 20/20 [00:02<00:00,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.68it/s]\n",
      "                   all         36         55      0.833      0.633      0.672      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     57/100      2.33G     0.4582     0.4618     0.9822         51        640: 100%|| 20/20 [00:02<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.46it/s]\n",
      "                   all         36         55      0.752      0.455      0.446      0.227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     58/100      2.33G     0.4468     0.4693     0.9898         73        640: 100%|| 20/20 [00:02<00:00,  9.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.97it/s]\n",
      "                   all         36         55      0.783      0.524      0.572      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     59/100      2.33G      0.431     0.4569     0.9831         32        640: 100%|| 20/20 [00:02<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.30it/s]\n",
      "                   all         36         55      0.713      0.543      0.524      0.238\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     60/100      2.31G      0.441     0.4574     0.9896         61        640: 100%|| 20/20 [00:02<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.48it/s]\n",
      "                   all         36         55      0.616      0.437      0.478      0.264\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     61/100      2.33G     0.4457     0.4438     0.9804         54        640: 100%|| 20/20 [00:01<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.45it/s]\n",
      "                   all         36         55      0.769      0.527       0.59      0.296\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     62/100      2.31G     0.4433     0.4393     0.9826         52        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.78it/s]\n",
      "                   all         36         55      0.815      0.655        0.7      0.332\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     63/100      2.36G     0.4422     0.4394     0.9719         73        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 16.70it/s]\n",
      "                   all         36         55      0.687      0.655      0.646      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     64/100      2.33G      0.409     0.4201     0.9664         31        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.10it/s]\n",
      "                   all         36         55      0.631      0.473      0.522      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     65/100      2.31G     0.4015     0.4182     0.9713         65        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.83it/s]\n",
      "                   all         36         55      0.781      0.418      0.513      0.284\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     66/100      2.33G     0.4075     0.3993     0.9616         43        640: 100%|| 20/20 [00:02<00:00,  9.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 18.00it/s]\n",
      "                   all         36         55       0.71      0.455      0.545      0.327\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     67/100      2.33G     0.3989     0.3869     0.9522         45        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.38it/s]\n",
      "                   all         36         55      0.723      0.522      0.627      0.328\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     68/100      2.33G     0.3916     0.3838     0.9616         52        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.52it/s]\n",
      "                   all         36         55      0.792      0.436      0.559      0.279\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     69/100      2.34G     0.3969     0.4006     0.9603         49        640: 100%|| 20/20 [00:02<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.50it/s]\n",
      "                   all         36         55      0.887      0.436      0.531      0.227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     70/100      2.33G     0.3954     0.3952     0.9541         59        640: 100%|| 20/20 [00:02<00:00,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.72it/s]\n",
      "                   all         36         55      0.674      0.526      0.538      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     71/100      2.35G      0.388     0.3825     0.9563         62        640: 100%|| 20/20 [00:02<00:00,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.87it/s]\n",
      "                   all         36         55       0.78      0.473      0.541      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     72/100      2.33G     0.3772     0.3877     0.9518         51        640: 100%|| 20/20 [00:01<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.79it/s]\n",
      "                   all         36         55      0.812      0.473      0.539      0.252\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     73/100      2.31G     0.3603     0.3729     0.9352         73        640: 100%|| 20/20 [00:02<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.53it/s]\n",
      "                   all         36         55      0.865      0.473      0.635      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     74/100      2.33G     0.3806      0.379     0.9597         46        640: 100%|| 20/20 [00:02<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.59it/s]\n",
      "                   all         36         55      0.644        0.6      0.634      0.298\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     75/100      2.36G     0.3737      0.377     0.9518         62        640: 100%|| 20/20 [00:02<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.74it/s]\n",
      "                   all         36         55      0.821      0.618      0.708       0.35\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     76/100      2.33G     0.3448       0.35     0.9413         41        640: 100%|| 20/20 [00:02<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.85it/s]\n",
      "                   all         36         55      0.841      0.564      0.625      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     77/100      2.36G     0.3641     0.3613     0.9509         57        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.69it/s]\n",
      "                   all         36         55      0.714      0.591       0.64      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     78/100      2.33G     0.3724     0.3664     0.9505         65        640: 100%|| 20/20 [00:02<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.68it/s]\n",
      "                   all         36         55      0.728      0.545       0.62      0.335\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     79/100      2.31G     0.3632     0.3567     0.9439         66        640: 100%|| 20/20 [00:02<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.72it/s]\n",
      "                   all         36         55       0.81      0.491        0.6      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     80/100      2.34G     0.3872     0.3775     0.9543         45        640: 100%|| 20/20 [00:02<00:00,  9.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.66it/s]\n",
      "                   all         36         55       0.77      0.473      0.611      0.303\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     81/100      2.36G     0.3536     0.3509     0.9328         31        640: 100%|| 20/20 [00:02<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.23it/s]\n",
      "                   all         36         55       0.64      0.564      0.589      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     82/100      2.33G     0.3504     0.3528     0.9441         58        640: 100%|| 20/20 [00:02<00:00,  9.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.67it/s]\n",
      "                   all         36         55      0.664      0.618      0.621      0.328\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     83/100      2.35G     0.3365     0.3393     0.9304         55        640: 100%|| 20/20 [00:01<00:00, 10.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.62it/s]\n",
      "                   all         36         55      0.703      0.647       0.66      0.337\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     84/100      2.33G     0.3492     0.3506     0.9382         67        640: 100%|| 20/20 [00:02<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.66it/s]\n",
      "                   all         36         55      0.708      0.655      0.677      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     85/100      2.33G     0.3367     0.3317     0.9311         49        640: 100%|| 20/20 [00:01<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.57it/s]\n",
      "                   all         36         55      0.795      0.545       0.61      0.304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     86/100      2.36G     0.3289      0.332     0.9187         52        640: 100%|| 20/20 [00:02<00:00,  9.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.36it/s]\n",
      "                   all         36         55      0.795      0.509      0.604      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     87/100      2.33G     0.3284     0.3309     0.9296         57        640: 100%|| 20/20 [00:02<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.52it/s]\n",
      "                   all         36         55       0.65      0.575       0.61      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     88/100      2.36G     0.3313     0.3323     0.9258         66        640: 100%|| 20/20 [00:02<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.79it/s]\n",
      "                   all         36         55      0.701       0.47      0.551      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     89/100      2.33G     0.3203     0.3222     0.9264         52        640: 100%|| 20/20 [00:01<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00, 17.63it/s]\n",
      "                   all         36         55      0.783      0.473      0.591       0.29\n",
      "Stopping training early as no improvement observed in last 50 epochs. Best results observed at epoch 39, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=50) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "89 epochs completed in 0.061 hours.\n",
      "Optimizer stripped from runs/detect/train6/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train6/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train6/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.209  Python-3.9.12 torch-1.12.1 CUDA:0 (NVIDIA GeForce RTX 3060, 12043MiB)\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  9.11it/s]\n",
      "                   all         36         55      0.807      0.709      0.763      0.391\n",
      "                   CAT         36         55      0.807      0.709      0.763      0.391\n",
      "Speed: 1.4ms preprocess, 1.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7f7d23b88d00>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,        0.92,\n",
       "               0.92,        0.92,        0.92,        0.92,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n",
       "                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n",
       "                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,\n",
       "                0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,         0.9,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,\n",
       "            0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,\n",
       "            0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,\n",
       "            0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,\n",
       "            0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,\n",
       "            0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.87179,     0.83721,     0.83721,     0.83721,\n",
       "            0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,\n",
       "            0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.83721,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,\n",
       "            0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,\n",
       "            0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.82979,     0.78431,     0.78431,     0.78431,     0.78431,\n",
       "            0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.78431,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,\n",
       "            0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.77358,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,\n",
       "            0.76364,     0.76364,     0.76364,     0.76364,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,     0.74138,\n",
       "            0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.59459,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,\n",
       "            0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,     0.47872,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,\n",
       "             0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,      0.4466,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,     0.41593,\n",
       "            0.41593,     0.41593,     0.41593,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,       0.384,     0.37692,     0.37692,\n",
       "            0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.37692,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,\n",
       "            0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.33784,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,\n",
       "            0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.15741,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,     0.11429,\n",
       "            0.11429,     0.11429,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.085761,    0.067597,    0.065719,    0.063841,\n",
       "           0.061964,    0.060086,    0.058208,     0.05633,    0.054453,    0.052575,    0.050697,     0.04882,    0.046942,    0.045064,    0.043187,    0.041309,    0.039431,    0.037554,    0.035676,    0.033798,    0.031921,    0.030043,    0.028165,    0.026288,     0.02441,    0.022532,    0.020655,\n",
       "           0.018777,    0.016899,    0.015021,    0.013144,    0.011266,   0.0093884,   0.0075107,    0.005633,   0.0037554,   0.0018777,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.1274,      0.1274,     0.21931,     0.27771,     0.32353,      0.3686,     0.38914,     0.42002,     0.43792,     0.45532,     0.46633,     0.47505,     0.48156,     0.49125,     0.49881,      0.5122,     0.52492,     0.52588,      0.5324,     0.52674,     0.53563,     0.54279,     0.55192,\n",
       "            0.56849,     0.57554,     0.57699,     0.57843,     0.57488,     0.58446,     0.58767,     0.59204,      0.6036,     0.59992,     0.59623,     0.60061,     0.60482,     0.60821,     0.61269,     0.61558,     0.61817,     0.62127,      0.6274,     0.63415,     0.63864,     0.64163,     0.64372,\n",
       "            0.64552,     0.64783,     0.65224,     0.65377,     0.65529,     0.65686,     0.65944,     0.66184,     0.66319,     0.66454,     0.66588,     0.66809,     0.67149,     0.67265,     0.67362,     0.67459,     0.67556,     0.67653,     0.67788,     0.67951,     0.68113,     0.68138,     0.67917,\n",
       "            0.67695,     0.67472,      0.6725,     0.67439,     0.67734,     0.67824,     0.67913,     0.68002,     0.68091,      0.6818,     0.68542,     0.68906,     0.69029,     0.69152,     0.69275,     0.69514,     0.69923,     0.69959,     0.69994,     0.70029,     0.70065,       0.701,     0.70135,\n",
       "             0.7017,     0.70206,     0.70241,     0.70276,     0.70311,     0.70346,     0.70381,     0.70416,     0.70451,     0.70486,     0.70896,     0.71278,     0.71601,     0.71775,     0.71911,     0.72046,     0.72181,     0.72427,     0.72881,     0.72915,     0.72948,     0.72982,     0.73015,\n",
       "            0.73048,     0.73081,     0.73115,     0.73148,     0.73181,     0.73214,     0.73247,     0.73281,     0.73314,     0.73347,      0.7338,     0.73413,     0.73446,     0.73479,     0.73594,     0.73994,     0.74398,     0.74785,     0.74835,     0.74885,     0.74934,     0.74984,     0.75033,\n",
       "            0.75083,     0.75132,     0.75181,      0.7523,      0.7528,     0.75329,     0.75378,     0.75427,     0.75474,     0.75519,     0.75565,     0.75611,     0.75657,     0.75703,     0.75748,     0.75794,      0.7584,     0.75885,     0.75931,     0.75976,     0.76021,     0.76067,     0.75917,\n",
       "            0.75438,     0.75717,     0.75769,     0.75821,     0.75873,     0.75925,     0.75977,     0.76028,      0.7608,     0.76132,     0.76183,     0.76235,     0.76286,     0.76338,     0.76038,     0.75373,     0.75467,     0.75767,     0.75873,     0.75761,     0.75648,     0.75535,     0.75422,\n",
       "            0.75309,     0.75195,     0.75082,     0.74968,     0.74855,     0.74835,     0.75141,     0.75444,     0.75446,     0.75417,     0.75388,     0.75359,      0.7533,     0.75302,     0.75273,     0.75244,     0.75215,     0.75186,     0.75157,     0.75129,       0.751,     0.75071,     0.75042,\n",
       "            0.75013,     0.74984,     0.74955,     0.74926,     0.74897,     0.74868,     0.74839,      0.7481,     0.74782,     0.74753,     0.74724,     0.74695,     0.74666,     0.74637,     0.74608,     0.74578,     0.74549,      0.7452,     0.74491,     0.74462,     0.74433,     0.74404,     0.74375,\n",
       "            0.74346,     0.74317,     0.74288,      0.7432,     0.74358,     0.74395,     0.74432,     0.74469,     0.74506,     0.74543,      0.7458,     0.74617,     0.74654,      0.7469,     0.74727,     0.74764,       0.748,     0.74837,     0.74874,      0.7491,     0.74947,     0.74983,      0.7501,\n",
       "            0.75029,     0.75048,     0.75066,     0.75085,     0.75104,     0.75122,     0.75141,      0.7516,     0.75178,     0.75197,     0.75216,     0.75234,     0.75253,     0.75272,      0.7529,     0.75309,     0.75327,     0.75346,     0.75364,     0.75383,     0.75401,      0.7542,     0.75438,\n",
       "            0.75457,     0.75475,     0.75494,     0.75512,     0.75531,     0.75549,     0.75568,     0.75586,     0.75605,     0.75623,     0.75641,      0.7566,     0.75678,     0.75697,     0.75715,     0.75735,     0.75761,     0.75787,     0.75813,     0.75839,     0.75864,      0.7589,     0.75916,\n",
       "            0.75942,     0.75967,     0.75993,     0.76019,     0.76044,      0.7607,     0.76095,     0.76121,     0.76147,     0.76172,     0.76198,     0.76223,     0.76249,     0.76274,     0.76299,     0.76325,      0.7635,     0.76376,     0.76401,     0.76426,     0.76452,     0.76397,     0.76103,\n",
       "            0.75809,     0.75513,     0.75222,      0.7498,     0.74737,     0.74493,     0.74249,     0.74004,     0.73939,     0.73876,     0.73814,     0.73751,     0.73688,     0.73626,     0.73563,       0.735,     0.73437,     0.73375,     0.73312,     0.73249,     0.73186,     0.73123,      0.7306,\n",
       "            0.72997,     0.72933,      0.7287,     0.72807,     0.72744,     0.72769,     0.72825,      0.7288,     0.72936,     0.72992,     0.73047,     0.73103,     0.73158,     0.73213,     0.73268,     0.73323,     0.73378,     0.73433,     0.73463,     0.73442,     0.73422,     0.73401,     0.73381,\n",
       "             0.7336,      0.7334,     0.73319,     0.73299,     0.73278,     0.73258,     0.73237,     0.73216,     0.73196,     0.73175,     0.73155,     0.73134,     0.73114,     0.73093,     0.73072,     0.73052,     0.73031,     0.73011,      0.7299,     0.72969,     0.72949,     0.72928,     0.72908,\n",
       "            0.72887,     0.72866,     0.72846,     0.72825,     0.72804,     0.72784,     0.72763,     0.72742,     0.72722,     0.72701,      0.7268,      0.7266,     0.72639,     0.72618,     0.72598,     0.72577,     0.72556,     0.72535,     0.72515,     0.72494,     0.72473,     0.72453,     0.72432,\n",
       "            0.72411,      0.7239,      0.7237,     0.72349,     0.72328,     0.72307,     0.72286,     0.72266,     0.72245,     0.72224,     0.72203,     0.72182,     0.70836,     0.70851,     0.70866,     0.70882,     0.70897,     0.70912,     0.70928,     0.70943,     0.70958,     0.70974,     0.70989,\n",
       "            0.71004,     0.71019,     0.71035,      0.7105,     0.71065,      0.7108,     0.71095,     0.71111,     0.71126,     0.71141,     0.71156,     0.71171,     0.71186,     0.71202,     0.71217,     0.71232,     0.71247,     0.71262,     0.71277,     0.71292,     0.71307,     0.71322,     0.71337,\n",
       "            0.71353,     0.71368,     0.71383,     0.71398,     0.71413,     0.71428,     0.71443,     0.71458,     0.71473,     0.71488,     0.71503,     0.71518,     0.71532,     0.71547,     0.71562,     0.71577,      0.7159,     0.71602,     0.71614,     0.71626,     0.71638,      0.7165,     0.71662,\n",
       "            0.71673,     0.71685,     0.71697,     0.71709,     0.71721,     0.71733,     0.71745,     0.71757,     0.71769,     0.71781,     0.71793,     0.71805,     0.71817,     0.71828,      0.7184,     0.71852,     0.71864,     0.71876,     0.71888,       0.719,     0.71912,     0.71923,     0.71935,\n",
       "            0.71947,     0.71959,     0.71971,     0.71983,     0.71994,     0.72006,     0.72018,      0.7203,     0.72042,     0.72053,     0.72065,     0.72077,     0.72089,       0.721,     0.72112,     0.72124,     0.72136,     0.72147,     0.72159,     0.72171,     0.72183,     0.72194,     0.72206,\n",
       "            0.72218,     0.72229,     0.72241,     0.72253,     0.72265,     0.72276,     0.72288,       0.723,     0.72311,     0.72323,     0.72335,     0.72335,     0.72324,     0.72313,     0.72303,     0.72292,     0.72281,      0.7227,      0.7226,     0.72249,     0.72238,     0.72227,     0.72216,\n",
       "            0.72206,     0.72195,     0.72184,     0.72173,     0.72162,     0.72152,     0.72141,      0.7213,     0.72119,     0.72108,     0.72098,     0.72087,     0.72076,     0.72065,     0.72054,     0.72044,     0.72033,     0.72022,     0.72011,        0.72,      0.7199,     0.71979,     0.71968,\n",
       "            0.71957,     0.71946,     0.71935,     0.71925,     0.71914,     0.71903,     0.71892,     0.71881,      0.7187,      0.7186,     0.71849,     0.71838,     0.71827,     0.71816,     0.71805,     0.71795,     0.71784,     0.71773,     0.71762,     0.71751,      0.7174,     0.71729,     0.71719,\n",
       "            0.71708,     0.71697,     0.71686,     0.71675,     0.71664,     0.71653,     0.71643,     0.71632,     0.71621,      0.7161,     0.71599,     0.71588,     0.71577,     0.71566,     0.71556,     0.71545,     0.71534,     0.71523,     0.71512,     0.71501,      0.7149,     0.71479,     0.71468,\n",
       "            0.71458,     0.71447,     0.71436,     0.71425,     0.71414,     0.71403,     0.71392,     0.71381,      0.7137,      0.7136,     0.71349,     0.71338,     0.71327,     0.71316,     0.71305,     0.71294,     0.71283,     0.71272,     0.71261,      0.7125,     0.71239,     0.71229,     0.71218,\n",
       "            0.71207,     0.71196,     0.71185,     0.71174,     0.71163,     0.71152,     0.71141,      0.7113,     0.71119,     0.71108,     0.71097,     0.71086,     0.71075,     0.71065,     0.71054,     0.71043,     0.71032,     0.71021,      0.7101,     0.70999,     0.70988,     0.70977,     0.70946,\n",
       "             0.7081,     0.70674,     0.70537,     0.70401,     0.70264,     0.70127,      0.6999,     0.69852,     0.69714,     0.69576,      0.6923,     0.68863,     0.68495,     0.68131,     0.68061,     0.67992,     0.67923,     0.67853,     0.67784,     0.67714,     0.67645,     0.67575,     0.67505,\n",
       "            0.67435,     0.67365,     0.67296,     0.67226,     0.67156,     0.67085,     0.67015,     0.66945,     0.66875,     0.66805,     0.66734,     0.66649,     0.66212,     0.65774,     0.65333,     0.65099,     0.64987,     0.64876,     0.64764,     0.64653,     0.64541,     0.64428,     0.64316,\n",
       "            0.64204,     0.64091,     0.63979,     0.63866,     0.63753,      0.6364,     0.63522,     0.63404,     0.63285,     0.63167,     0.63048,     0.62929,      0.6281,     0.62691,     0.62572,     0.62453,     0.62333,     0.62213,     0.62093,     0.62107,     0.62154,     0.62201,     0.62247,\n",
       "            0.62294,      0.6234,     0.62387,     0.62433,     0.62479,     0.62525,     0.62571,     0.62617,     0.62663,     0.62708,     0.62754,     0.62812,     0.62922,     0.63031,      0.6314,     0.63248,     0.63355,     0.63461,     0.63484,     0.63357,      0.6323,     0.63102,     0.62975,\n",
       "            0.62847,     0.62719,     0.62591,     0.62462,     0.62334,     0.62205,     0.62076,     0.61947,     0.61778,      0.6159,     0.61401,     0.61212,     0.61023,     0.60833,     0.60643,     0.60452,     0.60261,     0.58675,      0.5842,     0.58293,     0.58166,     0.58039,     0.57911,\n",
       "            0.57784,     0.57656,     0.57528,       0.574,     0.57272,     0.57143,     0.57014,     0.56885,     0.56815,     0.56908,        0.57,     0.57092,     0.57183,     0.57273,     0.57363,     0.57452,     0.57395,     0.57166,     0.56936,     0.56706,     0.56474,     0.56243,      0.5601,\n",
       "            0.55777,     0.55521,     0.55252,     0.54982,     0.54712,      0.5444,     0.54168,     0.53894,     0.53758,     0.53651,     0.53544,     0.53436,     0.53329,     0.53221,     0.53114,     0.53006,     0.52898,      0.5279,     0.52681,     0.52573,     0.52464,     0.52355,     0.52247,\n",
       "            0.52138,     0.52028,     0.51887,     0.51654,     0.51421,     0.51187,     0.50952,     0.50717,     0.50481,     0.50244,     0.50007,     0.49718,     0.49427,     0.49135,     0.48842,     0.48549,     0.48254,     0.47958,      0.4766,     0.47361,     0.47061,      0.4676,     0.46458,\n",
       "            0.46155,     0.45924,     0.45852,     0.45781,     0.45709,     0.45638,     0.45566,     0.45495,     0.45423,     0.45352,      0.4528,     0.45208,     0.45136,     0.45064,     0.44992,      0.4492,     0.44848,     0.44776,     0.44704,     0.44631,     0.44559,     0.44487,     0.44414,\n",
       "            0.44342,     0.44269,     0.44196,     0.44124,     0.44051,     0.43978,     0.43905,     0.43815,     0.43375,     0.42932,     0.42487,      0.4204,     0.41286,     0.39138,     0.37572,     0.34062,     0.33066,     0.32058,     0.31037,     0.30005,     0.30122,     0.28234,     0.26045,\n",
       "            0.23766,     0.19515,     0.19414,     0.19472,     0.19527,      0.1958,      0.1963,     0.19551,     0.18534,     0.17505,       0.166,     0.16251,       0.159,     0.15549,     0.15196,     0.14842,     0.14486,     0.14129,      0.1377,     0.13023,     0.11711,      0.1038,     0.10127,\n",
       "           0.099028,     0.09678,    0.094527,    0.092269,    0.090006,    0.087737,    0.085462,    0.083182,    0.080897,    0.078607,     0.07631,    0.074009,    0.071702,    0.069438,    0.067265,    0.065087,    0.062904,    0.060716,    0.058523,    0.056326,    0.054123,    0.051915,    0.049703,\n",
       "           0.047485,    0.045262,    0.043034,    0.040801,    0.038563,     0.03632,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.068211,    0.068211,     0.12436,     0.16388,     0.19678,     0.23116,     0.24755,      0.2731,     0.28843,     0.30372,     0.31359,     0.32153,     0.32752,     0.33655,     0.34637,     0.35942,     0.37208,     0.37632,     0.38343,        0.38,     0.39006,      0.3977,     0.41185,\n",
       "            0.43058,     0.43872,     0.44041,     0.44209,     0.44216,      0.4546,      0.4585,     0.46384,     0.47847,     0.47628,     0.47409,     0.48078,     0.48621,     0.49059,     0.49645,     0.50025,     0.50368,     0.50782,     0.51606,     0.52525,     0.53144,      0.5356,     0.53851,\n",
       "            0.54104,      0.5443,     0.55056,     0.55273,     0.55491,     0.55717,     0.56089,     0.56437,     0.56634,     0.56831,     0.57028,     0.57352,     0.57856,     0.58028,     0.58173,     0.58318,     0.58462,     0.58607,     0.58811,     0.59056,     0.59301,     0.59417,     0.59297,\n",
       "            0.59177,     0.59057,     0.58938,     0.59291,      0.5975,     0.59889,     0.60028,     0.60168,     0.60307,     0.60446,     0.61019,     0.61597,     0.61795,     0.61992,     0.62189,     0.62577,     0.63242,       0.633,     0.63358,     0.63416,     0.63474,     0.63532,      0.6359,\n",
       "            0.63648,     0.63706,     0.63764,     0.63822,      0.6388,     0.63938,     0.63996,     0.64054,     0.64112,      0.6417,     0.64852,     0.65494,     0.66042,     0.66339,     0.66571,     0.66803,     0.67035,     0.67461,     0.68254,     0.68313,     0.68371,      0.6843,     0.68489,\n",
       "            0.68547,     0.68606,     0.68664,     0.68723,     0.68782,      0.6884,     0.68899,     0.68958,     0.69016,     0.69075,     0.69133,     0.69192,     0.69251,     0.69309,     0.69515,     0.70232,     0.70964,     0.71671,     0.71763,     0.71854,     0.71946,     0.72037,     0.72128,\n",
       "             0.7222,     0.72311,     0.72402,     0.72494,     0.72585,     0.72677,     0.72768,     0.72859,     0.72947,     0.73032,     0.73118,     0.73204,      0.7329,     0.73376,     0.73462,     0.73548,     0.73634,     0.73719,     0.73805,     0.73891,     0.73977,     0.74063,      0.7406,\n",
       "            0.74535,      0.7508,     0.75183,     0.75286,     0.75388,     0.75491,     0.75593,     0.75696,     0.75799,     0.75901,     0.76004,     0.76106,     0.76209,     0.76312,     0.76237,     0.75981,     0.76411,     0.77029,     0.77339,     0.77296,     0.77254,     0.77211,     0.77169,\n",
       "            0.77126,     0.77084,     0.77041,     0.76999,     0.76956,     0.77069,      0.7772,     0.78371,     0.78422,     0.78411,     0.78401,      0.7839,      0.7838,     0.78369,     0.78359,     0.78348,     0.78338,     0.78327,     0.78317,     0.78306,     0.78295,     0.78285,     0.78274,\n",
       "            0.78264,     0.78253,     0.78243,     0.78232,     0.78222,     0.78211,     0.78201,      0.7819,      0.7818,     0.78169,     0.78159,     0.78148,     0.78138,     0.78127,     0.78117,     0.78106,     0.78095,     0.78085,     0.78074,     0.78064,     0.78053,     0.78043,     0.78032,\n",
       "            0.78022,     0.78011,     0.78001,     0.78077,     0.78159,     0.78241,     0.78323,     0.78405,     0.78487,     0.78569,     0.78651,     0.78733,     0.78815,     0.78898,      0.7898,     0.79062,     0.79144,     0.79226,     0.79308,      0.7939,     0.79472,     0.79554,     0.79615,\n",
       "            0.79657,     0.79699,     0.79741,     0.79784,     0.79826,     0.79868,      0.7991,     0.79953,     0.79995,     0.80037,     0.80079,     0.80121,     0.80164,     0.80206,     0.80248,      0.8029,     0.80333,     0.80375,     0.80417,     0.80459,     0.80502,     0.80544,     0.80586,\n",
       "            0.80628,      0.8067,     0.80713,     0.80755,     0.80797,     0.80839,     0.80882,     0.80924,     0.80966,     0.81008,      0.8105,     0.81093,     0.81135,     0.81177,     0.81219,     0.81266,     0.81326,     0.81386,     0.81445,     0.81505,     0.81564,     0.81624,     0.81683,\n",
       "            0.81743,     0.81802,     0.81862,     0.81922,     0.81981,     0.82041,       0.821,      0.8216,     0.82219,     0.82279,     0.82339,     0.82398,     0.82458,     0.82517,     0.82577,     0.82636,     0.82696,     0.82755,     0.82815,     0.82875,     0.82934,     0.82956,     0.82867,\n",
       "            0.82778,     0.82689,     0.82601,     0.82525,      0.8245,     0.82374,     0.82299,     0.82223,     0.82203,     0.82183,     0.82163,     0.82143,     0.82123,     0.82103,     0.82083,     0.82063,     0.82043,     0.82023,     0.82003,     0.81983,     0.81963,     0.81943,     0.81923,\n",
       "            0.81903,     0.81883,     0.81863,     0.81843,     0.81823,     0.81923,     0.82065,     0.82207,     0.82349,     0.82491,     0.82632,     0.82774,     0.82916,     0.83058,       0.832,     0.83342,     0.83483,     0.83625,     0.83719,     0.83713,     0.83707,     0.83701,     0.83694,\n",
       "            0.83688,     0.83682,     0.83676,      0.8367,     0.83664,     0.83658,     0.83651,     0.83645,     0.83639,     0.83633,     0.83627,     0.83621,     0.83615,     0.83608,     0.83602,     0.83596,      0.8359,     0.83584,     0.83578,     0.83572,     0.83566,     0.83559,     0.83553,\n",
       "            0.83547,     0.83541,     0.83535,     0.83529,     0.83523,     0.83516,      0.8351,     0.83504,     0.83498,     0.83492,     0.83486,      0.8348,     0.83473,     0.83467,     0.83461,     0.83455,     0.83449,     0.83443,     0.83437,     0.83431,     0.83424,     0.83418,     0.83412,\n",
       "            0.83406,       0.834,     0.83394,     0.83388,     0.83381,     0.83375,     0.83369,     0.83363,     0.83357,     0.83351,     0.83345,     0.83339,     0.82933,     0.82976,     0.83018,      0.8306,     0.83102,     0.83144,     0.83186,     0.83228,      0.8327,     0.83312,     0.83354,\n",
       "            0.83396,     0.83438,      0.8348,     0.83523,     0.83565,     0.83607,     0.83649,     0.83691,     0.83733,     0.83775,     0.83817,     0.83859,     0.83901,     0.83943,     0.83985,     0.84028,      0.8407,     0.84112,     0.84154,     0.84196,     0.84238,      0.8428,     0.84322,\n",
       "            0.84364,     0.84406,     0.84448,      0.8449,     0.84532,     0.84575,     0.84617,     0.84659,     0.84701,     0.84743,     0.84785,     0.84827,     0.84869,     0.84911,     0.84953,     0.84995,      0.8503,     0.85064,     0.85098,     0.85132,     0.85166,     0.85199,     0.85233,\n",
       "            0.85267,     0.85301,     0.85335,     0.85369,     0.85402,     0.85436,      0.8547,     0.85504,     0.85538,     0.85572,     0.85606,     0.85639,     0.85673,     0.85707,     0.85741,     0.85775,     0.85809,     0.85842,     0.85876,      0.8591,     0.85944,     0.85978,     0.86012,\n",
       "            0.86046,     0.86079,     0.86113,     0.86147,     0.86181,     0.86215,     0.86249,     0.86283,     0.86316,      0.8635,     0.86384,     0.86418,     0.86452,     0.86486,     0.86519,     0.86553,     0.86587,     0.86621,     0.86655,     0.86689,     0.86723,     0.86756,      0.8679,\n",
       "            0.86824,     0.86858,     0.86892,     0.86926,      0.8696,     0.86993,     0.87027,     0.87061,     0.87095,     0.87129,     0.87163,     0.87178,     0.87175,     0.87173,      0.8717,     0.87167,     0.87165,     0.87162,     0.87159,     0.87157,     0.87154,     0.87151,     0.87149,\n",
       "            0.87146,     0.87143,     0.87141,     0.87138,     0.87135,     0.87133,      0.8713,     0.87127,     0.87125,     0.87122,     0.87119,     0.87117,     0.87114,     0.87111,     0.87109,     0.87106,     0.87103,     0.87101,     0.87098,     0.87095,     0.87093,      0.8709,     0.87087,\n",
       "            0.87085,     0.87082,     0.87079,     0.87077,     0.87074,     0.87071,     0.87069,     0.87066,     0.87063,     0.87061,     0.87058,     0.87055,     0.87053,      0.8705,     0.87047,     0.87045,     0.87042,     0.87039,     0.87037,     0.87034,     0.87031,     0.87029,     0.87026,\n",
       "            0.87023,     0.87021,     0.87018,     0.87015,     0.87013,      0.8701,     0.87007,     0.87005,     0.87002,     0.86999,     0.86997,     0.86994,     0.86991,     0.86989,     0.86986,     0.86983,     0.86981,     0.86978,     0.86975,     0.86973,      0.8697,     0.86967,     0.86965,\n",
       "            0.86962,     0.86959,     0.86957,     0.86954,     0.86951,     0.86948,     0.86946,     0.86943,      0.8694,     0.86938,     0.86935,     0.86932,      0.8693,     0.86927,     0.86924,     0.86922,     0.86919,     0.86916,     0.86914,     0.86911,     0.86908,     0.86906,     0.86903,\n",
       "              0.869,     0.86898,     0.86895,     0.86892,      0.8689,     0.86887,     0.86884,     0.86882,     0.86879,     0.86876,     0.86874,     0.86871,     0.86868,     0.86866,     0.86863,      0.8686,     0.86858,     0.86855,     0.86852,      0.8685,     0.86847,     0.86844,     0.86836,\n",
       "            0.86802,     0.86767,     0.86732,     0.86698,     0.86663,     0.86628,     0.86593,     0.86559,     0.86524,     0.86489,     0.86398,     0.86302,     0.86206,     0.86111,     0.86092,     0.86073,     0.86054,     0.86035,     0.86016,     0.85997,     0.85978,     0.85959,     0.85941,\n",
       "            0.85922,     0.85903,     0.85884,     0.85865,     0.85846,     0.85827,     0.85808,     0.85789,      0.8577,     0.85751,     0.85732,     0.85709,     0.85586,     0.85463,      0.8534,     0.85274,     0.85241,     0.85208,     0.85176,     0.85143,     0.85111,     0.85078,     0.85045,\n",
       "            0.85013,      0.8498,     0.84947,     0.84915,     0.84882,      0.8485,     0.84814,     0.84778,     0.84742,     0.84706,      0.8467,     0.84634,     0.84598,     0.84562,     0.84526,      0.8449,     0.84454,     0.84418,     0.84382,     0.84514,     0.84689,     0.84863,     0.85037,\n",
       "            0.85212,     0.85386,      0.8556,     0.85735,     0.85909,     0.86084,     0.86258,     0.86432,     0.86607,     0.86781,     0.86955,     0.87177,     0.87602,     0.88027,     0.88452,     0.88878,     0.89303,     0.89728,      0.8999,     0.89963,     0.89936,     0.89909,     0.89881,\n",
       "            0.89854,     0.89827,       0.898,     0.89773,     0.89746,     0.89718,     0.89691,     0.89664,     0.89627,     0.89585,     0.89543,       0.895,     0.89458,     0.89416,     0.89374,     0.89332,      0.8929,     0.88921,      0.8886,     0.88829,     0.88797,     0.88766,     0.88735,\n",
       "            0.88704,     0.88672,     0.88641,      0.8861,     0.88578,     0.88547,     0.88516,     0.88485,     0.88581,     0.89034,     0.89488,     0.89941,     0.90394,     0.90848,     0.91301,     0.91754,      0.9198,     0.91938,     0.91895,     0.91852,      0.9181,     0.91767,     0.91724,\n",
       "            0.91681,     0.91632,     0.91579,     0.91526,     0.91473,      0.9142,     0.91367,     0.91314,     0.91286,     0.91263,     0.91241,     0.91218,     0.91196,     0.91173,     0.91151,     0.91128,     0.91106,     0.91083,     0.91061,     0.91038,     0.91016,     0.90993,     0.90971,\n",
       "            0.90948,     0.90926,     0.90895,     0.90843,     0.90791,     0.90739,     0.90686,     0.90634,     0.90582,      0.9053,     0.90478,     0.90408,     0.90339,     0.90269,     0.90199,      0.9013,      0.9006,     0.89989,     0.89912,     0.89835,     0.89758,     0.89681,     0.89604,\n",
       "            0.89527,     0.89467,     0.89447,     0.89427,     0.89407,     0.89387,     0.89368,     0.89348,     0.89328,     0.89308,     0.89288,     0.89268,     0.89248,     0.89228,     0.89208,     0.89188,     0.89168,     0.89148,     0.89128,     0.89108,     0.89088,     0.89068,     0.89048,\n",
       "            0.89028,     0.89008,     0.88988,     0.88968,     0.88948,     0.88928,     0.88908,     0.88883,     0.88749,     0.88615,     0.88481,     0.88347,     0.88109,      0.8739,     0.86821,     0.85386,     0.84935,     0.84463,     0.83937,     0.83412,     0.87739,     0.90187,     0.89317,\n",
       "            0.88267,     0.85813,     0.88089,      0.9053,      0.9297,     0.95411,     0.97851,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.96364,     0.96364,     0.92727,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.90909,     0.89091,     0.89091,     0.89091,     0.87273,     0.87065,      0.8581,     0.85455,     0.85455,     0.83636,\n",
       "            0.83636,     0.83636,     0.83636,     0.83636,     0.82147,     0.81818,     0.81818,     0.81818,     0.81735,     0.81025,     0.80315,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,\n",
       "                0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,         0.8,     0.79861,     0.79468,\n",
       "            0.79076,     0.78684,     0.78291,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,\n",
       "            0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,\n",
       "            0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,\n",
       "            0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.78182,     0.77869,\n",
       "            0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,     0.76364,      0.7584,     0.74775,     0.74545,     0.74545,     0.74462,     0.74285,     0.74107,      0.7393,     0.73752,\n",
       "            0.73575,     0.73397,      0.7322,     0.73042,     0.72865,     0.72727,     0.72727,     0.72727,     0.72687,     0.72643,     0.72598,     0.72554,      0.7251,     0.72465,     0.72421,     0.72376,     0.72332,     0.72288,     0.72243,     0.72199,     0.72155,      0.7211,     0.72066,\n",
       "            0.72021,     0.71977,     0.71933,     0.71888,     0.71844,       0.718,     0.71755,     0.71711,     0.71666,     0.71622,     0.71578,     0.71533,     0.71489,     0.71445,       0.714,     0.71356,     0.71311,     0.71267,     0.71223,     0.71178,     0.71134,      0.7109,     0.71045,\n",
       "            0.71001,     0.70956,     0.70912,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,\n",
       "            0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,\n",
       "            0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,\n",
       "            0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70909,     0.70799,      0.7036,\n",
       "            0.69922,     0.69483,     0.69053,     0.68698,     0.68343,     0.67989,     0.67634,     0.67279,     0.67184,     0.67095,     0.67005,     0.66915,     0.66825,     0.66735,     0.66645,     0.66556,     0.66466,     0.66376,     0.66286,     0.66196,     0.66107,     0.66017,     0.65927,\n",
       "            0.65837,     0.65747,     0.65658,     0.65568,     0.65478,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65455,     0.65445,     0.65416,     0.65388,     0.65359,      0.6533,\n",
       "            0.65301,     0.65272,     0.65244,     0.65215,     0.65186,     0.65157,     0.65129,       0.651,     0.65071,     0.65042,     0.65013,     0.64985,     0.64956,     0.64927,     0.64898,      0.6487,     0.64841,     0.64812,     0.64783,     0.64754,     0.64726,     0.64697,     0.64668,\n",
       "            0.64639,      0.6461,     0.64582,     0.64553,     0.64524,     0.64495,     0.64467,     0.64438,     0.64409,      0.6438,     0.64351,     0.64323,     0.64294,     0.64265,     0.64236,     0.64208,     0.64179,      0.6415,     0.64121,     0.64092,     0.64064,     0.64035,     0.64006,\n",
       "            0.63977,     0.63948,      0.6392,     0.63891,     0.63862,     0.63833,     0.63805,     0.63776,     0.63747,     0.63718,     0.63689,     0.63661,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,\n",
       "            0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,\n",
       "            0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,\n",
       "            0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,\n",
       "            0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,\n",
       "            0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61818,     0.61811,     0.61797,     0.61782,     0.61768,     0.61753,     0.61739,     0.61725,      0.6171,     0.61696,     0.61681,     0.61667,     0.61653,\n",
       "            0.61638,     0.61624,     0.61609,     0.61595,     0.61581,     0.61566,     0.61552,     0.61538,     0.61523,     0.61509,     0.61494,      0.6148,     0.61466,     0.61451,     0.61437,     0.61422,     0.61408,     0.61394,     0.61379,     0.61365,      0.6135,     0.61336,     0.61322,\n",
       "            0.61307,     0.61293,     0.61278,     0.61264,      0.6125,     0.61235,     0.61221,     0.61207,     0.61192,     0.61178,     0.61163,     0.61149,     0.61135,      0.6112,     0.61106,     0.61091,     0.61077,     0.61063,     0.61048,     0.61034,     0.61019,     0.61005,     0.60991,\n",
       "            0.60976,     0.60962,     0.60947,     0.60933,     0.60919,     0.60904,      0.6089,     0.60876,     0.60861,     0.60847,     0.60832,     0.60818,     0.60804,     0.60789,     0.60775,      0.6076,     0.60746,     0.60732,     0.60717,     0.60703,     0.60688,     0.60674,      0.6066,\n",
       "            0.60645,     0.60631,     0.60616,     0.60602,     0.60588,     0.60573,     0.60559,     0.60545,      0.6053,     0.60516,     0.60501,     0.60487,     0.60473,     0.60458,     0.60444,     0.60429,     0.60415,     0.60401,     0.60386,     0.60372,     0.60357,     0.60343,     0.60329,\n",
       "            0.60314,       0.603,     0.60285,     0.60271,     0.60257,     0.60242,     0.60228,     0.60214,     0.60199,     0.60185,      0.6017,     0.60156,     0.60142,     0.60127,     0.60113,     0.60098,     0.60084,      0.6007,     0.60055,     0.60041,     0.60026,     0.60012,     0.59971,\n",
       "            0.59794,     0.59616,     0.59439,     0.59261,     0.59084,     0.58906,     0.58729,     0.58551,     0.58374,     0.58196,     0.57754,     0.57288,     0.56822,     0.56362,     0.56276,     0.56189,     0.56102,     0.56015,     0.55929,     0.55842,     0.55755,     0.55669,     0.55582,\n",
       "            0.55495,     0.55409,     0.55322,     0.55235,     0.55149,     0.55062,     0.54975,     0.54889,     0.54802,     0.54715,     0.54629,     0.54523,     0.53991,     0.53458,     0.52926,     0.52644,     0.52511,     0.52378,     0.52244,     0.52111,     0.51978,     0.51845,     0.51712,\n",
       "            0.51579,     0.51446,     0.51313,     0.51179,     0.51046,     0.50913,     0.50775,     0.50637,     0.50499,     0.50361,     0.50223,     0.50085,     0.49947,     0.49809,     0.49671,     0.49533,     0.49395,     0.49257,     0.49119,     0.49091,     0.49091,     0.49091,     0.49091,\n",
       "            0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49091,     0.49039,     0.48896,     0.48753,     0.48609,     0.48466,\n",
       "            0.48323,     0.48179,     0.48036,     0.47892,     0.47749,     0.47606,     0.47462,     0.47319,     0.47132,     0.46925,     0.46718,     0.46511,     0.46304,     0.46097,      0.4589,     0.45683,     0.45476,     0.43783,     0.43514,     0.43381,     0.43247,     0.43114,     0.42981,\n",
       "            0.42848,     0.42715,     0.42582,     0.42449,     0.42316,     0.42183,     0.42049,     0.41916,     0.41818,     0.41818,     0.41818,     0.41818,     0.41818,     0.41818,     0.41818,     0.41818,     0.41711,     0.41478,     0.41245,     0.41012,      0.4078,     0.40547,     0.40314,\n",
       "            0.40081,     0.39826,      0.3956,     0.39293,     0.39027,     0.38761,     0.38495,     0.38228,     0.38096,     0.37993,     0.37889,     0.37786,     0.37682,     0.37579,     0.37475,     0.37372,     0.37268,     0.37165,     0.37061,     0.36958,     0.36854,      0.3675,     0.36647,\n",
       "            0.36543,      0.3644,     0.36306,     0.36086,     0.35867,     0.35648,     0.35429,     0.35209,      0.3499,     0.34771,     0.34552,     0.34287,     0.34021,     0.33754,     0.33488,     0.33222,     0.32956,     0.32689,     0.32423,     0.32157,     0.31891,     0.31624,     0.31358,\n",
       "            0.31092,      0.3089,     0.30827,     0.30765,     0.30703,     0.30641,     0.30579,     0.30517,     0.30455,     0.30393,     0.30331,     0.30268,     0.30206,     0.30144,     0.30082,      0.3002,     0.29958,     0.29896,     0.29834,     0.29771,     0.29709,     0.29647,     0.29585,\n",
       "            0.29523,     0.29461,     0.29399,     0.29337,     0.29274,     0.29212,      0.2915,     0.29074,     0.28701,     0.28328,     0.27956,     0.27583,      0.2696,     0.25216,     0.23973,     0.21275,     0.20529,     0.19784,     0.19038,     0.18293,     0.18182,     0.16737,     0.15246,\n",
       "            0.13732,     0.11009,     0.10909,     0.10909,     0.10909,     0.10909,     0.10909,     0.10835,     0.10214,    0.095923,     0.09051,    0.088439,    0.086368,    0.084297,    0.082227,    0.080156,    0.078085,    0.076014,    0.073944,    0.069651,    0.062197,    0.054742,    0.053336,\n",
       "           0.052093,    0.050851,    0.049608,    0.048366,    0.047123,    0.045881,    0.044639,    0.043396,    0.042154,    0.040911,    0.039669,    0.038426,    0.037184,    0.035968,    0.034803,    0.033638,    0.032473,    0.031309,    0.030144,    0.028979,    0.027814,    0.026649,    0.025485,\n",
       "            0.02432,    0.023155,     0.02199,    0.020825,    0.019661,    0.018496,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: 0.42774431375606736\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.39053,     0.39053])\n",
       "names: {0: 'CAT', 1: 'cats'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.8067044338672555, 'metrics/recall(B)': 0.7090909090909091, 'metrics/mAP50(B)': 0.7626938367821431, 'metrics/mAP50-95(B)': 0.3905277000865034, 'fitness': 0.42774431375606736}\n",
       "save_dir: PosixPath('runs/detect/train6')\n",
       "speed: {'preprocess': 1.4084908697340224, 'inference': 1.7364223798116047, 'loss': 0.00037087334526909725, 'postprocess': 0.30028820037841797}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "newmodel = YOLO('yolov8n.pt')\n",
    "# Train the model\n",
    "newmodel.train(data=\"/home/swmal10e23/swmal_grp10/O3/CatFinder-2/data.yaml\")\n",
    "\n",
    "# Windows paths\n",
    "# res = newmodel.train(data=\"C:\\\\UNI_2023\\\\ml\\\\swmal_grp10\\\\O3\\\\CatFinder-2\\\\data.yaml\")\n",
    "# res = newmodel.predict(source=\"C:\\\\UNI_2023\\\\ml\\\\swmal_grp10\\\\O3\\\\CatFinder-2\\\\test\\\\images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the trained model can be found in './runs/train/' directory. Let's try to predict where the cat is on a picture from outside the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/swmal10e23/swmal_grp10/O3/kitty-catty.jpeg: 640x640 2 CATs, 4.9ms\n",
      "Speed: 16.4ms preprocess, 4.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/train63\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'CAT', 1: 'cats'}\n",
       " orig_img: array([[[ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  6],\n",
       "         [ 0,  0,  6],\n",
       "         [ 0,  0,  6]],\n",
       " \n",
       "        [[ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  6],\n",
       "         [ 0,  0,  6],\n",
       "         [ 0,  0,  6]],\n",
       " \n",
       "        [[ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  6],\n",
       "         [ 0,  0,  6],\n",
       "         [ 0,  0,  6]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[35, 51, 74],\n",
       "         [34, 50, 73],\n",
       "         [31, 47, 70],\n",
       "         ...,\n",
       "         [ 3,  6, 11],\n",
       "         [ 3,  4,  8],\n",
       "         [ 2,  3,  7]],\n",
       " \n",
       "        [[36, 52, 75],\n",
       "         [34, 50, 73],\n",
       "         [31, 47, 70],\n",
       "         ...,\n",
       "         [ 5,  8, 13],\n",
       "         [ 5,  6, 10],\n",
       "         [ 3,  4,  8]],\n",
       " \n",
       "        [[34, 50, 73],\n",
       "         [32, 48, 71],\n",
       "         [29, 45, 68],\n",
       "         ...,\n",
       "         [ 7, 10, 15],\n",
       "         [ 6,  7, 11],\n",
       "         [ 4,  5,  9]]], dtype=uint8)\n",
       " orig_shape: (2500, 2392)\n",
       " path: '/home/swmal10e23/swmal_grp10/O3/kitty-catty.jpeg'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/train63'\n",
       " speed: {'preprocess': 16.36958122253418, 'inference': 4.940986633300781, 'postprocess': 1.239776611328125}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "newmodel.predict(source=\"/home/swmal10e23/swmal_grp10/O3/kitty-catty.jpeg\", save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result:\n",
    "![katty](runs/detect/train11/kitty-catty.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not bad! Although we only have a single cat on this picture, so it's not perfect either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.517px",
    "left": "1228px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
